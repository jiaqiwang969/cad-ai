#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å¤±è´¥URLçš„é¡µé¢ç»“æ„
"""

import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler

def debug_failed_url():
    """è°ƒè¯•å¤±è´¥çš„URL"""
    
    # å¤±è´¥çš„URLç¤ºä¾‹
    failed_url = "https://www.traceparts.cn/en/product/item-industrietechnik-gmbh-stairway-assembly-set-pp-30deg?CatalogPath=TRACEPARTS%3ATP12002018002&Product=30-12112020-084493"
    
    # æˆåŠŸçš„URLå¯¹æ¯”
    success_url = "https://www.traceparts.cn/en/product/apostoli-f30?CatalogPath=TRACEPARTS%3ATP12002018003004&Product=90-23112023-059945"
    
    crawler = OptimizedSpecificationsCrawler()
    
    print("ğŸ§ª è°ƒè¯•å¤±è´¥URLçš„é¡µé¢ç»“æ„")
    print("="*60)
    
    # æµ‹è¯•å¤±è´¥çš„URL
    print(f"\nâŒ æµ‹è¯•å¤±è´¥URL:")
    print(f"   {failed_url}")
    
    result1 = crawler.extract_specifications(failed_url)
    print(f"   ç»“æœ: æˆåŠŸ={result1['success']}, è§„æ ¼æ•°={result1['count']}")
    
    if not result1['success']:
        print(f"   é”™è¯¯: {result1.get('error', 'Unknown')}")
    
    time.sleep(2)
    
    # æµ‹è¯•æˆåŠŸçš„URL
    print(f"\nâœ… æµ‹è¯•æˆåŠŸURL:")
    print(f"   {success_url}")
    
    result2 = crawler.extract_specifications(success_url)
    print(f"   ç»“æœ: æˆåŠŸ={result2['success']}, è§„æ ¼æ•°={result2['count']}")
    
    if not result2['success']:
        print(f"   é”™è¯¯: {result2.get('error', 'Unknown')}")
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    print(f"   å¤±è´¥URLä¾›åº”å•†: item-industrietechnik-gmbh")
    print(f"   æˆåŠŸURLä¾›åº”å•†: apostoli")
    print(f"   é¡µé¢ç»“æ„å¯èƒ½å­˜åœ¨å·®å¼‚")

if __name__ == '__main__':
    debug_failed_url()