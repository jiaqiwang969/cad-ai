#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TraceParts é«˜æ•ˆå¢é‡æ›´æ–°ç³»ç»Ÿå…¥å£
============================
æ™ºèƒ½å¿«é€Ÿæ£€æµ‹ï¼Œå¤§å¹…æå‡æ›´æ–°æ•ˆç‡
"""

import argparse
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.pipelines.efficient_incremental_manager import EfficientIncrementalManager, DetectionConfig
from src.pipelines.cache_manager import CacheLevel


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TraceParts é«˜æ•ˆå¢é‡æ›´æ–°ç³»ç»Ÿ')
    
    # æ›´æ–°çº§åˆ«
    parser.add_argument(
        '--level',
        type=str,
        choices=['classification', 'products', 'specifications'],
        default='specifications',
        help='ç›®æ ‡æ›´æ–°çº§åˆ« (é»˜è®¤: specifications)'
    )
    
    # æ€§èƒ½é…ç½®
    parser.add_argument('--workers', type=int, default=8, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 8)')
    parser.add_argument('--sample-ratio', type=float, default=0.1, help='åˆ†ç±»æ ‘é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.1)')
    parser.add_argument('--product-samples', type=int, default=50, help='äº§å“æ£€æµ‹é‡‡æ ·æ•° (é»˜è®¤: 50)')
    parser.add_argument('--spec-samples', type=int, default=20, help='è§„æ ¼æ£€æµ‹é‡‡æ ·æ•° (é»˜è®¤: 20)')
    
    # æ—¶é—´é…ç½®
    parser.add_argument('--min-interval', type=int, default=2, help='æœ€å°æ£€æµ‹é—´éš”å°æ—¶æ•° (é»˜è®¤: 2)')
    parser.add_argument('--quick-timeout', type=int, default=30, help='å¿«é€Ÿæ£€æµ‹è¶…æ—¶ç§’æ•° (é»˜è®¤: 30)')
    
    # é˜ˆå€¼é…ç½®
    parser.add_argument('--change-threshold', type=float, default=0.05, help='å˜åŒ–é˜ˆå€¼ (é»˜è®¤: 0.05)')
    parser.add_argument('--force-full-ratio', type=float, default=0.2, help='å¼ºåˆ¶å…¨é‡æ£€æŸ¥æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--cache-dir', type=str, default='results/cache', help='ç¼“å­˜ç›®å½•')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    
    # æ˜ å°„ç¼“å­˜çº§åˆ«
    level_map = {
        'classification': CacheLevel.CLASSIFICATION,
        'products': CacheLevel.PRODUCTS,
        'specifications': CacheLevel.SPECIFICATIONS
    }
    target_level = level_map[args.level]
    
    # åˆ›å»ºæ£€æµ‹é…ç½®
    config = DetectionConfig(
        classification_sample_ratio=args.sample_ratio,
        products_sample_size=args.product_samples,
        specs_sample_size=args.spec_samples,
        min_check_interval_hours=args.min_interval,
        max_parallel_workers=args.workers,
        quick_detection_timeout=args.quick_timeout,
        change_threshold=args.change_threshold,
        force_full_check_ratio=args.force_full_ratio
    )
    
    # åˆ›å»ºå¹¶è¿è¡Œé«˜æ•ˆå¢é‡æ›´æ–°ç®¡ç†å™¨
    update_manager = EfficientIncrementalManager(
        cache_dir=args.cache_dir,
        config=config
    )
    
    try:
        print("âš¡ TraceParts é«˜æ•ˆå¢é‡æ›´æ–°ç³»ç»Ÿ")
        print(f"é…ç½®å‚æ•°: é‡‡æ ·æ¯”ä¾‹={args.sample_ratio}, äº§å“é‡‡æ ·={args.product_samples}, å¹¶å‘æ•°={args.workers}")
        print("")
        
        # è¿è¡Œé«˜æ•ˆå¢é‡æ›´æ–°
        updated_data = update_manager.run_efficient_update(target_level=target_level)
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜ç»“æœ
        if args.output:
            import json
            from datetime import datetime
            
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ å¯¼å‡ºå…ƒæ•°æ®
            export_data = updated_data.copy()
            export_data['export_metadata'] = {
                'exported_at': datetime.now().isoformat(),
                'export_file': str(output_path),
                'update_type': 'efficient_incremental',
                'pipeline_version': '4.0-efficient',
                'detection_config': {
                    'sample_ratio': args.sample_ratio,
                    'product_samples': args.product_samples,
                    'spec_samples': args.spec_samples,
                    'workers': args.workers
                }
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            file_size_mb = output_path.stat().st_size / 1024 / 1024
            print(f"\nğŸ“„ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        
        print("\nâš¡ é«˜æ•ˆå¢é‡æ›´æ–°å®Œæˆï¼")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"\nâŒ é«˜æ•ˆå¢é‡æ›´æ–°å¤±è´¥: {e}")
        return 1


if __name__ == '__main__':
    exit(main()) 