#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TraceParts å¢é‡æ›´æ–°ç³»ç»Ÿå…¥å£
========================
æ™ºèƒ½å·®å¼‚æ£€æµ‹ï¼Œåªæ›´æ–°æ–°å¢å†…å®¹
"""

import argparse
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.pipelines.incremental_update_manager import IncrementalUpdateManager
from src.pipelines.cache_manager import CacheLevel


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TraceParts æ™ºèƒ½å¢é‡æ›´æ–°ç³»ç»Ÿ')
    
    # æ›´æ–°çº§åˆ«
    parser.add_argument(
        '--level',
        type=str,
        choices=['classification', 'products', 'specifications'],
        default='specifications',
        help='ç›®æ ‡æ›´æ–°çº§åˆ« (é»˜è®¤: specifications)'
    )
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--workers', type=int, default=16, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 16)')
    parser.add_argument('--cache-dir', type=str, default='results/cache', help='ç¼“å­˜ç›®å½•')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    
    # æ˜ å°„ç¼“å­˜çº§åˆ«
    level_map = {
        'classification': CacheLevel.CLASSIFICATION,
        'products': CacheLevel.PRODUCTS,
        'specifications': CacheLevel.SPECIFICATIONS
    }
    target_level = level_map[args.level]
    
    # åˆ›å»ºå¹¶è¿è¡Œå¢é‡æ›´æ–°ç®¡ç†å™¨
    update_manager = IncrementalUpdateManager(
        cache_dir=args.cache_dir,
        max_workers=args.workers
    )
    
    try:
        # è¿è¡Œå¢é‡æ›´æ–°
        updated_data = update_manager.run_incremental_update(target_level=target_level)
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜ç»“æœ
        if args.output:
            import json
            from datetime import datetime
            
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ å¯¼å‡ºå…ƒæ•°æ®
            export_data = updated_data.copy()
            export_data['export_metadata'] = {
                'exported_at': datetime.now().isoformat(),
                'export_file': str(output_path),
                'update_type': 'incremental',
                'pipeline_version': '4.0-incremental'
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            file_size_mb = output_path.stat().st_size / 1024 / 1024
            print(f"\nğŸ“„ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        
        print("\nğŸ‰ å¢é‡æ›´æ–°å®Œæˆï¼")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"\nâŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
        return 1


if __name__ == '__main__':
    exit(main()) 