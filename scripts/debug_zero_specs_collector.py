#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•è„šæœ¬ï¼šæ”¶é›†è§„æ ¼æ•°ä¸º0çš„äº§å“é“¾æ¥
ç”¨äºå‘ç°å’Œè®°å½•æ— æ³•æå–è§„æ ¼çš„äº§å“é¡µé¢
"""

import sys
sys.path.append('.')

import json
import time
from datetime import datetime
from pathlib import Path
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# åˆ›å»ºç»“æœç›®å½•
DEBUG_DIR = Path("results/debug")
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

def collect_zero_specs_products(cache_dir='results/cache/products', sample_size=100):
    """
    ä»äº§å“ç¼“å­˜ä¸­æŠ½æ ·æµ‹è¯•ï¼Œæ”¶é›†è§„æ ¼æ•°ä¸º0çš„äº§å“
    
    Args:
        cache_dir: äº§å“ç¼“å­˜ç›®å½•
        sample_size: æ¯ä¸ªå¶èŠ‚ç‚¹çš„æŠ½æ ·æ•°é‡
    """
    print("\n" + "="*80)
    print("ğŸ” è°ƒè¯•å·¥å…·ï¼šæ”¶é›†è§„æ ¼æ•°ä¸º0çš„äº§å“é“¾æ¥")
    print("="*80)
    
    # åˆå§‹åŒ–
    crawler = OptimizedSpecificationsCrawler()
    zero_specs_products = []
    total_tested = 0
    total_zero_specs = 0
    
    # è·å–æ‰€æœ‰äº§å“ç¼“å­˜æ–‡ä»¶
    cache_path = Path(cache_dir)
    product_files = list(cache_path.glob("*.json"))
    
    print(f"\nğŸ“‚ æ‰¾åˆ° {len(product_files)} ä¸ªäº§å“ç¼“å­˜æ–‡ä»¶")
    print(f"ğŸ“Š æ¯ä¸ªæ–‡ä»¶æŠ½æ ·æµ‹è¯• {sample_size} ä¸ªäº§å“")
    
    # éå†æ¯ä¸ªäº§å“ç¼“å­˜æ–‡ä»¶
    for file_idx, product_file in enumerate(product_files[:5], 1):  # å…ˆæµ‹è¯•å‰5ä¸ªæ–‡ä»¶
        print(f"\n[{file_idx}/{min(5, len(product_files))}] å¤„ç†æ–‡ä»¶: {product_file.name}")
        
        try:
            # è¯»å–äº§å“åˆ—è¡¨
            with open(product_file, 'r', encoding='utf-8') as f:
                products = json.load(f)
            
            print(f"  ğŸ“¦ æ–‡ä»¶åŒ…å« {len(products)} ä¸ªäº§å“")
            
            # æŠ½æ ·æµ‹è¯•
            test_products = products[:sample_size] if len(products) > sample_size else products
            print(f"  ğŸ§ª æµ‹è¯• {len(test_products)} ä¸ªäº§å“...")
            
            # æµ‹è¯•æ¯ä¸ªäº§å“
            for i, product_url in enumerate(test_products):
                total_tested += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    print(f"    è¿›åº¦: {i+1}/{len(test_products)}")
                
                try:
                    # æå–è§„æ ¼
                    result = crawler.extract_specifications(product_url)
                    
                    # æ£€æŸ¥æ˜¯å¦è§„æ ¼æ•°ä¸º0
                    if result['success'] and result['count'] == 0:
                        total_zero_specs += 1
                        zero_spec_info = {
                            'product_url': product_url,
                            'leaf_code': product_file.stem,
                            'test_time': datetime.now().isoformat(),
                            'result': result
                        }
                        zero_specs_products.append(zero_spec_info)
                        print(f"    âš ï¸ å‘ç°0è§„æ ¼äº§å“ #{total_zero_specs}: {product_url[:80]}...")
                        
                except Exception as e:
                    print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    # ä¿å­˜ç»“æœ
    print(f"\n" + "="*80)
    print(f"ğŸ“Š æµ‹è¯•æ±‡æ€»")
    print(f"="*80)
    print(f"âœ… æ€»æµ‹è¯•äº§å“æ•°: {total_tested}")
    print(f"âš ï¸  0è§„æ ¼äº§å“æ•°: {total_zero_specs}")
    print(f"ğŸ“ˆ 0è§„æ ¼æ¯”ä¾‹: {total_zero_specs/total_tested*100:.1f}%")
    
    if zero_specs_products:
        # ä¿å­˜è¯¦ç»†ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ ¼å¼ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        json_file = DEBUG_DIR / f"zero_specs_products_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tested': total_tested,
                    'total_zero_specs': total_zero_specs,
                    'test_time': datetime.now().isoformat(),
                    'sample_size_per_file': sample_size
                },
                'zero_specs_products': zero_specs_products
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç®€å•çš„URLåˆ—è¡¨ï¼ˆæ–¹ä¾¿å¿«é€ŸæŸ¥çœ‹ï¼‰
        txt_file = DEBUG_DIR / f"zero_specs_urls_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"# è§„æ ¼æ•°ä¸º0çš„äº§å“URLåˆ—è¡¨\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# æ€»æ•°: {len(zero_specs_products)}\n")
            f.write("#" * 80 + "\n\n")
            
            for item in zero_specs_products:
                f.write(f"# å¶èŠ‚ç‚¹: {item['leaf_code']}\n")
                f.write(f"{item['product_url']}\n\n")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  ğŸ“„ è¯¦ç»†ä¿¡æ¯: {json_file}")
        print(f"  ğŸ“„ URLåˆ—è¡¨: {txt_file}")
        
        # æ˜¾ç¤ºå‰5ä¸ªç¤ºä¾‹
        print(f"\nğŸ“‹ å‰5ä¸ª0è§„æ ¼äº§å“ç¤ºä¾‹:")
        for i, item in enumerate(zero_specs_products[:5], 1):
            print(f"\n{i}. å¶èŠ‚ç‚¹: {item['leaf_code']}")
            print(f"   URL: {item['product_url']}")
    else:
        print(f"\nâœ… å¤ªå¥½äº†ï¼æ²¡æœ‰å‘ç°è§„æ ¼æ•°ä¸º0çš„äº§å“ã€‚")


def test_specific_urls():
    """æµ‹è¯•ç‰¹å®šçš„URLåˆ—è¡¨ï¼ˆç”¨äºè°ƒè¯•å·²çŸ¥é—®é¢˜URLï¼‰"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•ç‰¹å®šURL")
    print("="*80)
    
    # åœ¨è¿™é‡Œæ·»åŠ éœ€è¦è°ƒè¯•çš„ç‰¹å®šURL
    test_urls = [
        # ç¤ºä¾‹ï¼šæ·»åŠ æ‚¨å‘ç°çš„0è§„æ ¼URL
        # "https://www.traceparts.cn/en/product/...",
    ]
    
    if not test_urls:
        print("â„¹ï¸ æ²¡æœ‰æŒ‡å®šæµ‹è¯•URLï¼Œè¯·åœ¨ä»£ç ä¸­æ·»åŠ éœ€è¦è°ƒè¯•çš„URL")
        return
    
    crawler = OptimizedSpecificationsCrawler()
    zero_specs = []
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n[{i}/{len(test_urls)}] æµ‹è¯•URL:")
        print(f"  {url}")
        
        try:
            result = crawler.extract_specifications(url)
            print(f"  æˆåŠŸ: {result['success']}")
            print(f"  è§„æ ¼æ•°: {result['count']}")
            
            if result['count'] == 0:
                zero_specs.append({
                    'url': url,
                    'result': result
                })
                
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
    
    if zero_specs:
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        debug_file = DEBUG_DIR / f"debug_specific_urls_{timestamp}.json"
        
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(zero_specs, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è°ƒè¯•ç»“æœå·²ä¿å­˜: {debug_file}")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¶é›†è§„æ ¼æ•°ä¸º0çš„äº§å“é“¾æ¥')
    parser.add_argument('--sample-size', type=int, default=20, 
                       help='æ¯ä¸ªå¶èŠ‚ç‚¹çš„æŠ½æ ·æ•°é‡ (é»˜è®¤: 20)')
    parser.add_argument('--test-specific', action='store_true',
                       help='æµ‹è¯•ç‰¹å®šçš„URLåˆ—è¡¨')
    
    args = parser.parse_args()
    
    if args.test_specific:
        test_specific_urls()
    else:
        collect_zero_specs_products(sample_size=args.sample_size) 