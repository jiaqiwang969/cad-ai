#!/usr/bin/env python3
"""
è„šæœ¬ï¼šä¿®å¤ç¼“å­˜æ–‡ä»¶ä¸­çš„ç›¸å¯¹URL
å°†æ‰€æœ‰ç›¸å¯¹è·¯å¾„çš„äº§å“URLè½¬æ¢ä¸ºç»å¯¹URLï¼ˆæ·»åŠ  https://www.traceparts.cn å‰ç¼€ï¼‰
"""

import json
import os
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_relative_urls_in_file(file_path: Path, base_url: str = "https://www.traceparts.cn") -> bool:
    """ä¿®å¤å•ä¸ªæ–‡ä»¶ä¸­çš„ç›¸å¯¹URL"""
    try:
        # è¯»å–åŸæ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯URLåˆ—è¡¨
        if isinstance(data, list):
            # ä¿®å¤URLåˆ—è¡¨
            fixed_urls = []
            changed_count = 0
            
            for url in data:
                if isinstance(url, str):
                    if url.startswith('/') and not url.startswith('http'):
                        # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ä¿®å¤
                        fixed_url = base_url + url
                        fixed_urls.append(fixed_url)
                        changed_count += 1
                    else:
                        # å·²ç»æ˜¯ç»å¯¹è·¯å¾„æˆ–å…¶ä»–æ ¼å¼
                        fixed_urls.append(url)
                else:
                    # éå­—ç¬¦ä¸²é¡¹ï¼Œä¿æŒåŸæ ·
                    fixed_urls.append(url)
            
            if changed_count > 0:
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = file_path.with_suffix('.json.backup')
                if not backup_path.exists():  # åªåœ¨æ²¡æœ‰å¤‡ä»½æ–‡ä»¶æ—¶åˆ›å»ºå¤‡ä»½
                    file_path.rename(backup_path)
                    logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
                
                # å†™å…¥ä¿®å¤åçš„æ•°æ®
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(fixed_urls, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… ä¿®å¤ {file_path.name}: {changed_count} ä¸ªç›¸å¯¹URL -> ç»å¯¹URL")
                return True
            else:
                logger.debug(f"âšªï¸ {file_path.name}: æ— éœ€ä¿®å¤")
                return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«äº§å“åˆ—è¡¨çš„å¤æ‚æ•°æ®ç»“æ„
        elif isinstance(data, dict):
            changed = False
            
            # å¤„ç†æ ¹çº§åˆ«çš„äº§å“åˆ—è¡¨
            if 'products' in data and isinstance(data['products'], list):
                fixed_products = []
                for product in data['products']:
                    if isinstance(product, str):
                        if product.startswith('/') and not product.startswith('http'):
                            fixed_products.append(base_url + product)
                            changed = True
                        else:
                            fixed_products.append(product)
                    else:
                        fixed_products.append(product)
                
                if changed:
                    data['products'] = fixed_products
            
            # å¤„ç†leavesä¸­çš„äº§å“åˆ—è¡¨
            if 'leaves' in data and isinstance(data['leaves'], list):
                for leaf in data['leaves']:
                    if isinstance(leaf, dict) and 'products' in leaf:
                        fixed_leaf_products = []
                        leaf_changed = False
                        
                        for product in leaf['products']:
                            if isinstance(product, str):
                                if product.startswith('/') and not product.startswith('http'):
                                    fixed_leaf_products.append(base_url + product)
                                    leaf_changed = True
                                else:
                                    fixed_leaf_products.append(product)
                            elif isinstance(product, dict) and 'product_url' in product:
                                # å¤„ç†åŒ…å«product_urlçš„å­—å…¸æ ¼å¼
                                if product['product_url'].startswith('/') and not product['product_url'].startswith('http'):
                                    product['product_url'] = base_url + product['product_url']
                                    leaf_changed = True
                                fixed_leaf_products.append(product)
                            else:
                                fixed_leaf_products.append(product)
                        
                        if leaf_changed:
                            leaf['products'] = fixed_leaf_products
                            changed = True
            
            if changed:
                # å¤‡ä»½å¹¶ä¿å­˜
                backup_path = file_path.with_suffix('.json.backup')
                if not backup_path.exists():
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_path}")
                
                # å†™å…¥ä¿®å¤åçš„æ•°æ®
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… ä¿®å¤å¤æ‚ç»“æ„ {file_path.name}")
                return True
            else:
                logger.debug(f"âšªï¸ {file_path.name}: æ— éœ€ä¿®å¤")
                return False
        
        else:
            logger.debug(f"âšªï¸ {file_path.name}: æœªçŸ¥æ•°æ®æ ¼å¼ï¼Œè·³è¿‡")
            return False
            
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ä¿®å¤ç¼“å­˜æ–‡ä»¶ä¸­çš„ç›¸å¯¹URL")
    
    # ç¼“å­˜ç›®å½•è·¯å¾„
    cache_base = Path("results/cache")
    
    # è¦å¤„ç†çš„ç¼“å­˜ç›®å½•
    directories_to_process = [
        cache_base / "products",      # å•ä¸ªå¶èŠ‚ç‚¹çš„äº§å“é“¾æ¥
        cache_base,                   # ä¸»ç¼“å­˜æ–‡ä»¶
    ]
    
    total_files = 0
    fixed_files = 0
    
    for cache_dir in directories_to_process:
        if not cache_dir.exists():
            logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
            continue
            
        logger.info(f"ğŸ“ å¤„ç†ç›®å½•: {cache_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = list(cache_dir.glob("*.json"))
        
        for json_file in json_files:
            # è·³è¿‡å¤‡ä»½æ–‡ä»¶
            if json_file.name.endswith('.backup'):
                continue
                
            total_files += 1
            if fix_relative_urls_in_file(json_file):
                fixed_files += 1
    
    logger.info("="*60)
    logger.info(f"ğŸ ä¿®å¤å®Œæˆ!")
    logger.info(f"   ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    logger.info(f"   ğŸ”§ ä¿®å¤æ–‡ä»¶æ•°: {fixed_files}")
    logger.info(f"   âšªï¸ æ— éœ€ä¿®å¤: {total_files - fixed_files}")
    
    if fixed_files > 0:
        logger.info("   âœ… æ‰€æœ‰ç›¸å¯¹URLå·²è½¬æ¢ä¸ºç»å¯¹URL")
        logger.info("   ğŸ“‹ åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º .json.backup")
    else:
        logger.info("   âšªï¸ æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯ç»å¯¹URLï¼Œæ— éœ€ä¿®å¤")

if __name__ == "__main__":
    main() 