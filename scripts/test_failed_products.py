#!/usr/bin/env python3
"""
æµ‹è¯•å¤±è´¥äº§å“URLçš„ä¸“ç”¨è„šæœ¬
ä»é”™è¯¯æ—¥å¿—ä¸­æå–å¤±è´¥çš„å¶èŠ‚ç‚¹URLï¼Œé€ä¸ªè¿›è¡Œäº§å“é“¾æ¥æµ‹è¯•
"""

import sys
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(BASE_DIR))

from src.crawler.ultimate_products_v2 import UltimateProductLinksCrawlerV2

def setup_logging(debug: bool = False):
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'results/test_failed_products_{int(time.time())}.log')
        ]
    )
    return logging.getLogger(__name__)

def get_latest_error_log() -> Path:
    """è·å–æœ€æ–°çš„é”™è¯¯æ—¥å¿—æ–‡ä»¶"""
    error_logs_dir = BASE_DIR / "results" / "cache" / "error_logs"
    if not error_logs_dir.exists():
        raise FileNotFoundError("é”™è¯¯æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
    
    error_files = list(error_logs_dir.glob("error_log_v*.json"))
    if not error_files:
        raise FileNotFoundError("æœªæ‰¾åˆ°é”™è¯¯æ—¥å¿—æ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œè·å–æœ€æ–°çš„
    return sorted(error_files, key=lambda x: x.name)[-1]

def extract_failed_product_urls(error_log_path: Path, max_test_count: int = 5) -> List[Dict[str, Any]]:
    """ä»é”™è¯¯æ—¥å¿—ä¸­æå–å¤±è´¥çš„äº§å“URL"""
    try:
        with open(error_log_path, 'r', encoding='utf-8') as f:
            error_data = json.load(f)
        
        product_errors = error_data.get('details', {}).get('products', [])
        failed_urls = []
        
        for error in product_errors:
            leaf_code = error.get('leaf_code')
            leaf_url = error.get('leaf_url')
            error_type = error.get('error_type', '')
            
            if leaf_code and leaf_url and error_type in ['zero_products', 'zero_products_no_exception', 'product_extraction_failed']:
                failed_urls.append({
                    'leaf_code': leaf_code,
                    'leaf_url': leaf_url,
                    'leaf_name': error.get('leaf_name', ''),
                    'error_type': error_type,
                    'reason': error.get('note', error.get('exception', 'æœªçŸ¥'))
                })
        
        # å»é‡ï¼ˆæŒ‰leaf_codeï¼‰
        unique_urls = {}
        for item in failed_urls:
            unique_urls[item['leaf_code']] = item
        
        result = list(unique_urls.values())[:max_test_count]
        return result
        
    except Exception as e:
        logging.error(f"è¯»å–é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")
        return []

def enhance_url(url: str) -> str:
    """ç»™URLæ·»åŠ å¢å¼ºå‚æ•°"""
    if 'PageSize=' in url:
        return url
    params = "PageSize=500&ShowAll=true&IncludeVariants=true"
    return f"{url}{'&' if '?' in url else '?'}{params}"

def test_single_url(crawler: UltimateProductLinksCrawlerV2, url_info: Dict[str, Any], logger: logging.Logger) -> Dict[str, Any]:
    """æµ‹è¯•å•ä¸ªURL"""
    leaf_code = url_info['leaf_code']
    original_url = url_info['leaf_url']
    enhanced_url = enhance_url(original_url)
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ§ª æµ‹è¯•å¶èŠ‚ç‚¹: {leaf_code}")
    logger.info(f"ğŸ“ åç§°: {url_info.get('leaf_name', 'æœªçŸ¥')}")
    logger.info(f"ğŸš¨ é”™è¯¯ç±»å‹: {url_info['error_type']}")
    logger.info(f"ğŸ“ åŸå§‹URL: {original_url}")
    logger.info(f"ğŸ”§ å¢å¼ºURL: {enhanced_url}")
    logger.info(f"{'='*80}")
    
    result = {
        'leaf_code': leaf_code,
        'original_url': original_url,
        'enhanced_url': enhanced_url,
        'error_type': url_info['error_type'],
        'test_result': 'unknown',
        'product_count': 0,
        'target_count': 0,
        'progress_percentage': 0.0,
        'completion_status': 'unknown',
        'test_duration': 0.0,
        'error_message': None
    }
    
    start_time = time.time()
    
    try:
        # æµ‹è¯•å¢å¼ºURL
        logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•å¢å¼ºURL...")
        product_links, progress_info = crawler.collect_all_product_links(
            leaf_url=enhanced_url,
            tp_code=leaf_code
        )
        
        end_time = time.time()
        result['test_duration'] = round(end_time - start_time, 2)
        result['product_count'] = len(product_links)
        
        if 'target_count_on_page' in progress_info:
            result['target_count'] = progress_info['target_count_on_page']
        
        if 'progress_percentage' in progress_info:
            result['progress_percentage'] = progress_info['progress_percentage']
            
        if 'completion_status' in progress_info:
            result['completion_status'] = progress_info['completion_status']
        
        # åˆ¤æ–­æµ‹è¯•ç»“æœ
        if len(product_links) > 0:
            result['test_result'] = 'success'
            logger.info(f"âœ… æµ‹è¯•æˆåŠŸï¼")
            logger.info(f"   äº§å“æ•°é‡: {len(product_links)}")
            logger.info(f"   ç›®æ ‡æ•°é‡: {result['target_count']}")
            logger.info(f"   å®Œæˆåº¦: {result['progress_percentage']:.1f}%")
            logger.info(f"   æµ‹è¯•è€—æ—¶: {result['test_duration']}ç§’")
        else:
            result['test_result'] = 'no_products'
            logger.warning(f"âš ï¸ æµ‹è¯•ç»“æœï¼šæœªæ‰¾åˆ°äº§å“")
            logger.info(f"   ç›®æ ‡æ•°é‡: {result['target_count']}")
            logger.info(f"   æµ‹è¯•è€—æ—¶: {result['test_duration']}ç§’")
        
        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        if progress_info:
            logger.info(f"ğŸ“Š è¯¦ç»†ä¿¡æ¯: {json.dumps(progress_info, ensure_ascii=False, indent=2)}")
            
    except Exception as e:
        end_time = time.time()
        result['test_duration'] = round(end_time - start_time, 2)
        result['test_result'] = 'error'
        result['error_message'] = str(e)
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.info(f"   æµ‹è¯•è€—æ—¶: {result['test_duration']}ç§’")
    
    return result

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•å¤±è´¥äº§å“URL")
    parser.add_argument('--max-test', type=int, default=5, help='æœ€å¤§æµ‹è¯•URLæ•°é‡')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--headless', action='store_true', default=True, help='æ— å¤´æ¨¡å¼')
    parser.add_argument('--specific-code', type=str, help='æµ‹è¯•ç‰¹å®šçš„å¶èŠ‚ç‚¹ä»£ç ')
    
    args = parser.parse_args()
    
    logger = setup_logging(args.debug)
    logger.info("ğŸš€ å¯åŠ¨å¤±è´¥äº§å“URLæµ‹è¯•ç¨‹åº")
    
    try:
        # è·å–æœ€æ–°é”™è¯¯æ—¥å¿—
        error_log_path = get_latest_error_log()
        logger.info(f"ğŸ“ ä½¿ç”¨é”™è¯¯æ—¥å¿—: {error_log_path.name}")
        
        # æå–å¤±è´¥çš„URL
        failed_urls = extract_failed_product_urls(error_log_path, args.max_test)
        
        if not failed_urls:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å¤±è´¥çš„äº§å“URL")
            return
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šä»£ç ï¼Œåªæµ‹è¯•è¯¥ä»£ç 
        if args.specific_code:
            failed_urls = [url for url in failed_urls if url['leaf_code'] == args.specific_code]
            if not failed_urls:
                logger.error(f"âŒ æœªæ‰¾åˆ°ä»£ç ä¸º {args.specific_code} çš„å¤±è´¥è®°å½•")
                return
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(failed_urls)} ä¸ªå¤±è´¥çš„URLå¾…æµ‹è¯•")
        
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = UltimateProductLinksCrawlerV2(
            log_level=logging.DEBUG if args.debug else logging.INFO,
            headless=args.headless,
            debug_mode=args.debug
        )
        
        test_results = []
        
        with crawler:
            for i, url_info in enumerate(failed_urls, 1):
                logger.info(f"\nğŸ”„ è¿›åº¦: {i}/{len(failed_urls)}")
                
                result = test_single_url(crawler, url_info, logger)
                test_results.append(result)
                
                # ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                if i < len(failed_urls):
                    logger.info(f"ğŸ˜´ ä¼‘æ¯ 3 ç§’...")
                    time.sleep(3)
        
        # æ±‡æ€»ç»“æœ
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        logger.info(f"{'='*80}")
        
        success_count = len([r for r in test_results if r['test_result'] == 'success'])
        no_products_count = len([r for r in test_results if r['test_result'] == 'no_products'])
        error_count = len([r for r in test_results if r['test_result'] == 'error'])
        
        logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        logger.info(f"âš ï¸ æ— äº§å“: {no_products_count} ä¸ª")
        logger.info(f"âŒ é”™è¯¯: {error_count} ä¸ª")
        
        # è¯¦ç»†ç»“æœ
        for result in test_results:
            status_emoji = {'success': 'âœ…', 'no_products': 'âš ï¸', 'error': 'âŒ'}[result['test_result']]
            logger.info(f"{status_emoji} {result['leaf_code']}: {result['product_count']} äº§å“ (è€—æ—¶: {result['test_duration']}s)")
            
            if result['test_result'] == 'success':
                logger.info(f"   ç›®æ ‡: {result['target_count']}, å®Œæˆåº¦: {result['progress_percentage']:.1f}%")
            elif result['test_result'] == 'error':
                logger.info(f"   é”™è¯¯: {result['error_message']}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = BASE_DIR / "results" / f"test_failed_products_results_{int(time.time())}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tested': len(test_results),
                    'success_count': success_count,
                    'no_products_count': no_products_count,
                    'error_count': error_count,
                    'test_timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
                },
                'details': test_results
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

if __name__ == '__main__':
    main() 