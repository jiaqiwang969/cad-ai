#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ç±»æ ‘çˆ¬å–æ¨¡å—
=============
è´Ÿè´£çˆ¬å–TracePartsçš„åˆ†ç±»æ ‘ç»“æ„
"""

import re
import time
from typing import List, Dict, Any, Tuple
from urllib.parse import urlparse
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from config.settings import Settings
from config.logging_config import LoggerMixin
from src.utils.browser_manager import create_browser_manager
from src.utils.net_guard import register_success, register_fail


class ClassificationCrawler(LoggerMixin):
    """åˆ†ç±»æ ‘çˆ¬å–å™¨"""
    
    # æ’é™¤çš„é“¾æ¥æ¨¡å¼
    EXCLUDE_PATTERNS = [
        "sign-in", "sign-up", "login", "register",
        "javascript:", "mailto:", "#"
    ]
    
    def __init__(self, browser_manager=None):
        """
        åˆå§‹åŒ–åˆ†ç±»çˆ¬å–å™¨
        
        Args:
            browser_manager: æµè§ˆå™¨ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸æä¾›ä¼šåˆ›å»ºæ–°çš„
        """
        self.browser_manager = browser_manager or create_browser_manager()
        self.root_url = Settings.URLS['root']
    
    def _scroll_to_bottom(self, driver):
        """æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰å†…å®¹"""
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_pause = Settings.CRAWLER['scroll_pause']
        
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
            last_height = new_height
    
    def _is_valid_link(self, href: str) -> bool:
        """æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
        if not href:
            return False
        
        href_lower = href.lower()
        return not any(pattern in href_lower for pattern in self.EXCLUDE_PATTERNS)
    
    def extract_classification_links(self) -> List[Dict[str, str]]:
        """
        æå–æ‰€æœ‰åˆ†ç±»é“¾æ¥
        
        Returns:
            åˆ†ç±»é“¾æ¥åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« name å’Œ url
        """
        links = []
        
        with self.browser_manager.get_browser() as driver:
            try:
                self.logger.info(f"ğŸŒ æ‰“å¼€åˆ†ç±»æ ¹é¡µé¢: {self.root_url}")
                driver.get(self.root_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                WebDriverWait(driver, Settings.CRAWLER['timeout']).until(
                    EC.presence_of_element_located((By.TAG_NAME, 'body'))
                )
                
                # æ»šåŠ¨åŠ è½½æ‰€æœ‰å†…å®¹
                self._scroll_to_bottom(driver)
                
                # æŸ¥æ‰¾æ‰€æœ‰åˆ†ç±»é“¾æ¥
                elements = driver.find_elements(
                    By.CSS_SELECTOR, 
                    "a[href*='traceparts-classification-']"
                )
                
                seen_urls = set()
                for element in elements:
                    href = element.get_attribute('href') or ''
                    
                    if not self._is_valid_link(href) or href in seen_urls:
                        continue
                    
                    seen_urls.add(href)
                    name = element.text.strip() or href.split('/')[-1]
                    
                    links.append({
                        'name': name,
                        'url': href
                    })
                
                self.logger.info(f"ğŸ”— æå–åˆ° {len(links)} ä¸ªåˆ†ç±»é“¾æ¥")
                register_success()
                
            except TimeoutException as e:
                self.logger.error(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                register_fail('timeout')
                raise
            except Exception as e:
                self.logger.error(f"æå–åˆ†ç±»é“¾æ¥å¤±è´¥: {e}")
                register_fail('parse')
                raise
        
        return links
    
    def analyse_level(self, url: str) -> int:
        """
        åˆ†æURLçš„å±‚çº§
        
        Args:
            url: åˆ†ç±»URL
            
        Returns:
            å±‚çº§æ•°å­—
        """
        if '%3ATRACEPARTS' in url:
            return 1
        
        if 'CatalogPath=TRACEPARTS%3A' in url:
            code = url.split('CatalogPath=TRACEPARTS%3A')[1].split('&')[0]
            if code.startswith('TP'):
                suffix = code[2:]
                if len(suffix) <= 2:
                    return 2
                return 2 + len(suffix) // 3
        
        # é»˜è®¤è¿”å›ç¬¬2å±‚
        return 2
    
    def _extract_code(self, url: str) -> str:
        """ä»URLä¸­æå–ä»£ç """
        if 'CatalogPath=TRACEPARTS%3A' in url:
            code = url.split('CatalogPath=TRACEPARTS%3A')[1].split('&')[0]
            return code
        return url.split('/')[-1][:30]
    
    def build_tree(self, links: List[Dict[str, str]]) -> Tuple[Dict, List[Dict]]:
        """
        æ„å»ºåˆ†ç±»æ ‘ç»“æ„
        
        Args:
            links: åˆ†ç±»é“¾æ¥åˆ—è¡¨
            
        Returns:
            (æ ¹èŠ‚ç‚¹, å¶èŠ‚ç‚¹åˆ—è¡¨)
        """
        # ä¸ºæ¯ä¸ªé“¾æ¥æ·»åŠ å±‚çº§ä¿¡æ¯
        for link in links:
            link['level'] = self.analyse_level(link['url'])
        
        # æŒ‰å±‚çº§å’Œåç§°æ’åº
        links.sort(key=lambda x: (x['level'], x['name']))
        
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = {
            'name': 'TraceParts',
            'level': 1,
            'url': self.root_url,
            'code': 'TRACE_ROOT',
            'children': [],
            'is_leaf': False
        }
        
        # ä»£ç æ˜ å°„è¡¨
        code_map = {'TRACE_ROOT': root}
        leaves = []
        
        # æ„å»ºæ ‘ç»“æ„
        for link in links:
            code = self._extract_code(link['url'])
            
            node = {
                'name': link['name'],
                'url': link['url'],
                'level': link['level'],
                'code': code,
                'children': [],
                'is_leaf': False
            }
            
            code_map[code] = node
            
            # æ‰¾åˆ°çˆ¶èŠ‚ç‚¹
            if node['level'] == 2:
                parent_code = 'TRACE_ROOT'
            else:
                # å‡è®¾ä»£ç ç»“æ„æ˜¯æ¯3ä½ä¸€å±‚
                parent_code = code[:-3]
            
            parent = code_map.get(parent_code, root)
            parent['children'].append(node)
        
        # æ ‡è®°å¶èŠ‚ç‚¹
        def mark_leaves(node):
            if not node.get('children'):
                node['is_leaf'] = True
                leaves.append(node)
            else:
                for child in node['children']:
                    mark_leaves(child)
        
        mark_leaves(root)
        
        self.logger.info(f"ğŸŒ³ æ„å»ºåˆ†ç±»æ ‘å®Œæˆï¼Œå…± {len(leaves)} ä¸ªå¶èŠ‚ç‚¹")
        return root, leaves
    
    def crawl_full_tree(self) -> Tuple[Dict, List[Dict]]:
        """
        çˆ¬å–å®Œæ•´çš„åˆ†ç±»æ ‘
        
        Returns:
            (æ ¹èŠ‚ç‚¹, å¶èŠ‚ç‚¹åˆ—è¡¨)
        """
        try:
            # æå–åˆ†ç±»é“¾æ¥
            links = self.extract_classification_links()
            
            if not links:
                self.logger.warning("æœªèƒ½æå–åˆ°ä»»ä½•åˆ†ç±»é“¾æ¥")
                return None, []
            
            # æ„å»ºæ ‘ç»“æ„
            root, leaves = self.build_tree(links)
            
            return root, leaves
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–åˆ†ç±»æ ‘å¤±è´¥: {e}")
            raise
    
    def get_leaf_nodes(self, root: Dict) -> List[Dict]:
        """
        ä»æ ‘ç»“æ„ä¸­æå–æ‰€æœ‰å¶èŠ‚ç‚¹
        
        Args:
            root: æ ¹èŠ‚ç‚¹
            
        Returns:
            å¶èŠ‚ç‚¹åˆ—è¡¨
        """
        leaves = []
        
        def collect_leaves(node):
            if node.get('is_leaf', False):
                leaves.append(node)
            else:
                for child in node.get('children', []):
                    collect_leaves(child)
        
        collect_leaves(root)
        return leaves 