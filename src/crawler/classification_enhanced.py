#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ç±»æ ‘çˆ¬å–æ¨¡å—
============
ä½¿ç”¨ç®€å•çš„Seleniumæ–¹æ³•æ„å»ºåˆ†ç±»æ ‘ï¼ˆtest-06é£æ ¼ï¼‰
äº§å“æå–ä½¿ç”¨é«˜çº§Playwrightç­–ç•¥ï¼ˆtest-08é£æ ¼ï¼‰
"""

import re
import time
import logging
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import random

# Seleniumå¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# å¯¼å…¥é…ç½®
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))
from config.settings import Settings


class EnhancedClassificationCrawler:
    """åˆ†ç±»æ ‘çˆ¬å–å™¨ - ä½¿ç”¨ç®€å•Seleniumæ–¹æ³•ï¼ˆtest-06é£æ ¼ï¼‰"""
    
    # é¢„ç¼–è¯‘é…ç½®å¸¸é‡
    ROOT_URL = Settings.URLS['root']
    TIMEOUT = Settings.CRAWLER['timeout']
    SCROLL_PAUSE = Settings.CRAWLER['scroll_pause']
    
    # æ’é™¤çš„é“¾æ¥æ¨¡å¼
    EXCLUDE_PATTERNS = [
        "sign-in", "sign-up", "login", "register",
        "javascript:", "mailto:", "#", "cookie"
    ]
    
    def __init__(self, log_level: int = logging.INFO, headless: bool = True, debug_mode: bool = False):
        """åˆå§‹åŒ–åˆ†ç±»çˆ¬å–å™¨"""
        self.logger = logging.getLogger("classification-crawler")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
            self.logger.addHandler(handler)
        self.logger.setLevel(log_level)
        self.logger.propagate = False
        
        self.headless = headless
        self.debug_mode = debug_mode
        self.driver = None
        
        # æµè§ˆå™¨æ± ç”¨äºå¹¶è¡Œæ£€æµ‹
        self.browser_pool = Queue()
        self.pool_lock = threading.Lock()
        self.pool_initialized = False
    
    def _prepare_driver(self) -> webdriver.Chrome:
        """åˆ›å»ºç®€å•çš„Chromeé©±åŠ¨ï¼ˆtest-06é£æ ¼ï¼‰"""
        if not SELENIUM_AVAILABLE:
            raise RuntimeError("Seleniumæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œåˆ†ç±»çˆ¬å–å™¨ï¼")
        
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.set_page_load_timeout(40)
        self.logger.info("âœ… ç®€å•æµè§ˆå™¨åˆ›å»ºå®Œæˆ")
        return self.driver
    
    def _initialize_browser_pool(self, pool_size: int):
        """åˆå§‹åŒ–æµè§ˆå™¨æ± ï¼Œåˆ›å»ºæŒ‡å®šæ•°é‡çš„æµè§ˆå™¨å®ä¾‹"""
        with self.pool_lock:
            if self.pool_initialized:
                return
            
            self.logger.info(f"ğŸŠ åˆå§‹åŒ–æµè§ˆå™¨æ± ï¼Œåˆ›å»º {pool_size} ä¸ªæµè§ˆå™¨å®ä¾‹...")
            
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            for i in range(pool_size):
                try:
                    driver = webdriver.Chrome(options=chrome_options)
                    driver.set_page_load_timeout(40)
                    self.browser_pool.put(driver)
                    self.logger.info(f"  ğŸ‰ æµè§ˆå™¨æ±  {i+1}/{pool_size} åˆ›å»ºå¹¶åŠ å…¥æ± ä¸­")
                except Exception as e:
                    self.logger.error(f"  âŒ æµè§ˆå™¨ {i+1} åˆ›å»ºå¤±è´¥: {e}")
            
            self.pool_initialized = True
            self.logger.info(f"ğŸš€ æµè§ˆå™¨æ± åˆå§‹åŒ–å®Œæˆï¼Œå…± {self.browser_pool.qsize()} ä¸ªå¯ç”¨æµè§ˆå™¨")
    
    def _get_browser_from_pool(self) -> webdriver.Chrome:
        """ä»æµè§ˆå™¨æ± è·å–ä¸€ä¸ªæµè§ˆå™¨å®ä¾‹"""
        if self.debug_mode:
            self.logger.info(f"ğŸ”„ ä»æµè§ˆå™¨æ± è·å–æµè§ˆå™¨ï¼Œå½“å‰æ± å¤§å°: {self.browser_pool.qsize()}")
        
        try:
            driver = self.browser_pool.get(timeout=5)  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
            # éªŒè¯æµè§ˆå™¨æ˜¯å¦è¿˜æœ‰æ•ˆ
            try:
                driver.current_url  # ç®€å•æµ‹è¯•æµè§ˆå™¨æ˜¯å¦å¯ç”¨
                if self.debug_mode:
                    self.logger.info("âœ… ä»æ± ä¸­è·å–æœ‰æ•ˆæµè§ˆå™¨")
                return driver
            except:
                # æµè§ˆå™¨å·²å¤±æ•ˆï¼Œåˆ›å»ºæ–°çš„
                self.logger.warning("âš ï¸ ä»æ± ä¸­è·å–çš„æµè§ˆå™¨å·²å¤±æ•ˆï¼Œåˆ›å»ºæ–°çš„")
                return self._create_pool_browser()
        except Exception as e:
            # å¦‚æœæ± ä¸ºç©ºæˆ–è¶…æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶æµè§ˆå™¨
            self.logger.warning(f"âš ï¸ ä»æµè§ˆå™¨æ± è·å–å¤±è´¥: {e}ï¼Œåˆ›å»ºä¸´æ—¶æµè§ˆå™¨")
            return self._create_pool_browser()
    
    def _create_pool_browser(self) -> webdriver.Chrome:
        """åˆ›å»ºæ± ç”¨çš„æµè§ˆå™¨å®ä¾‹"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(40)
        return driver
    
    def _return_browser_to_pool(self, driver: webdriver.Chrome):
        """å°†æµè§ˆå™¨å®ä¾‹è¿”å›åˆ°æ± ä¸­"""
        if driver:
            try:
                # æ¸…ç†æµè§ˆå™¨çŠ¶æ€
                driver.delete_all_cookies()
                self.browser_pool.put(driver)
                if self.debug_mode:
                    self.logger.info(f"ğŸ”„ æµè§ˆå™¨è¿”å›åˆ°æ± ä¸­ï¼Œå½“å‰æ± å¤§å°: {self.browser_pool.qsize()}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¿”å›æµè§ˆå™¨åˆ°æ± ä¸­å¤±è´¥: {e}")
                try:
                    driver.quit()
                except:
                    pass
    
    def _cleanup_browser_pool(self):
        """æ¸…ç†æµè§ˆå™¨æ± ï¼Œå…³é—­æ‰€æœ‰æµè§ˆå™¨"""
        self.logger.info("ğŸ§¹ æ¸…ç†æµè§ˆå™¨æ± ...")
        closed_count = 0
        while not self.browser_pool.empty():
            try:
                driver = self.browser_pool.get_nowait()
                driver.quit()
                closed_count += 1
            except Exception as e:
                self.logger.warning(f"âš ï¸ å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
        self.logger.info(f"âœ… å·²å…³é—­ {closed_count} ä¸ªæµè§ˆå™¨")
    
    def _scroll_full(self, driver: webdriver.Chrome):
        """æ»šåŠ¨é¡µé¢åˆ°åº•éƒ¨ï¼ˆtest-06é£æ ¼ï¼‰"""
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
        # å›åˆ°é¡¶éƒ¨
        driver.execute_script("window.scrollTo(0,0);")
    
    def _extract_links(self, driver: webdriver.Chrome) -> List[Dict]:
        """æå–åˆ†ç±»é“¾æ¥ï¼ˆtest-06é£æ ¼ï¼‰"""
        elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='traceparts-classification-']")
        self.logger.info(f"ğŸ”— å…±æ•è· {len(elements)} ä¸ªåŒ…å« classification çš„é“¾æ¥èŠ‚ç‚¹")
        records = []
        seen = set()

        def guess_name_from_href(href: str) -> str:
            try:
                if 'traceparts-classification-' in href:
                    tail = href.split('traceparts-classification-')[1]
                    path_part = tail.split('?')[0].strip('-')
                    if path_part:
                        # æ‹¿æœ€åä¸€ä¸ªæ®µè½ä½œä¸ºåç§°
                        last_seg = path_part.split('-')[-1]
                        return last_seg.replace('-', ' ').replace('_', ' ').title()
            except Exception:
                pass
            return "Unnamed"

        for el in elements:
            href = el.get_attribute('href') or ""
            # å¯è§æ–‡æœ¬
            raw_text = el.text.strip()
            if not href or any(pat in href.lower() for pat in self.EXCLUDE_PATTERNS):
                continue
            # å»é‡
            if href in seen:
                continue
            seen.add(href)

            name = raw_text
            # å¦‚æœå¯è§æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•å…¶ä»–å±æ€§
            if not name:
                alt_sources = [
                    el.get_attribute('title'),
                    el.get_attribute('aria-label'),
                    el.get_attribute('data-original-title'),
                    el.get_attribute('innerText'),
                    el.get_attribute('textContent')
                ]
                for src in alt_sources:
                    if src and src.strip():
                        name = src.strip()
                        break
            # ä»ä¸ºç©ºï¼Œè§£æ innerHTML æ‹¿å­å…ƒç´ æ–‡æœ¬
            if not name:
                inner_html = el.get_attribute('innerHTML') or ""
                soup = BeautifulSoup(inner_html, 'html.parser')
                txt = soup.get_text(" ", strip=True)
                if txt:
                    name = txt
            # ä»ä¸ºç©ºï¼Œå°è¯•ä» href æ¨æ–­
            if not name:
                name = guess_name_from_href(href)

            records.append({"name": name, "url": href})
        self.logger.info(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(records)} æ¡å”¯ä¸€åˆ†ç±»é“¾æ¥ï¼Œå…¶ä¸­å·²å¡«å……åç§°")
        return records
    
    def extract_classification_links_enhanced(self) -> List[Dict[str, str]]:
        """æå–åˆ†ç±»é“¾æ¥ï¼ˆç®€å•Seleniumæ–¹æ³•ï¼Œtest-06é£æ ¼ï¼‰"""
        try:
            # åˆ›å»ºç®€å•çš„Chromeé©±åŠ¨
            driver = self._prepare_driver()
            
            self.logger.info(f"ğŸŒ æ‰“å¼€åˆ†ç±»æ ¹é¡µé¢: {self.ROOT_URL}")
            driver.get(self.ROOT_URL)
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
            time.sleep(4)
            
            # æ»šåŠ¨å¹¶åŠ è½½æ‰€æœ‰å†…å®¹
            self._scroll_full(driver)
            
            # æå–é“¾æ¥
            records = self._extract_links(driver)
            
            return records
            
        except Exception as e:
            self.logger.error(f"æå–åˆ†ç±»é“¾æ¥å¤±è´¥: {e}")
            raise
        finally:
            self._cleanup()
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
        except Exception as e:
            self.logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def analyse_level(self, url: str) -> int:
        """æ ¹æ® CatalogPath çš„ TP ç¼–ç æ¨æ–­å±‚çº§ï¼ˆå®Œå…¨å¤åˆ»æµ‹è¯•æ–‡ä»¶06ï¼‰"""
        if "%3ATRACEPARTS" in url:
            return 1  # æ ¹åˆ†ç±»é¡µé¢
        
        level_by_dash = None
        # å¤‡ç”¨ï¼š'-' è®¡æ•°
        try:
            tail = url.split('traceparts-classification-')[1]
            path_part = tail.split('?')[0].strip('-')
            if path_part:
                level_by_dash = len(path_part.split('-')) + 1  # L2 èµ·
        except Exception:
            pass
        
        # CatalogPath æ¨æ–­
        cat_path_part = None
        if "CatalogPath=TRACEPARTS%3A" in url:
            cat_path_part = url.split("CatalogPath=TRACEPARTS%3A")[1].split('&')[0]
        if cat_path_part and cat_path_part.startswith("TP"):
            code = cat_path_part[2:]
            if len(code) <= 2:  # TP01..TP14 ç­‰
                return 2
            # å‰©ä½™æ¯ 3 ä½ä¸€ä¸ªæ·±åº¦
            depth_groups = len(code) // 3
            return 2 + depth_groups
        
        return level_by_dash if level_by_dash else 2
    
    def _extract_code(self, url: str) -> str:
        """ä»URLä¸­æå–ä»£ç """
        if 'CatalogPath=TRACEPARTS%3A' in url:
            code = url.split('CatalogPath=TRACEPARTS%3A')[1].split('&')[0]
            return code
        return url.split('/')[-1][:30]
    
    def _check_if_real_leaf_node(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦çœŸçš„æ˜¯å¶èŠ‚ç‚¹ï¼ˆåŒ…å«äº§å“ï¼‰"""
        if not url:
            return False
            
        try:
            self.logger.info(f"ğŸ” æ£€æŸ¥å¶èŠ‚ç‚¹: {url[:100]}...")
            
            # ä½¿ç”¨å½“å‰å·²æœ‰çš„driverå®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
            if not self.driver:
                self.driver = self._prepare_driver()
            
            # ä¿®æ”¹URLä»¥æ”¯æŒæ›´å¤§çš„PageSizeï¼Œå¿«é€Ÿæ£€æµ‹
            enhanced_url = self._append_page_size(url, 100)
            self.driver.get(enhanced_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)  # å‡å°‘ç­‰å¾…æ—¶é—´
            
            # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
            page_text = self.driver.page_source
            
            # === å¶èŠ‚ç‚¹æ£€æµ‹ï¼ˆéµå¾ª test/08 é€»è¾‘ï¼‰ ===
            has_results_keyword = 'results' in page_text.lower()
            has_sort_by = 'sort by' in page_text.lower()

            target_count = self._extract_target_product_count(page_text) if has_results_keyword else 0
            has_positive_count = target_count > 0

            is_leaf = has_results_keyword and has_sort_by and has_positive_count

            # æ—¥å¿—ä¿¡æ¯
            self.logger.info(
                f"      â€¢ resultså…³é”®å­—: {has_results_keyword} | Sort by: {has_sort_by} | count: {target_count} -> {'âœ…å¶èŠ‚ç‚¹' if is_leaf else 'âŒéå¶èŠ‚ç‚¹'}"
            )
            # =====================================
            
            return is_leaf
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ å¶èŠ‚ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _append_page_size(self, url: str, page_size: int) -> str:
        """ä¸ºURLæ·»åŠ PageSizeå‚æ•°"""
        if '?' in url:
            return f"{url}&PageSize={page_size}"
        else:
            return f"{url}?PageSize={page_size}"
    
    def verify_leaf_nodes(self, tree_data: Dict, max_workers: int = 16) -> Dict:
        """éªŒè¯åˆ†ç±»æ ‘ä¸­çš„æ½œåœ¨å¶èŠ‚ç‚¹ï¼Œè¿”å›æ›´æ–°åçš„æ ‘"""
        self.logger.info("ğŸ§ å¼€å§‹éªŒè¯å¶èŠ‚ç‚¹...")
        
        potential_leaves_to_check = []
        
        def collect_leaves(node):
            if node.get('is_potential_leaf'):
                potential_leaves_to_check.append(node)
            for child in node.get('children', []):
                    collect_leaves(child)
        
        collect_leaves(tree_data)
        
        if not potential_leaves_to_check:
            self.logger.info("ğŸ¤· æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ£€æµ‹çš„æ½œåœ¨å¶èŠ‚ç‚¹ã€‚")
            return tree_data

        self.logger.info(f"ğŸ•µï¸â€â™€ï¸ å°†æ£€æµ‹ {len(potential_leaves_to_check)} ä¸ªæ½œåœ¨å¶èŠ‚ç‚¹...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ£€æµ‹
        # Ensure ThreadSafeLogger is used if logging from threads, which it is.
        # Playwright in _check_single_leaf_node is thread-safe as it creates new instances.

        # Limit max_workers to avoid overwhelming system resources, especially with Playwright
        effective_max_workers = min(max_workers, Settings.CRAWLER.get('classification_max_workers', 16))
        
        # Results will be stored by node code to update the tree later
        # However, direct tree modification from threads is not safe.
        # We collect results and then update the main tree sequentially.
        
        # Progress tracking
        processed_count = 0
        lock = threading.Lock()

        # Store results to apply them sequentially later
        results_map = {} # node_code -> (is_leaf_status, product_count_from_check, details_dict)

        def process_node(node, index, total):
            nonlocal processed_count
            node_code = node['code']
            try:
                # _check_single_leaf_node now returns: is_leaf, product_count, details_dict
                is_leaf_status, product_count_from_check, details_dict = self._check_single_leaf_node(node)
                with lock:
                    results_map[node_code] = (is_leaf_status, product_count_from_check, details_dict)
                    processed_count +=1
                    if processed_count % 20 == 0 or processed_count == total:
                         self.logger.info(f"â³ å¶èŠ‚ç‚¹æ£€æµ‹è¿›åº¦: {processed_count}/{total} ({(processed_count/total)*100:.1f}%)")
                return # Result stored in map
            except Exception as e:
                self.logger.error(f"Error processing node {node.get('name', 'Unknown')} ({node_code}): {e}", exc_info=self.debug_mode)
                with lock: # Still count as processed for progress
                    results_map[node_code] = (False, 0, {'enhanced_url': node.get('url', ''), 'has_results_keyword': False, 'has_sort_by_keyword': False, 'error': str(e)}) # Store error info
                    processed_count += 1 
                return

        with ThreadPoolExecutor(max_workers=effective_max_workers) as executor:
            futures = []
            for i, node_to_check in enumerate(potential_leaves_to_check):
                futures.append(executor.submit(process_node, node_to_check, i + 1, len(potential_leaves_to_check)))
            
            # Wait for all futures to complete
            for future in as_completed(futures):
                try:
                    future.result() # Ensure exceptions from threads are caught if not handled in process_node
                except Exception as e:
                    self.logger.error(f"Future result error during leaf verification: {e}", exc_info=self.debug_mode)

        self.logger.info(f"ğŸ æ‰€æœ‰ {len(potential_leaves_to_check)} ä¸ªæ½œåœ¨å¶èŠ‚ç‚¹æ£€æµ‹å®Œæˆã€‚å¼€å§‹æ›´æ–°æ ‘...")

        # Apply results to the tree (sequentially)
        # This part needs to traverse the tree again to find the nodes by 'code'
        # and update their 'is_leaf' and 'product_count' attributes.
        
        nodes_updated_count = 0
        current_log_index = 0 # For the [index/total] log message

        def update_tree_nodes(node):
            nonlocal nodes_updated_count, current_log_index
            
            node_code = node.get('code')
            if node_code in results_map:
                current_log_index +=1 # Increment for each node that was in results_map
                is_leaf_status, product_count_from_check, details_dict = results_map[node_code]
                
                node['is_leaf'] = is_leaf_status
                node['product_count'] = product_count_from_check
                node['is_verified'] = True # Mark as verified
                nodes_updated_count += 1

                # Construct and log the single INFO message here
                node_name = node.get('name', 'UnknownNode')
                node_level = node.get('level', 0)
                
                enhanced_url = details_dict.get('enhanced_url', node.get('url', 'N/A'))
                has_number_results = details_dict.get('has_number_results_pattern', False)

                result_symbol = 'âœ…' if is_leaf_status else 'âŒ'
                leaf_status_str = 'å¶' if is_leaf_status else 'éå¶'
                results_pattern_symbol = 'âœ…' if has_number_results else 'âŒ'
                count_str = str(product_count_from_check) if product_count_from_check > 0 else ('N/A' if not is_leaf_status and product_count_from_check == 0 else '0')


                log_message_main_part = f"ç»“æœ: [{result_symbol}{leaf_status_str}] (L{node_level}, æ•°å­—+resultsæ¨¡å¼: {results_pattern_symbol}) äº§å“æ•°: {count_str}"
                log_message_url_part = f"æµ‹è¯•é“¾æ¥åœ°å€: {enhanced_url}"
                
                if 'error' in details_dict:
                     self.logger.error(f"ERROR [{current_log_index}/{len(potential_leaves_to_check)}] {node_name}: Processing error - {details_dict['error']}")
                else:
                    self.logger.info(f"[{current_log_index}/{len(potential_leaves_to_check)}] {node_name}: {log_message_main_part}, {log_message_url_part}")

            # Recursively process children
            for child in node.get('children', []):
                update_tree_nodes(child)

        update_tree_nodes(tree_data)
        self.logger.info(f"âœ… åˆ†ç±»æ ‘å¶èŠ‚ç‚¹çŠ¶æ€æ›´æ–°å®Œæˆã€‚å…±æ›´æ–° {nodes_updated_count} ä¸ªèŠ‚ç‚¹ã€‚")
        
        return tree_data
    
    def _check_single_leaf_node(self, node: Dict) -> Tuple[bool, int, Dict]:
        """å•ä¸ªå¶èŠ‚ç‚¹æ£€æµ‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œç‹¬ç«‹driverï¼‰ï¼Œä½¿ç”¨ä¸ test-08 å®Œå…¨ä¸€è‡´çš„ Playwright é€»è¾‘"""
        url = node.get('url')
        node_name = node.get('name', 'UnknownNode')
        
        details_for_log = {
            'enhanced_url': '',
            'has_number_results_pattern': False,
        }

        if not url:
            self.logger.debug(f"Node {node_name} has no URL.")
            return False, 0, details_for_log
        
        try:
            from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
            import time
            import random
            
            # å¢å¼ºURL - ä¸ test-08 ç›¸åŒ
            enhanced_url = self._append_page_size(url, 500)  # ä½¿ç”¨ä¸ test-08 ç›¸åŒçš„ PageSize=500
            details_for_log['enhanced_url'] = enhanced_url
            self.logger.debug(f"ğŸ” Playwrightæ£€æµ‹ [{node_name}]: {enhanced_url}")
            
            with sync_playwright() as p:
                browser = None
                try:
                    browser = p.chromium.launch(
                        headless=Settings.CRAWLER.get('playwright_headless', True),
                        args=[
                            '--no-sandbox',
                            '--disable-dev-shm-usage',
                            '--disable-web-security',
                            '--disable-features=VizDisplayCompositor',
                            '--disable-background-timer-throttling',
                            '--disable-backgrounding-occluded-windows',
                            '--disable-renderer-backgrounding'
                        ]
                    )
                    context = browser.new_context(
                        user_agent=Settings.CRAWLER.get('playwright_user_agent', None),
                        java_script_enabled=True,
                        ignore_https_errors=True,
                        bypass_csp=True,  # ç»•è¿‡å†…å®¹å®‰å…¨ç­–ç•¥
                        extra_http_headers={'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'}
                    )
                    page = context.new_page()
                    
                    # è®¿é—®é¡µé¢å¹¶ç­‰å¾…ç½‘ç»œç©ºé—² - æ·»åŠ é‡è¯•æœºåˆ¶å’Œæ›´çµæ´»çš„ç­‰å¾…ç­–ç•¥
                    max_retries = 2
                    retry_delay = 3
                    
                    for attempt in range(max_retries + 1):
                        try:
                            # ä½¿ç”¨æ›´å®½æ¾çš„è¶…æ—¶è®¾ç½®å’Œç­‰å¾…ç­–ç•¥
                            page.goto(enhanced_url, timeout=45000, wait_until='domcontentloaded')
                            # å°è¯•ç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œä½†è®¾ç½®è¾ƒçŸ­è¶…æ—¶ï¼Œå¤±è´¥åˆ™ç»§ç»­
                            try:
                                page.wait_for_load_state("networkidle", timeout=10000)
                            except:
                                self.logger.debug(f"âš ï¸ [{node_name}] ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å¤„ç†...")
                            
                            time.sleep(2)  # ç­‰å¾…é¡µé¢ç¨³å®š
                            break  # æˆåŠŸåŠ è½½ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                            
                        except Exception as load_error:
                            if attempt < max_retries:
                                self.logger.warning(f"âš ï¸ [{node_name}] ç¬¬{attempt+1}æ¬¡åŠ è½½å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•: {load_error}")
                                time.sleep(retry_delay)
                                continue
                            else:
                                # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                                raise load_error
                    
                    # æ£€æµ‹å¶èŠ‚ç‚¹ - ä¸ test-08 å®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼ˆç®€åŒ–ä¸ºæ•°å­—+resultsæ¨¡å¼ï¼‰
                    page_text = page.text_content("body")
                    
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æµ‹"æ•°å­—+results"æ¨¡å¼
                    # æ”¯æŒé€—å·åˆ†éš”çš„æ•°å­—å’Œä¸é—´æ–­ç©ºæ ¼(\u00a0)
                    import re
                    results_pattern = r'\b[\d,]+(?:\s|\u00a0)+results?\b'
                    has_number_results = bool(re.search(results_pattern, page_text, re.IGNORECASE))
                    
                    # è®°å½•æ£€æµ‹ç»“æœ
                    details_for_log['has_number_results_pattern'] = has_number_results
                    
                    self.logger.debug(f"ğŸ” å¶èŠ‚ç‚¹æ£€æµ‹ [{node_name}]: æ•°å­—+resultsæ¨¡å¼={'âœ…' if has_number_results else 'âŒ'}")
                    
                    # æå–ç›®æ ‡äº§å“æ€»æ•°
                    target_count = 0
                    if has_number_results:
                        self.logger.debug(f"âœ… ç¡®è®¤è¿™æ˜¯ä¸€ä¸ªå¶èŠ‚ç‚¹é¡µé¢ï¼ˆåŸºäºæ•°å­—+resultsæ¨¡å¼ï¼‰: {node_name}")
                        target_count = self._extract_target_product_count_test08_style(page_text)
                        
                        # æœ€ç»ˆå¶èŠ‚ç‚¹åˆ¤æ–­é€»è¾‘ï¼šä¸ test-08 ä¿æŒä¸€è‡´
                        is_leaf = True  # æœ‰æ•°å­—+resultsæ¨¡å¼å°±æ˜¯å¶èŠ‚ç‚¹
                    else:
                        self.logger.debug(f"âš ï¸ è¿™å¯èƒ½ä¸æ˜¯å¶èŠ‚ç‚¹é¡µé¢ï¼ˆæœªæ£€æµ‹åˆ°æ•°å­—+resultsæ¨¡å¼ï¼‰: {node_name}")
                        is_leaf = False
                    
                    return is_leaf, target_count, details_for_log
                    
                except Exception as pw_error:
                    self.logger.warning(f"âš ï¸ Playwrighté¡µé¢å¤„ç†å¤±è´¥ for [{node_name}] ({enhanced_url}): {pw_error}", exc_info=self.debug_mode)
                    details_for_log['error'] = str(pw_error)
                    return False, 0, details_for_log
                finally:
                    if browser:
                        browser.close()
                    
        except Exception as e:
            self.logger.error(f"âŒ Playwrightæ£€æµ‹ä¸¥é‡å¤±è´¥ for [{node_name}] ({url}): {e}", exc_info=self.debug_mode)
            details_for_log['error'] = str(e)
            return False, 0, details_for_log
    
    def _check_if_real_leaf_node_batch(self, url: str) -> bool:
        """æ‰¹é‡æ£€æµ‹æ¨¡å¼ä¸‹çš„å¶èŠ‚ç‚¹æ£€æµ‹ï¼ˆå¤ç”¨driverï¼‰"""
        if not url:
            return False
            
        try:
            # ä¿®æ”¹URLä»¥æ”¯æŒæ›´å¤§çš„PageSizeï¼Œå¿«é€Ÿæ£€æµ‹
            enhanced_url = self._append_page_size(url, 100)
            self.driver.get(enhanced_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(2)
            
            # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
            page_text = self.driver.page_source
            
            # ä¸¥æ ¼æ£€æŸ¥"æ•°å­— + results"æ¨¡å¼ï¼Œæ’é™¤å¹²æ‰°
            import re
            # ä¿®æ­£çš„æ­£åˆ™ï¼šåŒ¹é…ä»»æ„æ•°å­—ï¼ˆå¯èƒ½å¸¦é€—å·åˆ†éš”ï¼‰+ ç©ºæ ¼ + results
            has_numbered_results = bool(re.search(r'\b\d{1,3}(?:,\d{3})*\s+results?\b|\b\d{4,}\s+results?\b', page_text, re.IGNORECASE))
            
            # é¢å¤–æ£€æŸ¥ï¼šæ’é™¤"0 results"å’Œå¹²æ‰°è¯
            if has_numbered_results:
                # æ’é™¤0ç»“æœ
                if re.search(r'\b0\s+results?\b', page_text, re.IGNORECASE):
                    has_numbered_results = False
                else:
                    # ç¡®ä¿ä¸æ˜¯"search results"ç­‰å¹²æ‰°è¯
                    interference_patterns = [
                        r'search\s+\d+\s+results',
                        r'filter\s+\d+\s+results', 
                        r'found\s+\d+\s+results',
                        r'showing\s+\d+\s+results'
                    ]
                    for pattern in interference_patterns:
                        if re.search(pattern, page_text, re.IGNORECASE):
                            # è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦çœŸçš„æ˜¯äº§å“ç»“æœ
                            if not (has_product_links or 'Sort by' in page_text):
                                has_numbered_results = False
                                break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰äº§å“é“¾æ¥
            product_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='&Product=']")
            has_product_links = len(product_links) > 0
            
            # æ£€æŸ¥Sort byå…³é”®å­—ï¼ˆè¾…åŠ©åˆ¤æ–­ï¼‰
            has_sort_by = 'Sort by' in page_text or 'sort by' in page_text.lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹ï¼šå¿…é¡»æœ‰æ•°å­—+results æˆ– äº§å“é“¾æ¥
            is_leaf = has_numbered_results or has_product_links
            
            if self.debug_mode or is_leaf:  # åªè®°å½•å¶èŠ‚ç‚¹æˆ–debugæ¨¡å¼
                self.logger.info(f"ğŸ“Š æ£€æµ‹ç»“æœ: æ•°å­—+results={has_numbered_results}, äº§å“é“¾æ¥={has_product_links}({len(product_links)}ä¸ª), Sort by={has_sort_by} => {'âœ…å¶èŠ‚ç‚¹' if is_leaf else 'âŒéå¶èŠ‚ç‚¹'}")
            
            return is_leaf
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ é¡µé¢æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def build_classification_tree(self, records: List[Dict[str, str]]) -> Tuple[Dict, List[Dict]]:
        """æ„å»ºåˆ†ç±»æ ‘ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.logger.info("ğŸŒ³ å¼€å§‹æ„å»ºåˆ†ç±»æ ‘...")
        
        # ä¸ºæ¯æ¡è®°å½•æ·»åŠ å±‚çº§å’Œcode
        enriched = []
        for rec in records:
            level = self.analyse_level(rec['url'])
            code = self._extract_code(rec['url'])
            enriched.append({
                'name': rec['name'],
                'url': rec['url'],
                'level': level,
                'code': code
            })
        
        # æŒ‰å±‚çº§æ’åº
        enriched.sort(key=lambda x: (x['level'], x['name']))
        
        # ç»Ÿè®¡å±‚çº§
        stats = defaultdict(int)
        for item in enriched:
            stats[item['level']] += 1
        self.logger.info("ğŸ“Š å±‚çº§ç»Ÿè®¡:" + ", ".join([f"L{lv}:{cnt}" for lv,cnt in sorted(stats.items())]))
        
        # æ„å»ºåµŒå¥—æ ‘ï¼ˆä¿®å¤ç‰ˆï¼‰
        root = {
            'name': 'TraceParts Classification',
            'url': self.ROOT_URL,
            'level': 1,
            'code': 'TRACEPARTS_ROOT',
            'children': []
        }
        code_map = {'TRACEPARTS_ROOT': root}
        
        # æ„å»ºæ ‘ç»“æ„ - ä¿®å¤ç‰ˆ
        for rec in enriched:
            node = {
                'name': rec['name'],
                'url': rec['url'],
                'level': rec['level'],
                'code': rec['code'],
                'children': []
            }
            code_map[node['code']] = node
            
            # ç¡®å®šçˆ¶code - ä½¿ç”¨ä¿®å¤çš„é€»è¾‘
            if node['level'] == 2:
                parent_code = 'TRACEPARTS_ROOT'
            else:
                # æ™ºèƒ½çš„çˆ¶codeè®¡ç®—
                code = node['code']
                if code.startswith('TP') and len(code) > 2:
                    code_num = code[2:]  # å»æ‰'TP'å‰ç¼€
                    
                    if len(code_num) <= 2:  # TP01, TP14 ç­‰
                        parent_code = 'TRACEPARTS_ROOT'
                    elif len(code_num) == 3:  # TP014 -> çˆ¶èŠ‚ç‚¹åº”è¯¥æ˜¯æ ¹èŠ‚ç‚¹
                        if code_num.isdigit():  # çº¯æ•°å­—å¦‚ TP014
                            parent_code = 'TRACEPARTS_ROOT'
                        else:  # TP001 -> TP01
                            parent_code = 'TP' + code_num[:2]
                    elif len(code_num) == 6:  # TP001002 -> TP001
                        parent_code = 'TP' + code_num[:3]
                    elif len(code_num) == 9:  # TP001002003 -> TP001002
                        parent_code = 'TP' + code_num[:6]
                    else:
                        # å›é€€åˆ°åŸé€»è¾‘
                        parent_code = code[:-3]
                else:
                    # éTPå¼€å¤´çš„codeï¼Œä½¿ç”¨åŸé€»è¾‘
                    parent_code = code[:-3] if len(code) > 3 else 'TRACEPARTS_ROOT'
            
            parent = code_map.get(parent_code)
            if not parent:
                # åˆ›å»ºå ä½çˆ¶èŠ‚ç‚¹
                parent = code_map.setdefault(parent_code, {
                    'name': '(placeholder)',
                    'url': '',
                    'level': node['level'] - 1,
                    'code': parent_code,
                    'children': []
                })
            parent['children'].append(node)
        
        # ç®€å•æ ‡è®°æ½œåœ¨å¶èŠ‚ç‚¹ï¼šå…ˆç”¨åŸºæœ¬è§„åˆ™æ ‡è®°ï¼Œåç»­æ‰¹é‡æ£€æµ‹
        leaves = []
        
        def mark_potential_leaves(node):
            """é€’å½’æ ‡è®°æ½œåœ¨å¶èŠ‚ç‚¹ï¼ˆåŸºæœ¬è§„åˆ™ï¼‰"""
            # è·³è¿‡å ä½ç¬¦èŠ‚ç‚¹å’Œæ ¹èŠ‚ç‚¹
            if node['level'] <= 1 or node['name'] == '(placeholder)':
                node['is_leaf'] = False
            else:
                # ä¼˜åŒ–ç­–ç•¥ï¼šåŒºåˆ†ç¡®å®šå¶èŠ‚ç‚¹å’Œéœ€è¦æ£€æµ‹çš„èŠ‚ç‚¹
                has_no_children = not node.get('children')
                
                if has_no_children:
                    # æ­¥éª¤1: ä»»ä½•æ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹éƒ½æ˜¯æ ‘çš„æœ€æœ«å°¾ï¼Œç›´æ¥ç¡®å®šä¸ºå¶èŠ‚ç‚¹ï¼Œæ— éœ€æ£€æµ‹
                    node['is_leaf'] = True
                    node['is_potential_leaf'] = False
                    leaves.append(node)
                    self.logger.info(f"âœ… ç¡®å®šå¶èŠ‚ç‚¹: {node['name']} (å±‚çº§: L{node['level']}) - æ ‘æœ«å°¾èŠ‚ç‚¹ï¼Œæ— éœ€æ£€æµ‹")
                else:
                    # æ­¥éª¤2: æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨å¶èŠ‚ç‚¹
                    node['is_leaf'] = False  # åˆå§‹æ ‡è®°ä¸ºéå¶èŠ‚ç‚¹ï¼Œç­‰å¾…æ£€æµ‹ç¡®è®¤
                    
                    # æ ¹æ®å±‚çº§åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨å¶èŠ‚ç‚¹ï¼ˆé‡‡ç”¨ä¹‹å‰çš„æ£€æµ‹é€»è¾‘ï¼‰
                    level = node.get('level', 0)
                    if level >= 2:  # L2åŠä»¥ä¸Šå±‚çº§çš„èŠ‚ç‚¹å¯èƒ½æ˜¯å¶èŠ‚ç‚¹ (ä»¥å‰æ˜¯ L4)
                        node['is_potential_leaf'] = True
                        leaves.append(node)  # åŠ å…¥å¾…æ£€æµ‹åˆ—è¡¨
                        self.logger.info(f"ğŸ” æ½œåœ¨å¶èŠ‚ç‚¹: {node['name']} (å±‚çº§: L{level}) - æœ‰å­èŠ‚ç‚¹ä½†éœ€æ£€æµ‹")
                    else:
                        node['is_potential_leaf'] = False
                        self.logger.debug(f"âŒ éå¶èŠ‚ç‚¹: {node['name']} (å±‚çº§: L{level}) - å±‚çº§è¿‡ä½ä¸”æœ‰å­èŠ‚ç‚¹")
            
            # å¤„ç†å­èŠ‚ç‚¹
            if node.get('children'):
                for child in node['children']:
                    mark_potential_leaves(child)
        
        mark_potential_leaves(root)
        
        # æ›´æ–°æ ¹èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
        root['total_nodes'] = len(enriched) + 1
        root['total_leaves'] = len(leaves)
        
        self.logger.info(f"ğŸŒ³ æ„å»ºåˆ†ç±»æ ‘å®Œæˆï¼Œå…± {len(leaves)} ä¸ªå¶èŠ‚ç‚¹")
        
        return root, leaves
    
    def crawl_full_tree_enhanced(self) -> Tuple[Dict, List[Dict]]:
        """çˆ¬å–å®Œæ•´åˆ†ç±»æ ‘ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            # æå–é“¾æ¥
            records = self.extract_classification_links_enhanced()
            
            if not records:
                self.logger.warning("æœªèƒ½æå–åˆ°ä»»ä½•åˆ†ç±»é“¾æ¥")
                return None, []
            
            # æ„å»ºæ ‘
            root, potential_leaves = self.build_classification_tree(records)
            
            # æ‰¹é‡æ£€æµ‹å¶èŠ‚ç‚¹
            self.logger.info("ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹çœŸå®å¶èŠ‚ç‚¹...")
            verified_root = self.verify_leaf_nodes(root)
            
            # æ”¶é›†æ‰€æœ‰å¶èŠ‚ç‚¹ï¼ˆç¡®å®šçš„+æ£€æµ‹ç¡®è®¤çš„ï¼‰
            all_leaves = []
            def collect_all_leaves(node):
                if node.get('is_leaf') == True:
                    all_leaves.append(node)
                if node.get('children'):
                    for child in node['children']:
                        collect_all_leaves(child)
            
            collect_all_leaves(verified_root)
            
            self.logger.info(f"âœ… å¶èŠ‚ç‚¹éªŒè¯å®Œæˆ: æ€»å…± {len(all_leaves)} ä¸ªçœŸå®å¶èŠ‚ç‚¹")
            
            return verified_root, all_leaves
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–åˆ†ç±»æ ‘å¤±è´¥: {e}")
            return None, []

    # ----------------- æ–°å¢: æå–äº§å“æ•°é‡ -----------------
    def _extract_target_product_count_test08_style(self, page_text: str) -> int:
        """ä»é¡µé¢æå–ç›®æ ‡äº§å“æ€»æ•° - ä¸ test-08 å®Œå…¨ä¸€è‡´çš„é€»è¾‘"""
        try:
            # å¸¸è§çš„äº§å“æ•°é‡æ˜¾ç¤ºæ¨¡å¼ï¼Œæ”¯æŒé€—å·åˆ†éš”ç¬¦
            count_patterns = [
                r"([\d,]+)\s*results?",
                r"([\d,]+)\s*products?", 
                r"([\d,]+)\s*items?",
                r"showing\s*[\d,]+\s*[-â€“]\s*[\d,]+\s*of\s*([\d,]+)",
                r"([\d,]+)\s*total",
                r"found\s*([\d,]+)"
            ]
            
            # è·å–é¡µé¢å…¨éƒ¨æ–‡æœ¬å†…å®¹å¹¶è½¬ä¸ºå°å†™
            page_text_lower = page_text.lower()
            
            self.logger.debug(f"ğŸ” æœç´¢äº§å“æ•°é‡æ¨¡å¼...")
            
            # å°è¯•åŒ¹é…å„ç§æ¨¡å¼
            for pattern in count_patterns:
                if self.debug_mode:
                    self.logger.debug(f"  ğŸ“„ å°è¯•æ¨¡å¼: {pattern}")
                matches = re.findall(pattern, page_text_lower, re.IGNORECASE)
                if matches:
                    if self.debug_mode:
                        self.logger.debug(f"    ğŸ‰ æ¨¡å¼ {pattern} åŒ¹é…åˆ°: {matches}")
                    for match_item in matches:
                        try:
                            # re.findall è¿”å›çš„æ˜¯å…ƒç»„åˆ—è¡¨ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªæ•è·ç»„
                            # å¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨
                            actual_match_str = match_item if isinstance(match_item, str) else match_item[0]
                            
                            # ç§»é™¤é€—å·åè½¬æ¢ä¸ºæ•´æ•°
                            count_str = actual_match_str.replace(',', '')
                            if not count_str.isdigit(): # ç¡®ä¿æ˜¯çº¯æ•°å­—
                                self.logger.warning(f"      âš ï¸ éæ•°å­—å†…å®¹: '{count_str}' (æ¥è‡ª: '{actual_match_str}')")
                                continue
                            count = int(count_str)
                            
                            # æ›´æ–°äº§å“æ•°é‡èŒƒå›´çš„ä¸‹é™ä¸º1ï¼Œå› ä¸ºæˆ‘ä»¬å…³å¿ƒçš„æ˜¯>0
                            if 1 <= count <= 50000:  # åˆç†çš„äº§å“æ•°é‡èŒƒå›´
                                self.logger.debug(f"ğŸ¯ å‘ç°ç›®æ ‡äº§å“æ€»æ•°: {count} (æ¥è‡ªæ¨¡å¼: '{pattern}', åŸæ–‡: '{actual_match_str}')")
                                return count
                            else:
                                if self.debug_mode:
                                    self.logger.debug(f"      ğŸ”¶ æ•°é‡ {count} ä¸åœ¨æœ‰æ•ˆèŒƒå›´ [1, 50000] (æ¥è‡ª: '{actual_match_str}')")
                        except (ValueError, IndexError) as e_inner:
                            self.logger.warning(f"      âš ï¸ å¤„ç†åŒ¹é…é¡¹ '{match_item}' æ—¶å‡ºé”™: {e_inner}")
                            continue
                else:
                    if self.debug_mode:
                        self.logger.debug(f"    âŒ æ¨¡å¼ {pattern} æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹")
            
            self.logger.debug("âš ï¸ æœªèƒ½æå–åˆ°ç›®æ ‡äº§å“æ€»æ•°")
            return 0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æå–ç›®æ ‡äº§å“æ€»æ•°å¤±è´¥: {e}")
            return 0

    def _extract_target_product_count(self, page_text_lower: str) -> int:
        """ä»é¡µé¢æå–ç›®æ ‡äº§å“æ€»æ•° - ä¸¥æ ¼å¯¹é½ test/08-test_leaf_product_links.py"""
        try:
            # å¸¸è§çš„äº§å“æ•°é‡æ˜¾ç¤ºæ¨¡å¼ï¼Œæ”¯æŒé€—å·åˆ†éš”ç¬¦ (from test/08)
            count_patterns = [
                r"([\d,]+)\s*results?",
                r"([\d,]+)\s*products?", 
                r"([\d,]+)\s*items?",
                r"showing\s*[\d,]+\s*[-â€“]\s*[\d,]+\s*of\s*([\d,]+)",
                r"([\d,]+)\s*total",
                r"found\s*([\d,]+)"
            ]
            
            self.logger.debug(f"ğŸ” [ClassEnhanced] æœç´¢äº§å“æ•°é‡æ¨¡å¼...") 
            
            for pattern in count_patterns:
                if self.debug_mode: 
                    self.logger.debug(f"  ğŸ“„ [ClassEnhanced] å°è¯•æ¨¡å¼: {pattern}")
                
                matches = re.findall(pattern, page_text_lower, re.IGNORECASE)

                if matches:
                    if self.debug_mode: 
                        self.logger.debug(f"    ğŸ‰ [ClassEnhanced] æ¨¡å¼ {pattern} åŒ¹é…åˆ°: {matches}")
                    for match_item in matches:
                        try:
                            # å¯¹é½ test-08 çš„ç®€å•é€»è¾‘ï¼šå¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼›å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨
                            actual_match_str = match_item if isinstance(match_item, str) else match_item[0]
                            
                            # ç§»é™¤é€—å·åè½¬æ¢ä¸ºæ•´æ•°
                            count_str = actual_match_str.replace(',', '')
                            if not count_str.isdigit(): # ç¡®ä¿æ˜¯çº¯æ•°å­—
                                if self.debug_mode:
                                    self.logger.debug(f"      âš ï¸ [ClassEnhanced] éæ•°å­—å†…å®¹: '{count_str}' (æ¥è‡ª: '{actual_match_str}')")
                                continue
                            count = int(count_str)
                            
                            # åˆç†çš„äº§å“æ•°é‡èŒƒå›´ (1 <= count <= 50000)
                            if 1 <= count <= 50000:  
                                self.logger.debug(f"ğŸ¯ [ClassEnhanced] å‘ç°ç›®æ ‡äº§å“æ€»æ•°: {count} (æ¥è‡ªæ¨¡å¼: '{pattern}', åŸæ–‡: '{actual_match_str}')")
                                return count
                            else:
                                if self.debug_mode: 
                                    self.logger.debug(f"      ğŸ”¶ [ClassEnhanced] æ•°é‡ {count} ä¸åœ¨æœ‰æ•ˆèŒƒå›´ [1, 50000] (æ¥è‡ª: '{actual_match_str}')")
                        except (ValueError, IndexError) as e_inner:
                            if self.debug_mode: 
                                self.logger.debug(f"      âš ï¸ [ClassEnhanced] å¤„ç†åŒ¹é…é¡¹ '{match_item}' æ—¶å‡ºé”™: {e_inner}")
                            continue
                else:
                    if self.debug_mode: 
                        self.logger.debug(f"    âŒ [ClassEnhanced] æ¨¡å¼ {pattern} æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹")
            
            self.logger.debug("âš ï¸ [ClassEnhanced] æœªèƒ½æå–åˆ°ç›®æ ‡äº§å“æ€»æ•°")
            return 0
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ [ClassEnhanced] æå–ç›®æ ‡äº§å“æ€»æ•°å¤±è´¥: {e}", exc_info=self.debug_mode)
            return 0

    def get_classification_tree(self, force_refresh: bool = False, retry_failed: bool = False) -> Dict:
        """
        Orchestrates the fetching or generation of the full, verified classification tree.
        This method is called by CacheManager. Caching itself is handled by CacheManager.
        This method focuses on the crawling and verification steps.
        """
        self.logger.info(f"ğŸš€ EnhancedClassificationCrawler:get_classification_tree called (force_refresh={force_refresh}, retry_failed={retry_failed})")

        # Step 1: Crawl the basic tree structure.
        # crawl_full_tree_enhanced returns a tuple (tree_data, all_leaf_nodes_from_crawl)
        self.logger.info("ğŸŒ³ Building initial classification tree structure...")
        # Note: force_refresh and retry_failed are not directly used by crawl_full_tree_enhanced
        # as it always rebuilds from scratch. CacheManager handles whether to call this.
        tree_data, _ = self.crawl_full_tree_enhanced()
        self.logger.info("ğŸŒ³ Initial classification tree structure built.")

        # Step 2: Verify the leaf nodes in the tree.
        # verify_leaf_nodes takes the tree_data and max_workers.
        self.logger.info("ğŸ§ Verifying leaf nodes in the tree...")
        
        # Get max_workers from settings, similar to how it's done elsewhere.
        # Ensure Settings is accessible here. If not, a default or passed param is needed.
        # Assuming Settings class is available (it's used above for playwright_headless etc.)
        max_workers_for_verification = Settings.CRAWLER.get('classification_workers', 16)
        
        verified_tree_data = self.verify_leaf_nodes(tree_data, max_workers=max_workers_for_verification)
        self.logger.info("ğŸ§ Leaf node verification complete.")
        
        return verified_tree_data