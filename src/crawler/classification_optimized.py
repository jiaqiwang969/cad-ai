#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆåˆ†ç±»æ ‘çˆ¬å–æ¨¡å—
==================
å®Œå…¨å¤åˆ»æµ‹è¯•æ–‡ä»¶06çš„é€»è¾‘
"""

import re
import time
import logging
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException


class OptimizedClassificationCrawler:
    """ä¼˜åŒ–ç‰ˆåˆ†ç±»æ ‘çˆ¬å–å™¨"""
    
    # é¢„ç¼–è¯‘é…ç½®å¸¸é‡
    ROOT_URL = 'https://www.traceparts.cn/en/search/traceparts-classification?CatalogPath=TRACEPARTS%3ATRACEPARTS'
    TIMEOUT = 40
    SCROLL_PAUSE = 1.5
    
    # æ’é™¤çš„é“¾æ¥æ¨¡å¼
    EXCLUDE_PATTERNS = [
        "sign-in", "sign-up", "login", "register",
        "javascript:", "mailto:", "#"
    ]
    
    def __init__(self, log_level: int = logging.INFO):
        """åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆåˆ†ç±»çˆ¬å–å™¨"""
        # ç®€å•æ—¥å¿—è®¾ç½® (ä¸€æ¬¡æ€§)
        self.logger = logging.getLogger("opt-classification")
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
            self.logger.addHandler(handler)
        self.logger.setLevel(log_level)
        # é˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­ï¼Œé¿å…é‡å¤è¾“å‡º
        self.logger.propagate = False
    
    def _create_optimized_driver(self):
        """åˆ›å»ºä¼˜åŒ–çš„é©±åŠ¨"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(self.TIMEOUT)
        return driver
    
    def _scroll_to_bottom(self, driver):
        """æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ï¼ŒåŠ è½½æ‰€æœ‰å†…å®¹"""
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(self.SCROLL_PAUSE)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
            last_height = new_height
        
        # å›åˆ°é¡¶éƒ¨
        driver.execute_script("window.scrollTo(0,0);")
    
    def extract_classification_links(self) -> List[Dict[str, str]]:
        """æå–åˆ†ç±»é“¾æ¥ï¼ˆå®Œå…¨å¤åˆ»æµ‹è¯•æ–‡ä»¶06ï¼‰"""
        driver = None
        
        try:
            driver = self._create_optimized_driver()
            self.logger.info(f"ğŸŒ æ‰“å¼€åˆ†ç±»æ ¹é¡µé¢: {self.ROOT_URL}")
            driver.get(self.ROOT_URL)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.TAG_NAME, 'body'))
            )
            time.sleep(4)  # é¢å¤–ç­‰å¾…
            
            # æ»šåŠ¨åŠ è½½æ‰€æœ‰å†…å®¹
            self._scroll_to_bottom(driver)
            
            # æŸ¥æ‰¾æ‰€æœ‰åˆ†ç±»é“¾æ¥
            elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='traceparts-classification-']")
            self.logger.info(f"ğŸ”— å…±æ•è· {len(elements)} ä¸ªåŒ…å« classification çš„é“¾æ¥èŠ‚ç‚¹")
            
            records = []
            seen = set()
            
            def guess_name_from_href(href: str) -> str:
                """ä»URLæ¨æ–­åç§°"""
                try:
                    if 'traceparts-classification-' in href:
                        tail = href.split('traceparts-classification-')[1]
                        path_part = tail.split('?')[0].strip('-')
                        if path_part:
                            # æ‹¿æœ€åä¸€ä¸ªæ®µè½ä½œä¸ºåç§°
                            last_seg = path_part.split('-')[-1]
                            return last_seg.replace('-', ' ').replace('_', ' ').title()
                except Exception:
                    pass
                return "Unnamed"
            
            for el in elements:
                href = el.get_attribute('href') or ""
                # å¯è§æ–‡æœ¬
                raw_text = el.text.strip()
                if not href or any(pat in href.lower() for pat in self.EXCLUDE_PATTERNS):
                    continue
                # å»é‡
                if href in seen:
                    continue
                seen.add(href)
                
                name = raw_text
                # å¦‚æœå¯è§æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•å…¶ä»–å±æ€§
                if not name:
                    alt_sources = [
                        el.get_attribute('title'),
                        el.get_attribute('aria-label'),
                        el.get_attribute('data-original-title'),
                        el.get_attribute('innerText'),
                        el.get_attribute('textContent')
                    ]
                    for src in alt_sources:
                        if src and src.strip():
                            name = src.strip()
                            break
                # ä»ä¸ºç©ºï¼Œè§£æ innerHTML æ‹¿å­å…ƒç´ æ–‡æœ¬
                if not name:
                    inner_html = el.get_attribute('innerHTML') or ""
                    soup = BeautifulSoup(inner_html, 'html.parser')
                    txt = soup.get_text(" ", strip=True)
                    if txt:
                        name = txt
                # ä»ä¸ºç©ºï¼Œå°è¯•ä» href æ¨æ–­
                if not name:
                    name = guess_name_from_href(href)
                
                records.append({"name": name, "url": href})
            
            self.logger.info(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(records)} æ¡å”¯ä¸€åˆ†ç±»é“¾æ¥ï¼Œå…¶ä¸­å·²å¡«å……åç§°")
            return records
            
        except TimeoutException as e:
            self.logger.error(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
            raise
        except Exception as e:
            self.logger.error(f"æå–åˆ†ç±»é“¾æ¥å¤±è´¥: {e}")
            raise
        finally:
            if driver:
                driver.quit()
    
    def analyse_level(self, url: str) -> int:
        """æ ¹æ® CatalogPath çš„ TP ç¼–ç æ¨æ–­å±‚çº§ï¼ˆå®Œå…¨å¤åˆ»æµ‹è¯•æ–‡ä»¶06ï¼‰
        TP##                           -> L2 (ä¸»ç±»ç›®)
        TP##XXX                        -> L3 (1 ç»„ 3 ä½)
        TP##XXXYYY                     -> L4 (2 ç»„)
        ä¾æ­¤ç±»æ¨ï¼›è‹¥æ— ç¬¦åˆè§„åˆ™ï¼Œé€€å›åˆ°åŸºäº '-' è®¡æ•°æ³•ã€‚
        """
        if "%3ATRACEPARTS" in url:
            return 1  # æ ¹åˆ†ç±»é¡µé¢
        
        level_by_dash = None
        # å¤‡ç”¨ï¼š'-' è®¡æ•°
        try:
            tail = url.split('traceparts-classification-')[1]
            path_part = tail.split('?')[0].strip('-')
            if path_part:
                level_by_dash = len(path_part.split('-')) + 1  # L2 èµ·
        except Exception:
            pass
        
        # CatalogPath æ¨æ–­
        cat_path_part = None
        if "CatalogPath=TRACEPARTS%3A" in url:
            cat_path_part = url.split("CatalogPath=TRACEPARTS%3A")[1].split('&')[0]
        if cat_path_part and cat_path_part.startswith("TP"):
            code = cat_path_part[2:]
            if len(code) <= 2:  # TP01..TP14 ç­‰
                return 2
            # å‰©ä½™æ¯ 3 ä½ä¸€ä¸ªæ·±åº¦
            depth_groups = len(code) // 3
            return 2 + depth_groups
        
        return level_by_dash if level_by_dash else 2
    
    def _extract_code(self, url: str) -> str:
        """ä»URLä¸­æå–ä»£ç """
        if 'CatalogPath=TRACEPARTS%3A' in url:
            code = url.split('CatalogPath=TRACEPARTS%3A')[1].split('&')[0]
            return code
        return url.split('/')[-1][:30]
    
    def build_classification_tree(self, records: List[Dict[str, str]]) -> Tuple[Dict, List[Dict]]:
        """æ„å»ºåˆ†ç±»æ ‘ï¼ˆå¤åˆ»æµ‹è¯•æ–‡ä»¶07çš„é€»è¾‘ï¼‰"""
        self.logger.info("ğŸŒ³ å¼€å§‹æ„å»ºåˆ†ç±»æ ‘...")
        
        # ä¸ºæ¯æ¡è®°å½•æ·»åŠ å±‚çº§å’Œcode
        enriched = []
        for rec in records:
            level = self.analyse_level(rec['url'])
            code = self._extract_code(rec['url'])
            enriched.append({
                'name': rec['name'],
                'url': rec['url'],
                'level': level,
                'code': code
            })
        
        # æŒ‰å±‚çº§æ’åº
        enriched.sort(key=lambda x: (x['level'], x['name']))
        
        # ç»Ÿè®¡å±‚çº§
        stats = defaultdict(int)
        for item in enriched:
            stats[item['level']] += 1
        self.logger.info("ğŸ“Š å±‚çº§ç»Ÿè®¡:" + ", ".join([f"L{lv}:{cnt}" for lv,cnt in sorted(stats.items())]))
        
        # æ„å»ºåµŒå¥—æ ‘ï¼ˆæŒ‰ç…§æµ‹è¯•æ–‡ä»¶07çš„é€»è¾‘ï¼‰
        root = {
            'name': 'TraceParts Classification',
            'url': self.ROOT_URL,
            'level': 1,
            'code': 'TRACEPARTS_ROOT',
            'children': []
        }
        code_map = {'TRACEPARTS_ROOT': root}
        
        # æ„å»ºæ ‘ç»“æ„
        for rec in enriched:
            node = {
                'name': rec['name'],
                'url': rec['url'],
                'level': rec['level'],
                'code': rec['code'],
                'children': []
            }
            code_map[node['code']] = node
            
            # ç¡®å®šçˆ¶code
            if node['level'] == 2:
                parent_code = 'TRACEPARTS_ROOT'
            else:
                parent_code = node['code'][:-3]  # æ¯3ä½ä¸Šæº¯ä¸€çº§
            
            parent = code_map.get(parent_code)
            if not parent:
                # åˆ›å»ºå ä½çˆ¶èŠ‚ç‚¹
                parent = code_map.setdefault(parent_code, {
                    'name': '(placeholder)',
                    'url': '',
                    'level': node['level'] - 1,
                    'code': parent_code,
                    'children': []
                })
            parent['children'].append(node)
        
        # é€’å½’æ ‡æ³¨å¶èŠ‚ç‚¹
        leaves = []
        
        def mark(node):
            if node.get('children'):
                node['is_leaf'] = False
                for ch in node['children']:
                    mark(ch)
            else:
                node['is_leaf'] = True
                # ä¸åŒ…å«æ ¹èŠ‚ç‚¹å’Œå ä½èŠ‚ç‚¹
                if node['level'] > 1 and node['name'] != '(placeholder)':
                    leaves.append(node)
        
        mark(root)
        
        # æ›´æ–°æ ¹èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
        root['total_nodes'] = len(enriched) + 1
        root['total_leaves'] = len(leaves)
        
        self.logger.info(f"ğŸŒ³ æ„å»ºåˆ†ç±»æ ‘å®Œæˆï¼Œå…± {len(leaves)} ä¸ªå¶èŠ‚ç‚¹")
        
        return root, leaves
    
    def crawl_full_tree(self) -> Tuple[Dict, List[Dict]]:
        """çˆ¬å–å®Œæ•´åˆ†ç±»æ ‘"""
        try:
            # æå–é“¾æ¥
            records = self.extract_classification_links()
            
            if not records:
                self.logger.warning("æœªèƒ½æå–åˆ°ä»»ä½•åˆ†ç±»é“¾æ¥")
                return None, []
            
            # æ„å»ºæ ‘
            root, leaves = self.build_classification_tree(records)
            
            return root, leaves
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–åˆ†ç±»æ ‘å¤±è´¥: {e}")
            return None, [] 