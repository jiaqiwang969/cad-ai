#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
=============
æ”¯æŒä¸‰é˜¶æ®µç¼“å­˜ï¼š
1. åˆ†ç±»æ ‘ï¼ˆclassification treeï¼‰
2. äº§å“é“¾æ¥ï¼ˆproduct linksï¼‰
3. äº§å“è§„æ ¼ï¼ˆproduct specificationsï¼‰

æ¯ä¸ªé˜¶æ®µå¯ä»¥ç‹¬ç«‹æ›´æ–°ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ç¼“å­˜çº§åˆ«
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum
import os
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.crawler.classification_enhanced import EnhancedClassificationCrawler
from src.crawler.ultimate_products_v2 import UltimateProductLinksCrawlerV2 as UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger, ProgressTracker


def _crawl_single_leaf_product_worker(args: dict) -> dict:
    """
    å¤šè¿›ç¨‹ worker å‡½æ•°ï¼šå¤„ç†å•ä¸ªå¶èŠ‚ç‚¹çš„äº§å“é“¾æ¥çˆ¬å–
    
    Args:
        args: åŒ…å«å¶èŠ‚ç‚¹ä¿¡æ¯å’Œé…ç½®çš„å­—å…¸
    
    Returns:
        dict: åŒ…å«å¶èŠ‚ç‚¹ä»£ç ã€äº§å“åˆ—è¡¨å’Œé”™è¯¯ä¿¡æ¯çš„ç»“æœå­—å…¸
    """
    import sys
    import os
    import json
    import time
    from pathlib import Path
    
    # ä»å‚æ•°ä¸­æå–ä¿¡æ¯
    leaf = args['leaf']
    cache_dir = Path(args['cache_dir'])
    cache_ttl_hours = args['cache_ttl_hours']
    debug_mode = args.get('debug_mode', False)
    
    leaf_code = leaf['code']
    leaf_url = leaf['url']
    
    # ç»“æœå­—å…¸
    result = {
        'leaf_code': leaf_code,
        'products': [],
        'from_cache': False,
        'error_info': None
    }
    
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_file = cache_dir / f"{leaf_code}.json"
        
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < cache_ttl_hours * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºï¼‰
                if products and len(products) > 0:
                    result['products'] = products
                    result['from_cache'] = True
                    print(f"ğŸ“¦ [è¿›ç¨‹] ä½¿ç”¨æœ‰æ•ˆç¼“å­˜: {leaf_code} ({len(products)} ä¸ªäº§å“)")
                    return result
                else:
                    print(f"âš ï¸ [è¿›ç¨‹] å‘ç°ç©ºç¼“å­˜: {leaf_code}ï¼Œå°†é‡æ–°çˆ¬å–")
        
        # éœ€è¦çˆ¬å–æ–°æ•°æ® - å¯¼å…¥çˆ¬å–å™¨
        # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿æ¨¡å—è·¯å¾„æ­£ç¡®
        current_dir = Path(__file__).parent.parent
        if str(current_dir) not in sys.path:
            sys.path.insert(0, str(current_dir))
        
        from crawler.ultimate_products_v2 import UltimateProductLinksCrawlerV2
        
        print(f"ğŸŒ [è¿›ç¨‹] å¼€å§‹çˆ¬å–: {leaf_code}")
        print(f"ğŸ”— [è¿›ç¨‹] URL: {leaf_url}")
        
        # åˆ›å»ºçˆ¬å–å™¨å®ä¾‹
        crawler = UltimateProductLinksCrawlerV2(
            headless=True,
            debug_mode=debug_mode
        )
        
        # çˆ¬å–äº§å“é“¾æ¥
        with crawler:
            products, progress_info = crawler.collect_all_product_links(leaf_url)
            
            # è®°å½•è¿›åº¦ä¿¡æ¯
            target_count = progress_info.get('target_count_on_page', 0)
            if target_count > 0:
                print(f"ğŸ“Š [è¿›ç¨‹] æŠ“å–å®Œæˆåº¦: {progress_info['progress_percentage']}% ({progress_info['extracted_count']}/{target_count})")
        
        result['products'] = products
        
        # ä¿å­˜ç¼“å­˜
        cache_dir.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… [è¿›ç¨‹] å®Œæˆ: {leaf_code} ({len(products)} ä¸ªäº§å“)")
        
        # è®°å½•ç©ºäº§å“æƒ…å†µ
        if not products:
            result['error_info'] = {
                'error_type': 'zero_products_no_exception',
                'leaf_code': leaf_code,
                'leaf_name': leaf.get('name', ''),
                'leaf_url': leaf_url,
                'product_count': 0,
                'note': 'çˆ¬å–å®Œæˆä½†è¿”å›ç©ºäº§å“åˆ—è¡¨'
            }
        
    except Exception as e:
        print(f"âŒ [è¿›ç¨‹] å¤±è´¥: {leaf_code} - {e}")
        
        # è®°å½•é”™è¯¯ä¿¡æ¯
        result['error_info'] = {
            'error_type': 'product_extraction_exception',
            'leaf_code': leaf_code,
            'leaf_name': leaf.get('name', ''),
            'leaf_url': leaf_url,
            'exception': str(e),
            'exception_type': type(e).__name__,
            'note': 'äº§å“é“¾æ¥çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸'
        }
        result['products'] = []
    
    return result


class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«æšä¸¾"""
    NONE = 0
    CLASSIFICATION = 1  # ä»…åˆ†ç±»æ ‘
    PRODUCTS = 2       # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥
    SPECIFICATIONS = 3  # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥ + äº§å“è§„æ ¼


class CacheManager:
    """ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = 'results/cache', max_workers: int = 16, debug_mode: bool = False):
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers
        self.logger = ThreadSafeLogger("cache-manager", logging.INFO)
        self.progress_tracker = ProgressTracker(self.logger)
        self.debug_mode = debug_mode
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„ - æ–°çš„å‘½åè§„èŒƒ
        self.cache_index_file = self.cache_dir / 'cache_index.json'  # ç´¢å¼•æ–‡ä»¶ï¼Œè®°å½•æœ€æ–°ç‰ˆæœ¬
        self.products_cache_dir = self.cache_dir / 'products'
        self.specs_cache_dir = self.cache_dir / 'specifications'
        self.error_logs_dir = self.cache_dir / 'error_logs'  # å¼‚å¸¸è®°å½•ç›®å½•
        
        # åˆ›å»ºç‰ˆæœ¬åŒ–çš„ç¼“å­˜æ–‡ä»¶å
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.classification_file = self.cache_dir / f'classification_tree_v{self.timestamp}.json'
        self.products_file = self.cache_dir / f'products_links_v{self.timestamp}.json'  
        self.specifications_file = self.cache_dir / f'specifications_v{self.timestamp}.json'
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.products_cache_dir.mkdir(parents=True, exist_ok=True)
        self.specs_cache_dir.mkdir(parents=True, exist_ok=True)
        self.error_logs_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤±è´¥è§„æ ¼è®°å½•æ–‡ä»¶ (jsonl)
        self.failed_specs_file = self.cache_dir / 'failed_specs.jsonl'
        self.failed_lock = threading.Lock()
        
        # åˆå§‹åŒ–æ—¶æ¸…ç†é‡å¤çš„å¤±è´¥è®°å½•
        self._cleanup_duplicate_failed_specs()
        
        # åˆå§‹åŒ–çˆ¬å–å™¨
        self.classification_crawler = EnhancedClassificationCrawler()
        # ä½¿ç”¨æ–°çš„v2ç‰ˆæœ¬ï¼Œé›†æˆtest-08çš„æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
        from ..crawler.ultimate_products_v2 import UltimateProductLinksCrawlerV2
        self.products_crawler = UltimateProductLinksCrawlerV2(headless=True)  # ä½¿ç”¨æ— å¤´æ¨¡å¼
        self.specifications_crawler = OptimizedSpecificationsCrawler(log_level=logging.INFO)
        
        # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        self.cache_ttl = {
            CacheLevel.CLASSIFICATION: 24 * 7,  # åˆ†ç±»æ ‘ï¼š7å¤©
            CacheLevel.PRODUCTS: 24 * 3,       # äº§å“é“¾æ¥ï¼š3å¤©
            CacheLevel.SPECIFICATIONS: 24      # äº§å“è§„æ ¼ï¼š1å¤©
        }
        
        # å¼‚å¸¸è®°å½•
        self.error_records = {
            'products': [],      # äº§å“é“¾æ¥çˆ¬å–å¤±è´¥è®°å½•
            'specifications': [] # äº§å“è§„æ ¼çˆ¬å–å¤±è´¥è®°å½•
        }
    
    def get_cache_level(self) -> Tuple[CacheLevel, Optional[Dict]]:
        """è·å–å½“å‰ç¼“å­˜çº§åˆ«å’Œç¼“å­˜æ•°æ®"""
        # è¯»å–ç¼“å­˜ç´¢å¼•æ–‡ä»¶
        if not self.cache_index_file.exists():
            # å¦‚æœæ²¡æœ‰ç´¢å¼•æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ—§ç‰ˆæœ¬çš„ç¼“å­˜æ–‡ä»¶
            old_cache_file = self.cache_dir / 'classification_tree_full.json'
            if old_cache_file.exists():
                self.logger.info("ğŸ”„ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬ç¼“å­˜æ–‡ä»¶ï¼Œå°†è¿›è¡Œè¿ç§»")
                try:
                    with open(old_cache_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    metadata = data.get('metadata', {})
                    cache_level = CacheLevel(metadata.get('cache_level', 1))
                    return cache_level, data
                except Exception as e:
                    self.logger.error(f"è¯»å–æ—§ç‰ˆæœ¬ç¼“å­˜å¤±è´¥: {e}")
            return CacheLevel.NONE, None
        
        try:
            with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            latest_files = index_data.get('latest_files', {})
            current_level = CacheLevel.NONE
            data = None
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if 'specifications' in latest_files:
                specs_file = self.cache_dir / latest_files['specifications']
                if specs_file.exists():
                    current_level = CacheLevel.SPECIFICATIONS
                    with open(specs_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    # === æ–°å¢: æ£€æŸ¥è§„æ ¼æ•°ï¼Œå¦‚ä¸º 0 åˆ™é™çº§ ===
                    try:
                        meta = data.get('metadata', {})
                        if meta.get('total_specifications', 0) == 0:
                            self.logger.warning("æ£€æµ‹åˆ°è§„æ ¼ç¼“å­˜æ–‡ä»¶ç¼ºå°‘è§„æ ¼æ•°æ®ï¼Œå°†é™çº§ä¸º PRODUCTS çº§åˆ«é‡æ–°çˆ¬å–")
                            current_level = CacheLevel.PRODUCTS
                    except Exception:
                        pass
            elif 'products' in latest_files:
                products_file = self.cache_dir / latest_files['products']
                if products_file.exists():
                    current_level = CacheLevel.PRODUCTS
                    with open(products_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
            elif 'classification' in latest_files:
                class_file = self.cache_dir / latest_files['classification']
                if class_file.exists():
                    current_level = CacheLevel.CLASSIFICATION
                    with open(class_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if data and 'metadata' in data:
                metadata = data['metadata']
            if 'generated' in metadata:
                generated_time = datetime.fromisoformat(metadata['generated'])
                age_hours = (datetime.now() - generated_time).total_seconds() / 3600
                
                if age_hours > self.cache_ttl.get(current_level, 24):
                    self.logger.warning(f"ç¼“å­˜å·²è¿‡æœŸ (å¹´é¾„: {age_hours:.1f}å°æ—¶)")
                    return CacheLevel.NONE, None
            
            return current_level, data
            
        except Exception as e:
            self.logger.error(f"è¯»å–ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            return CacheLevel.NONE, None
    
    def _update_cache_index(self, level: CacheLevel, filename: str):
        """æ›´æ–°ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        index_data = {}
        if self.cache_index_file.exists():
            try:
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            except Exception:
                pass
        
        if 'latest_files' not in index_data:
            index_data['latest_files'] = {}
        if 'version_history' not in index_data:
            index_data['version_history'] = []
        
        # æ›´æ–°æœ€æ–°æ–‡ä»¶è®°å½•
        level_name = level.name.lower()
        index_data['latest_files'][level_name] = filename
        
        # æ·»åŠ ç‰ˆæœ¬å†å²
        version_record = {
            'level': level.name,
            'filename': filename,
            'timestamp': datetime.now().isoformat(),
            'version': self.timestamp
        }
        index_data['version_history'].append(version_record)
        
        # åªä¿ç•™æœ€è¿‘50ä¸ªç‰ˆæœ¬è®°å½•
        if len(index_data['version_history']) > 50:
            index_data['version_history'] = index_data['version_history'][-50:]
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        with open(self.cache_index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“‡ å·²æ›´æ–°ç¼“å­˜ç´¢å¼•: {level.name} -> {filename}")
    
    def save_cache(self, data: Dict, level: CacheLevel):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        # é€‰æ‹©å¯¹åº”çš„æ–‡ä»¶å
        if level == CacheLevel.CLASSIFICATION:
            cache_file = self.classification_file
        elif level == CacheLevel.PRODUCTS:
            cache_file = self.products_file
        elif level == CacheLevel.SPECIFICATIONS:
            cache_file = self.specifications_file
        else:
            raise ValueError(f"æœªçŸ¥çš„ç¼“å­˜çº§åˆ«: {level}")
        
        # å¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if cache_file.exists():
            backup_file = cache_file.with_suffix('.json.bak')
            cache_file.rename(backup_file)
            self.logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        
        try:
            # è®¡ç®—è§„æ ¼æ€»æ•°ï¼ˆåªæœ‰åœ¨SPECIFICATIONSçº§åˆ«æ‰æœ‰è§„æ ¼æ•°æ®ï¼‰
            total_specifications = 0
            if level == CacheLevel.SPECIFICATIONS:
                for leaf in data.get('leaves', []):
                    for product in leaf.get('products', []):
                        if isinstance(product, dict):
                            total_specifications += len(product.get('specifications', []))
            
            # æ›´æ–°å…ƒæ•°æ®
            data['metadata'] = {
                'generated': datetime.now().isoformat(),
                'cache_level': level.value,
                'cache_level_name': level.name,
                'version': f'v{self.timestamp}',
                'total_leaves': len(data.get('leaves', [])),
                'total_products': sum(leaf.get('product_count', 0) for leaf in data.get('leaves', [])),
                'total_specifications': total_specifications
            }
            
            # ä¿å­˜æ–‡ä»¶
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            file_size_mb = cache_file.stat().st_size / 1024 / 1024
            self.logger.info(f"ğŸ’¾ å·²ä¿å­˜ç¼“å­˜åˆ°: {cache_file}")
            self.logger.info(f"   ç¼“å­˜çº§åˆ«: {level.name}")
            self.logger.info(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
            self.logger.info(f"   ç‰ˆæœ¬å·: v{self.timestamp}")
            
            # æ›´æ–°ç´¢å¼•æ–‡ä»¶
            self._update_cache_index(level, cache_file.name)
            
            # æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘5ä¸ªç‰ˆæœ¬ï¼‰
            self._cleanup_old_versions(level)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _cleanup_old_versions(self, level: CacheLevel, keep_versions: int = 5):
        """æ¸…ç†æ—§ç‰ˆæœ¬çš„ç¼“å­˜æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ ä¸ªç‰ˆæœ¬"""
        try:
            # æ ¹æ®çº§åˆ«ç¡®å®šæ–‡ä»¶æ¨¡å¼
            if level == CacheLevel.CLASSIFICATION:
                pattern = "classification_tree_v*.json"
            elif level == CacheLevel.PRODUCTS:
                pattern = "products_links_v*.json"
            elif level == CacheLevel.SPECIFICATIONS:
                pattern = "specifications_v*.json"
            else:
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
            cache_files = list(self.cache_dir.glob(pattern))
            if len(cache_files) <= keep_versions:
                return
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤è¾ƒæ—§çš„æ–‡ä»¶
            cache_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            files_to_delete = cache_files[keep_versions:]
            
            for file_path in files_to_delete:
                try:
                    file_path.unlink()
                    self.logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§ç‰ˆæœ¬æ–‡ä»¶: {file_path.name}")
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            
            if files_to_delete:
                self.logger.info(f"ğŸ§¹ å·²æ¸…ç† {len(files_to_delete)} ä¸ªæ—§ç‰ˆæœ¬æ–‡ä»¶")
                
        except Exception as e:
            self.logger.warning(f"æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_version_history(self, level: Optional[CacheLevel] = None) -> List[Dict]:
        """è·å–ç‰ˆæœ¬å†å²è®°å½•"""
        try:
            if not self.cache_index_file.exists():
                return []
            
            with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            history = index_data.get('version_history', [])
            
            # å¦‚æœæŒ‡å®šäº†çº§åˆ«ï¼Œåªè¿”å›è¯¥çº§åˆ«çš„å†å²
            if level:
                history = [h for h in history if h.get('level') == level.name]
            
            # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            history.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
            return history
            
        except Exception as e:
            self.logger.error(f"è·å–ç‰ˆæœ¬å†å²å¤±è´¥: {e}")
            return []
    
    def get_cache_status(self) -> Dict:
        """è·å–è¯¦ç»†çš„ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        current_level, data = self.get_cache_level()
        
        status = {
            'current_level': current_level.name,
            'current_level_value': current_level.value,
            'cache_directory': str(self.cache_dir),
            'latest_files': {},
            'file_sizes': {},
            'metadata': {}
        }
        
        # è¯»å–ç´¢å¼•æ–‡ä»¶è·å–æœ€æ–°æ–‡ä»¶ä¿¡æ¯
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                latest_files = index_data.get('latest_files', {})
                for level_name, filename in latest_files.items():
                    file_path = self.cache_dir / filename
                    if file_path.exists():
                        status['latest_files'][level_name] = filename
                        status['file_sizes'][level_name] = f"{file_path.stat().st_size / 1024 / 1024:.1f} MB"
        except Exception as e:
            self.logger.warning(f"è¯»å–ç¼“å­˜ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
        
        # å¦‚æœæœ‰å½“å‰æ•°æ®ï¼Œæå–å…ƒæ•°æ®
        if data and 'metadata' in data:
            status['metadata'] = data['metadata']
        
        return status
    
    def _record_error(self, error_type: str, error_info: Dict):
        """è®°å½•é”™è¯¯ä¿¡æ¯ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼ŒåŒä¸€ä¸ªå¶èŠ‚ç‚¹åªè®°å½•æœ€æ–°é”™è¯¯ï¼‰"""
        error_record = {
            'timestamp': datetime.now().isoformat(),
            'version': f'v{self.timestamp}',
            **error_info
        }
        
        if error_type in self.error_records:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå¶èŠ‚ç‚¹çš„é”™è¯¯è®°å½•
            leaf_code = error_info.get('leaf_code')
            if leaf_code and error_type == 'products':
                # ç§»é™¤è¯¥å¶èŠ‚ç‚¹çš„æ—§è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                self.error_records[error_type] = [
                    record for record in self.error_records[error_type] 
                    if record.get('leaf_code') != leaf_code
                ]
            
            # æ·»åŠ æ–°è®°å½•
            self.error_records[error_type].append(error_record)
    
    def _save_error_logs(self):
        """ä¿å­˜å¼‚å¸¸è®°å½•åˆ°æ–‡ä»¶"""
        if not any(self.error_records.values()):
            return  # æ²¡æœ‰é”™è¯¯è®°å½•ï¼Œä¸éœ€è¦ä¿å­˜
        
        error_log_file = self.error_logs_dir / f'error_log_v{self.timestamp}.json'
        
        # ç»Ÿè®¡ä¿¡æ¯
        error_summary = {
            'generated': datetime.now().isoformat(),
            'version': f'v{self.timestamp}',
            'summary': {
                'total_product_errors': len(self.error_records['products']),
                'total_specification_errors': len(self.error_records['specifications']),
                'zero_specs_count': len([e for e in self.error_records['specifications'] if e.get('spec_count', 0) == 0]),
                'exception_count': len([e for e in self.error_records['specifications'] if 'exception' in e])
            },
            'details': self.error_records
        }
        
        # ä¿å­˜é”™è¯¯æ—¥å¿—
        with open(error_log_file, 'w', encoding='utf-8') as f:
            json.dump(error_summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“ å¼‚å¸¸è®°å½•å·²ä¿å­˜: {error_log_file}")
        self.logger.info(f"   â€¢ äº§å“é“¾æ¥å¤±è´¥: {error_summary['summary']['total_product_errors']} ä¸ª")
        self.logger.info(f"   â€¢ è§„æ ¼çˆ¬å–å¤±è´¥: {error_summary['summary']['total_specification_errors']} ä¸ª")
        self.logger.info(f"   â€¢ å…¶ä¸­é›¶è§„æ ¼: {error_summary['summary']['zero_specs_count']} ä¸ª")
    
    def extend_to_products(self, data: Dict) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“é“¾æ¥çº§åˆ«ï¼ˆè‡ªåŠ¨æ™ºèƒ½é‡è¯•å¤±è´¥è®°å½•ï¼‰"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“¦ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“é“¾æ¥")
        self.logger.info("="*60)
        
        # åŠ è½½å¤±è´¥è®°å½•ï¼ˆä»é”™è¯¯æ—¥å¿—ï¼Œæœ€å¤šé‡è¯•3æ¬¡ï¼Œé˜²æ­¢æ­»å¾ªç¯ï¼‰
        failed_products_db = self._load_failed_products_from_error_logs(max_retry_times=3)
        
        # å¤±è´¥è®°å½•ç»Ÿè®¡
        if failed_products_db:
            tries_stats = {}
            for record in failed_products_db.values():
                tries = record.get('tries', 1)
                tries_stats[tries] = tries_stats.get(tries, 0) + 1
            
            self.logger.info(f"ğŸ“‹ æ£€æµ‹åˆ°å¤±è´¥äº§å“è®°å½•: å…± {len(failed_products_db)} ä¸ªå¤±è´¥å¶èŠ‚ç‚¹")
            for tries in sorted(tries_stats.keys()):
                self.logger.info(f"   â€¢ å¤±è´¥ {tries} æ¬¡: {tries_stats[tries]} ä¸ªå¶èŠ‚ç‚¹")
        else:
            self.logger.info("ğŸ“‹ å¤±è´¥äº§å“è®°å½•: æ— å¤±è´¥è®°å½•")
        
        # æ™ºèƒ½æ„å»ºå¤„ç†åˆ—è¡¨ï¼šåˆ†åˆ«å¤„ç†å¤±è´¥çš„å¶èŠ‚ç‚¹å’Œæ­£å¸¸å¶èŠ‚ç‚¹
        all_leaves = data['leaves']
        priority_failed_leaves = []
        normal_leaves = []
        
        # å…ˆæ·»åŠ å¤±è´¥çš„å¶èŠ‚ç‚¹è¿›è¡Œé‡è¯•
        failed_leaf_codes = set(failed_products_db.keys())
        for leaf in all_leaves:
            if leaf['code'] in failed_leaf_codes:
                leaf['is_retry'] = True
                leaf['previous_tries'] = failed_products_db[leaf['code']].get('tries', 0)
                priority_failed_leaves.append(leaf)
            else:
                normal_leaves.append(leaf)
        
        if priority_failed_leaves:
            self.logger.info(f"ğŸ”„ ä¼˜å…ˆé‡è¯•å¤±è´¥å¶èŠ‚ç‚¹: {len(priority_failed_leaves)} ä¸ª")
        if normal_leaves:
            self.logger.info(f"ğŸ“‹ æ­£å¸¸å¤„ç†å¶èŠ‚ç‚¹: {len(normal_leaves)} ä¸ª")
        
        if not priority_failed_leaves and not normal_leaves:
            self.logger.info("âšªï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„å¶èŠ‚ç‚¹")
            return data
        
        # æ€»è®¡ä»»åŠ¡æ•°
        total_leaves = len(priority_failed_leaves) + len(normal_leaves)
        self.progress_tracker.register_task("äº§å“é“¾æ¥æ‰©å±•", total_leaves)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä¼˜å…ˆå¹¶è¡Œå¤„ç†å¤±è´¥é‡è¯•çš„å¶èŠ‚ç‚¹
        leaf_products = {}
        
        if priority_failed_leaves:
            self.logger.info(f"\nğŸ“ [é˜¶æ®µ 1/2] å¹¶è¡Œé‡è¯•å¤±è´¥å¶èŠ‚ç‚¹")
            self.logger.info("-" * 50)
            
            # å¤±è´¥é‡è¯•æ€»æ˜¯ä½¿ç”¨å¹¶è¡Œï¼ˆé™¤éåªæœ‰1ä¸ªï¼‰
            use_parallel_retry = self.max_workers > 1 and len(priority_failed_leaves) > 1
            
            if use_parallel_retry:
                retry_workers = min(self.max_workers//2, len(priority_failed_leaves), 8)  # ä¸ºå¤±è´¥é‡è¯•åˆ†é…ä¸€åŠè¿›ç¨‹
                self.logger.info(f"ğŸš€ å¹¶è¡Œé‡è¯•æ¨¡å¼: {len(priority_failed_leaves)} ä¸ªå¤±è´¥å¶èŠ‚ç‚¹ï¼ˆ{retry_workers} è¿›ç¨‹ï¼‰")
                retry_results = self._crawl_products_parallel(priority_failed_leaves, max_processes=retry_workers)
            else:
                self.logger.info(f"ğŸ”„ ä¸²è¡Œé‡è¯•æ¨¡å¼: {len(priority_failed_leaves)} ä¸ªå¤±è´¥å¶èŠ‚ç‚¹")
                retry_results = self._crawl_products_serial(priority_failed_leaves)
            
            # åˆå¹¶é‡è¯•ç»“æœ
            leaf_products.update(retry_results)
            
            # ç»Ÿè®¡é‡è¯•æˆåŠŸæƒ…å†µ
            successful_retries = []
            for leaf_code, products in retry_results.items():
                if products and len(products) > 0:
                    successful_retries.append(leaf_code)
            
            if successful_retries:
                self.logger.info(f"ğŸ‰ é‡è¯•æˆåŠŸ: {len(successful_retries)} ä¸ªå¶èŠ‚ç‚¹ä¿®å¤")
            
            retry_failures = len(priority_failed_leaves) - len(successful_retries)
            if retry_failures > 0:
                self.logger.info(f"âš ï¸  é‡è¯•ä»å¤±è´¥: {retry_failures} ä¸ªå¶èŠ‚ç‚¹")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æ­£å¸¸å¶èŠ‚ç‚¹
        if normal_leaves:
            self.logger.info(f"\nğŸ“ [é˜¶æ®µ 2/2] å¤„ç†æ­£å¸¸å¶èŠ‚ç‚¹")
            self.logger.info("-" * 50)
            
            # é€‰æ‹©å¤„ç†æ¨¡å¼ï¼šå¹¶è¡Œ vs ä¸²è¡Œ
            use_parallel_normal = self.max_workers > 1 and len(normal_leaves) > 3
            
            if use_parallel_normal:
                normal_workers = min(self.max_workers, len(normal_leaves), 8)  # æ­£å¸¸å¤„ç†å¯ä»¥ä½¿ç”¨å…¨éƒ¨è¿›ç¨‹
                self.logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(normal_leaves)} ä¸ªæ­£å¸¸å¶èŠ‚ç‚¹ï¼ˆ{normal_workers} è¿›ç¨‹ï¼‰")
                normal_results = self._crawl_products_parallel(normal_leaves, max_processes=normal_workers)
            else:
                self.logger.info(f"ğŸ”„ ä¸²è¡Œå¤„ç† {len(normal_leaves)} ä¸ªæ­£å¸¸å¶èŠ‚ç‚¹")
                normal_results = self._crawl_products_serial(normal_leaves)
            
            # åˆå¹¶æ­£å¸¸ç»“æœ
            leaf_products.update(normal_results)
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_products(data, leaf_products)
        
        # ç»Ÿè®¡
        total_products = sum(len(products) for products in leaf_products.values())
        productive_leaves = sum(1 for products in leaf_products.values() if len(products) > 0)
        total_processed = len(priority_failed_leaves) + len(normal_leaves)
        empty_leaves = total_processed - productive_leaves
        zero_product_errors = len([e for e in self.error_records['products'] if e.get('error_type') == 'zero_products'])
        failed_errors = len([e for e in self.error_records['products'] if e.get('error_type') == 'product_extraction_failed'])
        
        self.logger.info(f"\nâœ… äº§å“é“¾æ¥æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†å¶èŠ‚ç‚¹: {total_processed} ä¸ª")
        if total_processed > 0:
            self.logger.info(f"   â€¢ æœ‰æ•ˆå¶èŠ‚ç‚¹: {productive_leaves} ä¸ª ({productive_leaves/total_processed*100:.1f}%)")
        else:
            self.logger.info(f"   â€¢ æœ‰æ•ˆå¶èŠ‚ç‚¹: {productive_leaves} ä¸ª (0.0%)")
        self.logger.info(f"   â€¢ ç©ºå¶èŠ‚ç‚¹: {zero_product_errors} ä¸ª (æ— äº§å“)")
        self.logger.info(f"   â€¢ å¤±è´¥å¶èŠ‚ç‚¹: {failed_errors} ä¸ª (çˆ¬å–å¼‚å¸¸)")
        self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {total_products} ä¸ª")
        if productive_leaves > 0:
            self.logger.info(f"   â€¢ å¹³å‡æ¯ä¸ªæœ‰æ•ˆå¶èŠ‚ç‚¹: {total_products/productive_leaves:.1f} ä¸ªäº§å“")
        
        # æ·»åŠ åˆ†é˜¶æ®µç»Ÿè®¡
        if priority_failed_leaves:
            retry_success = sum(1 for leaf in priority_failed_leaves if leaf_products.get(leaf['code'], []))
            self.logger.info(f"   ğŸ“Š é‡è¯•ç»Ÿè®¡: {retry_success}/{len(priority_failed_leaves)} ä¸ªå¤±è´¥å¶èŠ‚ç‚¹ä¿®å¤")
        if normal_leaves:
            normal_success = sum(1 for leaf in normal_leaves if leaf_products.get(leaf['code'], []))
            self.logger.info(f"   ğŸ“Š æ­£å¸¸ç»Ÿè®¡: {normal_success}/{len(normal_leaves)} ä¸ªå¶èŠ‚ç‚¹æˆåŠŸ")
        
        # ä¿å­˜å¼‚å¸¸è®°å½•ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if self.error_records['products']:
            self._save_error_logs()
        
        return data
    
    def extend_to_specifications(self, data: Dict, retry_failed_only: bool = False) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“è§„æ ¼çº§åˆ«"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“‹ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“è§„æ ¼")
        self.logger.info("="*60)
        
        failed_db = self._load_failed_specs()
        
        # è·å–å½“å‰å·²ç¼“å­˜çš„è§„æ ¼æ•°é‡
        existing_cache_count = self._get_cached_specs_count()
        self.logger.info(f"ğŸ“Š å½“å‰å·²ç¼“å­˜è§„æ ¼æ–‡ä»¶: {existing_cache_count} ä¸ª")
        
        # å¤±è´¥è®°å½•ç»Ÿè®¡
        if failed_db:
            tries_stats = {}
            for record in failed_db.values():
                tries = record.get('tries', 1)
                tries_stats[tries] = tries_stats.get(tries, 0) + 1
            
            self.logger.info(f"ğŸ“‹ å¤±è´¥è®°å½•ç»Ÿè®¡: å…± {len(failed_db)} ä¸ªå¤±è´¥URL")
            for tries in sorted(tries_stats.keys()):
                self.logger.info(f"   â€¢ å¤±è´¥ {tries} æ¬¡: {tries_stats[tries]} ä¸ªURL")
        else:
            self.logger.info("ğŸ“‹ å¤±è´¥è®°å½•: æ— å¤±è´¥è®°å½•")
        
        # build product list
        if retry_failed_only:
            self.logger.info("ğŸ”„ ä»…é‡è¯•æ¨¡å¼ï¼šåªå¤„ç†å¤±è´¥çš„äº§å“")
            product_urls = list(failed_db.keys())
            all_products = [{'product_url': u, 'leaf_code': failed_db[u].get('leaf','unknown')} for u in product_urls]
        else:
            # ä¼˜å…ˆå¤„ç†å¤±è´¥äº§å“ + æ™ºèƒ½è¿‡æ»¤
            self.logger.info("ğŸ” æ”¶é›†éœ€è¦å¤„ç†çš„äº§å“...")
            all_products = []
            skipped_cached = 0
            skipped_failed = 0
            
            # === æ–°å¢ï¼šä¼˜å…ˆæ·»åŠ å¤±è´¥çš„äº§å“è¿›è¡Œé‡è¯• ===
            priority_failed = 0
            failed_urls_added = set()  # è®°å½•å·²æ·»åŠ çš„å¤±è´¥URL
            for failed_url, failed_record in failed_db.items():
                # å¤±è´¥äº§å“ä¼˜å…ˆé‡è¯•ï¼Œä¸å—æ¬¡æ•°é™åˆ¶
                leaf_code = failed_record.get('leaf', 'unknown')
                all_products.append({
                    'product_url': failed_url, 
                    'leaf_code': leaf_code,
                    'is_retry': True,  # æ ‡è®°ä¸ºé‡è¯•äº§å“
                    'previous_tries': failed_record.get('tries', 0)
                })
                failed_urls_added.add(failed_url)
                priority_failed += 1
            
            if priority_failed > 0:
                self.logger.info(f"ğŸ“‹ ä¼˜å…ˆå¤„ç†å¤±è´¥äº§å“: {priority_failed} ä¸ª")
            
            for leaf in data['leaves']:
                leaf_code = leaf['code']
                
                for product_url in leaf.get('products', []):
                    if isinstance(product_url, str):
                        product_info = {'product_url': product_url, 'leaf_code': leaf_code}
                    else:
                        # å¤„ç†å­—å…¸æ ¼å¼çš„äº§å“ï¼ˆå¯èƒ½æ¥è‡ª SPECIFICATIONS çº§åˆ«çš„ç¼“å­˜ï¼‰
                        product_info = product_url.copy() if isinstance(product_url, dict) else {'product_url': str(product_url)}
                        product_info['leaf_code'] = leaf_code
                    
                    product_url_str = product_info['product_url']
                    
                    # 1. æ£€æŸ¥æ˜¯å¦å·²ç»æˆåŠŸç¼“å­˜
                    if self._is_product_cached(product_url_str, leaf_code):
                        skipped_cached += 1
                        continue
                    
                    # 2. å¦‚æœå·²ç»åœ¨å¤±è´¥åˆ—è¡¨ä¸­ï¼Œè·³è¿‡ï¼ˆå› ä¸ºå·²ç»åœ¨ä¸Šé¢ä¼˜å…ˆå¤„ç†äº†ï¼‰
                    if product_url_str in failed_urls_added:
                        skipped_failed += 1
                        continue
                    
                    # 3. æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                    all_products.append(product_info)
            
            # è®¡ç®—æ–°äº§å“æ•°é‡
            new_products = len(all_products) - priority_failed
            
            self.logger.info(f"ğŸ“‹ æ™ºèƒ½è¿‡æ»¤ç»“æœ:")
            self.logger.info(f"   â€¢ ä¼˜å…ˆé‡è¯•å¤±è´¥: {priority_failed} ä¸ª")
            self.logger.info(f"   â€¢ è·³è¿‡å·²ç¼“å­˜: {skipped_cached} ä¸ª")
            self.logger.info(f"   â€¢ è·³è¿‡é‡å¤å¤±è´¥: {skipped_failed} ä¸ª") 
            self.logger.info(f"   â€¢ æ–°äº§å“å¾…å¤„ç†: {new_products} ä¸ª")
            self.logger.info(f"   â€¢ éœ€è¦å¤„ç†æ€»è®¡: {len(all_products)} ä¸ª")

        # å¦‚æœæ²¡æœ‰äº§å“éœ€è¦å¤„ç†ï¼Œç›´æ¥è¿”å›
        if len(all_products) == 0:
            self.logger.info("âœ… æ‰€æœ‰äº§å“è§„æ ¼éƒ½å·²ç¼“å­˜ï¼Œæ— éœ€é‡æ–°çˆ¬å–")
            return data
        
        self.logger.info(f"å‡†å¤‡çˆ¬å– {len(all_products)} ä¸ªäº§å“çš„è§„æ ¼â€¦")
        
        # æ˜¾ç¤ºé¢„ä¼°èŠ‚çœçš„æ—¶é—´
        if not retry_failed_only and (skipped_cached > 0 or skipped_failed > 0):
            total_skipped = skipped_cached + skipped_failed
            estimated_time_saved = total_skipped * 15 / 60  # å‡è®¾æ¯ä¸ªäº§å“15ç§’ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿ
            self.logger.info(f"âš¡ æ™ºèƒ½è·³è¿‡èŠ‚çœé¢„ä¼°æ—¶é—´: {estimated_time_saved:.1f} åˆ†é’Ÿ")
        
        # æ¢å¤åŸç‰ˆçº¿ç¨‹æ± å¤„ç†ï¼Œä½†ä¿æŒå®æ—¶å†™å…¥ä¼˜åŒ–
        self.logger.info(f"å¼€å§‹å¹¶è¡Œæå–äº§å“è§„æ ¼ (çº¿ç¨‹æ•°: {min(len(all_products), self.max_workers)})")
        
        # å¤„ç†ç»“æœ
        product_specs = {}
        success_count = 0
        total_specs = 0
        processed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼Œä½†å®æ—¶å¤„ç†ç»“æœ
        with ThreadPoolExecutor(max_workers=min(len(all_products), self.max_workers)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_product = {
                executor.submit(self.specifications_crawler.extract_specifications, p['product_url'] if isinstance(p, dict) else p): p
                for p in all_products
            }
            
            # å®æ—¶å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_product):
                product_info = future_to_product[future]
                product_url = product_info['product_url'] if isinstance(product_info, dict) else product_info
                
                try:
                    result = future.result()
                except Exception as e:
                    self.logger.error(f"âŒ è§„æ ¼æå–å¼‚å¸¸: {e} | url={product_url}")
                    result = {
                        'product_url': product_url,
                        'specifications': [],
                        'count': 0,
                        'success': False,
                        'error': str(e)
                    }
                
                # ä»¥ä¸‹æ˜¯åŸæœ‰çš„å¤„ç†é€»è¾‘ï¼Œä½†ç°åœ¨æ˜¯å®æ—¶æ‰§è¡Œ
                specs = result.get('specifications', [])
                
                # è°ƒè¯•ï¼šæ‰“å°åŸå§‹æ•°æ®ç»“æ„
                if specs and len(specs) > 0:
                    self.logger.debug(f"ğŸ“Š è§„æ ¼æ•°æ®æ ·ä¾‹: {specs[0] if isinstance(specs[0], dict) else specs[:3]}")
                
                # æ–°å¢: è¯¦ç»†æ—¥å¿—ï¼Œè®°å½•æ¯ä¸ªäº§å“çš„è§„æ ¼æå–ç»“æœ
                retry_info = ""
                if isinstance(product_info, dict) and product_info.get('is_retry'):
                    retry_info = f" (é‡è¯•{product_info.get('previous_tries', 0)}æ¬¡)"
                
                self.logger.info(
                    f"ğŸ” è§„æ ¼æå–ç»“æœ | {'âœ… æˆåŠŸ' if specs else 'âš ï¸ æ— è§„æ ¼' if result.get('success') else 'âŒ å¤±è´¥'} | "
                    f"specs={len(specs)}{retry_info} | url={product_url}"
                )
                product_specs[product_url] = specs
                
                # è°ƒè¯•ï¼šå¦‚æœæ‰¾ä¸åˆ° product_info
                if not product_info:
                    self.logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°äº§å“ä¿¡æ¯: {product_url}")
                    self.logger.debug(f"   all_products æ ·æœ¬: {all_products[:2] if all_products else 'empty'}")
                
                if result.get('success', False):
                    success_count += 1
                    total_specs += len(specs)
                    if len(specs)==0:
                        # zero spec -> treat as failure record
                        rec = {
                            'url': product_url,
                            'leaf': product_info.get('leaf_code','unknown') if isinstance(product_info,dict) else 'unknown',
                            'reason': 'ZeroSpecifications',
                            'tries': failed_db.get(product_url,{}).get('tries',0)+1,
                            'ts': datetime.now().isoformat()
                        }
                        self._append_failed_spec(rec)
                    else:
                        # æˆåŠŸä¸”æœ‰è§„æ ¼æ•°æ®ï¼Œä»å¤±è´¥è®°å½•ä¸­ç§»é™¤
                        if product_url in failed_db:
                            prev_tries = failed_db[product_url].get('tries', 0)
                            self._remove_from_failed_specs(product_url)
                            self.logger.info(f"ğŸ‰ æˆåŠŸä¿®å¤ï¼å·²ä»å¤±è´¥è®°å½•ä¸­æ¸…ç†: {product_url} (ä¹‹å‰å¤±è´¥ {prev_tries} æ¬¡)")
                        else:
                            self.logger.debug(f"âœ… æ–°äº§å“æˆåŠŸæå–è§„æ ¼: {len(specs)} ä¸ª")
                else:
                    prev_tries = failed_db.get(product_url,{}).get('tries',0)
                    new_tries = prev_tries + 1
                    rec = {
                        'url': product_url,
                        'leaf': product_info.get('leaf_code','unknown') if isinstance(product_info,dict) else 'unknown',
                        'reason': result.get('error','Exception'),
                        'tries': new_tries,
                        'ts': datetime.now().isoformat()
                    }
                    
                    if product_url in failed_db:
                        self.logger.warning(f"âš ï¸ é‡è¯•ä»å¤±è´¥: {product_url} (ç¬¬ {new_tries} æ¬¡å¤±è´¥, åŸå› : {rec['reason']})")
                    else:
                        self.logger.warning(f"âŒ æ–°å¢å¤±è´¥è®°å½•: {product_url} (åŸå› : {rec['reason']})")
                    
                    self._append_failed_spec(rec)
                
                # END of failure branch

                # === æŒ‰äº§å“ç«‹å³å†™å…¥è§„æ ¼ç¼“å­˜æ–‡ä»¶ï¼ˆæˆåŠŸæˆ–å¤±è´¥å‡å°è¯•ï¼Œé¿å…é—æ¼ï¼‰ ===
                try:
                    import hashlib, json as _json
                    from urllib.parse import urlparse, parse_qs
                    
                    leaf_code_tmp = 'unknown'
                    if product_info and isinstance(product_info, dict):
                        leaf_code_tmp = product_info.get('leaf_code', 'unknown')
                    # è°ƒè¯•ï¼šç¡®ä¿æˆ‘ä»¬çŸ¥é“ leaf_code    
                    self.logger.debug(f"ğŸ“ äº§å“ {product_url[:50]}... -> leaf_code={leaf_code_tmp}")
                    url_hash_tmp = hashlib.md5(product_url.encode()).hexdigest()[:12]
                    base_name = f"{leaf_code_tmp}_{url_hash_tmp}"
                    
                    # ä»…åœ¨æˆåŠŸä¸”æ‹¿åˆ°è§„æ ¼æ—¶å†™å…¥æ–‡ä»¶ï¼Œé¿å…ç©ºæ–‡ä»¶å ä½
                    if specs:
                        # ç¡®ä¿ç›®å½•å­˜åœ¨
                        self.specs_cache_dir.mkdir(parents=True, exist_ok=True)
                        
                        # === ç”Ÿæˆå¤šæ ¼å¼è¾“å‡ºæ–‡ä»¶ï¼ˆæ¨¡ä»¿test-09-1ï¼‰ ===
                        
                        # 1. è§£æäº§å“URLè·å–åŸºç¡€ä¿¡æ¯
                        parsed_url = urlparse(product_url)
                        query_params = parse_qs(parsed_url.query)
                        product_name = parsed_url.path.split('/')[-1] if parsed_url.path else 'unknown'
                        
                        # 2. æ„é€ å®Œæ•´JSONæ ¼å¼
                        complete_data = {
                            "extraction_info": {
                                "timestamp": int(time.time()),
                                "extraction_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                "base_product_url": product_url,
                                "total_specifications_found": len(specs),
                                "leaf_code": leaf_code_tmp,
                                "source": "pipeline-v2"
                            },
                            "base_product_info": {
                                "base_url": f"{parsed_url.scheme}://{parsed_url.netloc}",
                                "base_path": parsed_url.path,
                                "base_product_name": product_name,
                                "catalog_path": query_params.get('CatalogPath', [''])[0],
                                "product_id": query_params.get('Product', [''])[0],
                                "query_params": query_params
                            },
                            "product_specifications": specs,
                            "summary": {
                                "series_distribution": {},
                                "specification_samples": []
                            }
                        }
                        
                        # 3. æ„é€ ç®€åŒ–JSONæ ¼å¼
                        simplified_data = {
                            "extraction_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            "total_count": len(specs),
                            "leaf_code": leaf_code_tmp,
                            "base_url": product_url,
                            "specifications": []
                        }
                        
                        # 4. æ„é€ URLåˆ—è¡¨æ–‡æœ¬
                        url_lines = [
                            f"# äº§å“è§„æ ¼é“¾æ¥åˆ—è¡¨",
                            f"# åŸºç¡€äº§å“: {product_url}",
                            f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                            f"# å¶èŠ‚ç‚¹: {leaf_code_tmp}",
                            f"# æ‰¾åˆ° {len(specs)} ä¸ªè§„æ ¼",
                            ""
                        ]
                        
                        # å¤„ç†æ¯ä¸ªè§„æ ¼
                        for i, spec in enumerate(specs, 1):
                            if isinstance(spec, dict):
                                ref = spec.get('reference', f'spec_{i}')
                                dims = spec.get('dimensions', '')
                                weight = spec.get('weight', '')
                                
                                # æ›´æ–°æ‘˜è¦ä¿¡æ¯
                                series = ref.split()[0] if ref else f'series_{i}'
                                complete_data["summary"]["series_distribution"][series] = complete_data["summary"]["series_distribution"].get(series, 0) + 1
                                
                                if i <= 10:  # åªä¿ç•™å‰10ä¸ªæ ·ä¾‹
                                    complete_data["summary"]["specification_samples"].append({
                                        "index": i,
                                        "reference": ref,
                                        "dimensions": dims,
                                        "weight": weight
                                    })
                                
                                # æ·»åŠ åˆ°ç®€åŒ–æ ¼å¼
                                simplified_data["specifications"].append({
                                    "id": i,
                                    "reference": ref,
                                    "dimensions": dims,
                                    "weight": weight,
                                    "series": series
                                })
                                
                                # æ·»åŠ åˆ°URLåˆ—è¡¨
                                url_lines.append(f"# {ref} ({dims})")
                                url_lines.append(f"# Spec {i}: {ref}")
                                url_lines.append("")
                        
                        # å†™å…¥ä¸‰ç§æ ¼å¼çš„æ–‡ä»¶
                        complete_path = self.specs_cache_dir / f"{base_name}_complete.json"
                        simplified_path = self.specs_cache_dir / f"{base_name}_list.json"
                        urls_path = self.specs_cache_dir / f"{base_name}_urls.txt"
                        
                        # ä¿å­˜å®Œæ•´JSON
                        with open(complete_path, 'w', encoding='utf-8') as f:
                            _json.dump(complete_data, f, ensure_ascii=False, indent=2)
                        
                        # ä¿å­˜ç®€åŒ–JSON
                        with open(simplified_path, 'w', encoding='utf-8') as f:
                            _json.dump(simplified_data, f, ensure_ascii=False, indent=2)
                        
                        # ä¿å­˜URLæ–‡æœ¬
                        with open(urls_path, 'w', encoding='utf-8') as f:
                            f.write('\n'.join(url_lines))
                        
                        # åŒæ—¶ä¿ç•™åŸå§‹æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                        cache_path_tmp = self.specs_cache_dir / f"{base_name}.json"
                        with open(cache_path_tmp, 'w', encoding='utf-8') as f:
                            _json.dump(specs, f, ensure_ascii=False, indent=2)
                        
                        self.logger.info(f"ğŸ’¾ å†™å…¥è§„æ ¼ç¼“å­˜æ–‡ä»¶: {base_name} ({len(specs)} specs, 4 formats)")
                        self.logger.debug(f"   â€¢ å®Œæ•´æ ¼å¼: {complete_path.name}")
                        self.logger.debug(f"   â€¢ ç®€åŒ–æ ¼å¼: {simplified_path.name}")
                        self.logger.debug(f"   â€¢ URLåˆ—è¡¨: {urls_path.name}")
                        self.logger.debug(f"   â€¢ åŸå§‹æ ¼å¼: {cache_path_tmp.name}")
                        
                        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„å†™å…¥
                        for file_path in [complete_path, simplified_path, urls_path, cache_path_tmp]:
                            if file_path.exists():
                                file_size = file_path.stat().st_size
                                self.logger.debug(f"âœ… {file_path.name}: {file_size} bytes")
                            else:
                                self.logger.error(f"âŒ æ–‡ä»¶å†™å…¥åä¸å­˜åœ¨ï¼{file_path}")
                    else:
                        self.logger.debug(f"âš ï¸ è·³è¿‡ç©ºè§„æ ¼: {product_url}")
                except Exception as _e:
                    self.logger.error(f"âŒ å†™å…¥è§„æ ¼ç¼“å­˜æ–‡ä»¶å¤±è´¥: {_e}", exc_info=True)
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_specifications(data, product_specs)
        
        # æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
        final_cache_count = self._get_cached_specs_count()
        newly_cached = final_cache_count - existing_cache_count
        
        # ç»Ÿè®¡ 
        self.logger.info(f"\nâœ… äº§å“è§„æ ¼æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†äº§å“: {len(all_products)} ä¸ª")
        self.logger.info(f"   â€¢ æˆåŠŸçˆ¬å–: {success_count} ä¸ª")
        self.logger.info(f"   â€¢ æ€»è§„æ ¼æ•°: {total_specs} ä¸ª")
        self.logger.info(f"   â€¢ æ–°å¢ç¼“å­˜æ–‡ä»¶: {newly_cached} ä¸ª")
        self.logger.info(f"   â€¢ å½“å‰æ€»ç¼“å­˜: {final_cache_count} ä¸ª")
        if len(all_products) > 0:
            success_rate = success_count / len(all_products) * 100
            self.logger.info(f"   â€¢ æœ¬æ¬¡æˆåŠŸç‡: {success_rate:.1f}%")
        
        # ä¿å­˜å¼‚å¸¸è®°å½•
        self._save_error_logs()
        
        return data

    def _crawl_products_serial(self, leaves: List[Dict]) -> Dict[str, List[str]]:
        """ä¸²è¡Œå¤„ç†å¶èŠ‚ç‚¹äº§å“é“¾æ¥ï¼ˆåŸå§‹æ–¹æ³•ï¼‰"""
        leaf_products = {}
        
        for i, leaf in enumerate(leaves, 1):
            self.logger.info(f"[{i}/{len(leaves)}] å¤„ç†å¶èŠ‚ç‚¹: {leaf['code']}")
            try:
                products = self._crawl_products_for_leaf(leaf)
                leaf_products[leaf['code']] = products
                self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=True)
                
                # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼ˆåŒ…å«URLï¼‰
                retry_info = ""
                if leaf.get('is_retry'):
                    retry_info = f" (é‡è¯•{leaf.get('previous_tries', 0)}æ¬¡)"
                
                if products:
                    self.logger.info(f"âœ… å¶èŠ‚ç‚¹ {leaf['code']} äº§å“æ•°: {len(products)}{retry_info}")
                    self.logger.info(f"   åœ°å€: {leaf['url']}")
                    
                    # æˆåŠŸè·å–äº§å“ï¼Œæ ‡è®°ä¸ºæˆåŠŸä¿®å¤ï¼ˆä¸‹æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä¸ä¼šé‡è¯•ï¼‰
                    if leaf.get('is_retry'):
                        prev_tries = leaf.get('previous_tries', 0)
                        self.logger.info(f"ğŸ‰ æˆåŠŸä¿®å¤ï¼å¶èŠ‚ç‚¹ {leaf['code']} (ä¹‹å‰å¤±è´¥ {prev_tries} æ¬¡)")
                else:
                    self.logger.warning(f"âš ï¸  å¶èŠ‚ç‚¹ {leaf['code']} æ— äº§å“{retry_info}")
                    self.logger.warning(f"   åœ°å€: {leaf['url']}")
                    
                    # è®°å½•é›¶äº§å“æƒ…å†µåˆ°é”™è¯¯æ—¥å¿—
                    self._record_error('products', {
                        'error_type': 'zero_products',
                        'leaf_code': leaf['code'],
                        'leaf_name': leaf.get('name', ''),
                        'leaf_url': leaf['url'],
                        'product_count': 0,
                        'note': 'é¡µé¢è®¿é—®æ­£å¸¸ä½†æœªæ‰¾åˆ°äº§å“'
                    })
                    
                    # å¤±è´¥ä¿¡æ¯å·²ç»é€šè¿‡ _record_error è®°å½•åˆ°é”™è¯¯æ—¥å¿—ä¸­
                    
            except Exception as e:
                retry_info = ""
                if leaf.get('is_retry'):
                    retry_info = f" (é‡è¯•{leaf.get('previous_tries', 0)}æ¬¡)"
                
                self.logger.error(f"å¶èŠ‚ç‚¹ {leaf['code']} å¤„ç†å¤±è´¥: {e}{retry_info}")
                self.logger.error(f"   åœ°å€: {leaf['url']}")
                
                # è®°å½•äº§å“é“¾æ¥çˆ¬å–å¤±è´¥åˆ°é”™è¯¯æ—¥å¿—
                self._record_error('products', {
                    'error_type': 'product_extraction_failed',
                    'leaf_code': leaf['code'],
                    'leaf_name': leaf.get('name', ''),
                    'leaf_url': leaf['url'],
                    'exception': str(e),
                    'exception_type': type(e).__name__
                })
                
                # å¤±è´¥ä¿¡æ¯å·²ç»é€šè¿‡ _record_error è®°å½•åˆ°é”™è¯¯æ—¥å¿—ä¸­
                
                leaf_products[leaf['code']] = []
                self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=False)
        
        return leaf_products

    def _crawl_products_parallel(self, leaves: List[Dict], max_processes: int = None) -> Dict[str, List[str]]:
        """å¹¶è¡Œå¤„ç†å¶èŠ‚ç‚¹äº§å“é“¾æ¥ï¼ˆè¿›ç¨‹æ± æ¨¡å¼ï¼‰"""
        import multiprocessing as mp
        from multiprocessing import Pool
        import time
        
        # åŠ¨æ€ç¡®å®šè¿›ç¨‹æ•°
        if max_processes is None:
            # æ ¹æ® --workers å‚æ•°åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
            # å¯¹äºäº§å“é“¾æ¥æŠ“å–ï¼Œä½¿ç”¨ä¸€åŠçš„ workersï¼ˆå› ä¸ºæ¯ä¸ªè¿›ç¨‹ä¼šå¯åŠ¨ Playwrightï¼‰
            # ä½†è‡³å°‘ä¿è¯æœ‰ 2 ä¸ªè¿›ç¨‹ï¼Œæœ€å¤šä¸è¶…è¿‡å¶èŠ‚ç‚¹æ•°é‡
            max_processes = min(
                max(self.max_workers // 2, 2),  # è‡³å°‘2ä¸ªè¿›ç¨‹ï¼Œæœ€å¤šæ˜¯ workers çš„ä¸€åŠ
                len(leaves),  # ä¸è¶…è¿‡å¶èŠ‚ç‚¹æ•°é‡
                min(self.max_workers, 16)  # ä¸è¶…è¿‡ workers è®¾ç½®ï¼Œä½†ä¹Ÿä¸è¶…è¿‡ 16ï¼ˆç³»ç»Ÿèµ„æºè€ƒè™‘ï¼‰
            )
        else:
            # ä½¿ç”¨è°ƒç”¨è€…æŒ‡å®šçš„è¿›ç¨‹æ•°ï¼Œä½†ä»è¦éµå®ˆåˆç†é™åˆ¶
            max_processes = min(max_processes, len(leaves), 16)
        
        # å‡†å¤‡å‚æ•°ï¼šä¸ºæ¯ä¸ªå¶èŠ‚ç‚¹åˆ›å»ºç‹¬ç«‹çš„å‚æ•°åŒ…
        leaf_args = []
        for leaf in leaves:
            # ä¼ é€’ç¼“å­˜æ£€æŸ¥éœ€è¦çš„å‚æ•°
            leaf_args.append({
                'leaf': leaf,
                'cache_dir': str(self.products_cache_dir),
                'cache_ttl_hours': self.cache_ttl[CacheLevel.PRODUCTS],
                'debug_mode': self.debug_mode
            })
        
        leaf_products = {}
        errors = []
        
        try:
            with Pool(processes=max_processes) as pool:
                self.logger.info(f"ğŸš€ å¯åŠ¨ {max_processes} ä¸ªè¿›ç¨‹å¤„ç† {len(leaves)} ä¸ªå¶èŠ‚ç‚¹...")
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                results = pool.map(_crawl_single_leaf_product_worker, leaf_args)
                
                # å¤„ç†ç»“æœ
                for result in results:
                    leaf_code = result['leaf_code']
                    products = result['products']
                    error_info = result.get('error_info')
                    
                    leaf_products[leaf_code] = products
                    
                    # è®°å½•é”™è¯¯ä¿¡æ¯
                    if error_info:
                        errors.append(error_info)
                        self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=False)
                    else:
                        self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=True)
                    
                    # æ˜¾ç¤ºç»“æœ
                    if products:
                        self.logger.info(f"âœ… å¶èŠ‚ç‚¹ {leaf_code} äº§å“æ•°: {len(products)}")
                    else:
                        self.logger.warning(f"âš ï¸ å¶èŠ‚ç‚¹ {leaf_code} æ— äº§å“")
                        
        except Exception as e:
            self.logger.error(f"âŒ å¹¶è¡Œå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¸²è¡Œæ¨¡å¼: {e}")
            return self._crawl_products_serial(leaves)
        
        # æ‰¹é‡è®°å½•é”™è¯¯
        for error_info in errors:
            self._record_error('products', error_info)
        
        self.logger.info(f"ğŸ å¹¶è¡Œå¤„ç†å®Œæˆ: {len(leaf_products)} ä¸ªå¶èŠ‚ç‚¹, {len(errors)} ä¸ªé”™è¯¯")
        
        return leaf_products

    def _crawl_products_for_leaf(self, leaf: Dict) -> List[str]:
        """ä¸ºå¶èŠ‚ç‚¹çˆ¬å–äº§å“é“¾æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        code = leaf['code']
        cache_file = self.products_cache_dir / f"{code}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.PRODUCTS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºï¼‰
                if products and len(products) > 0:
                    products = self._ensure_absolute_urls(products)
                    self.logger.info(f"ğŸ“¦ ä½¿ç”¨æœ‰æ•ˆç¼“å­˜: {code} ({len(products)} ä¸ªäº§å“)")
                    return products
                else:
                    self.logger.warning(f"âš ï¸ å‘ç°ç©ºç¼“å­˜: {code}ï¼Œå°†é‡æ–°çˆ¬å–")
        
        # çˆ¬å–æ–°æ•°æ®
        self.logger.info(f"ğŸŒ çˆ¬å–äº§å“: {code}")
        self.logger.info(f"ğŸ”— å¶èŠ‚ç‚¹URL: {leaf['url']}")
        try:
            # ä½¿ç”¨æ–°çš„v2æ¥å£ï¼ŒåŒ…å«è¿›åº¦ä¿¡æ¯
            products, progress_info = self.products_crawler.collect_all_product_links(leaf['url'])
            products = self._ensure_absolute_urls(products)
            # è®°å½•è¿›åº¦ä¿¡æ¯åˆ°æ—¥å¿—
            target_count = progress_info.get('target_count_on_page', 0)
            if target_count > 0:
                self.logger.info(f"ğŸ“Š æŠ“å–å®Œæˆåº¦: {progress_info['progress_percentage']}% ({progress_info['extracted_count']}/{target_count})")
            
            # è®°å½•ç©ºäº§å“åˆ—è¡¨çš„æƒ…å†µï¼ˆæ²¡æœ‰å¼‚å¸¸ä½†ç»“æœä¸ºç©ºï¼‰
            if not products:
                self._record_error('products', {
                    'error_type': 'zero_products_no_exception',
                    'leaf_code': code,
                    'leaf_name': leaf.get('name', ''),
                    'leaf_url': leaf['url'],
                    'product_count': 0,
                    'note': 'çˆ¬å–å®Œæˆä½†è¿”å›ç©ºäº§å“åˆ—è¡¨'
                })
            
            # ä¿å­˜ç¼“å­˜ï¼ˆç¡®ä¿URLæ˜¯ç»å¯¹è·¯å¾„ï¼‰
            products_to_save = [link if link.startswith("http") else f"https://www.traceparts.cn{link}" for link in products]
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(products_to_save, f, ensure_ascii=False, indent=2)
            return products
        except Exception as e:
            self.logger.error(f"âŒ å¤±è´¥: {code} - {e}")
            
            # è®°å½•çˆ¬å–å¼‚å¸¸
            self._record_error('products', {
                'error_type': 'product_extraction_exception',
                'leaf_code': code,
                'leaf_name': leaf.get('name', ''),
                'leaf_url': leaf['url'],
                'exception': str(e),
                'exception_type': type(e).__name__,
                'note': 'äº§å“é“¾æ¥çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸'
            })
            
            return []
    
    def _crawl_specs_for_product(self, product: Any) -> List[Dict]:
        """ä¸ºäº§å“çˆ¬å–è§„æ ¼ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if isinstance(product, dict):
            product_url = product['product_url']
            leaf_code = product.get('leaf_code', 'unknown')
        else:
            product_url = product
            leaf_code = 'unknown'
        
        # ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆä½¿ç”¨URLçš„hashï¼‰
        import hashlib
        url_hash = hashlib.md5(product_url.encode()).hexdigest()[:12]
        cache_file = self.specs_cache_dir / f"{leaf_code}_{url_hash}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.SPECIFICATIONS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    specs = json.load(f)
                return specs
        
        # çˆ¬å–æ–°æ•°æ®
        try:
            result = self.specifications_crawler.extract_specifications(product_url)
            specs = result.get('specifications', [])
            # ä¿å­˜ç¼“å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(specs, f, ensure_ascii=False, indent=2)
            return specs
        except Exception as e:
            self.logger.error(f"è§„æ ¼çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def _update_tree_with_products(self, data: Dict, leaf_products: Dict[str, List[str]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“é“¾æ¥"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                code = node.get('code', '')
                products = leaf_products.get(code, [])
                node['products'] = products
                node['product_count'] = len(products)
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘ï¼ˆå¦‚æœå­˜åœ¨rootï¼‰
        if 'root' in data:
            update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            code = leaf['code']
            products = leaf_products.get(code, [])
            leaf['products'] = products
            leaf['product_count'] = len(products)
    
    def _update_tree_with_specifications(self, data: Dict, product_specs: Dict[str, List[Dict]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“è§„æ ¼"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                # æ›´æ–°äº§å“åˆ—è¡¨æ ¼å¼
                updated_products = []
                for product in node.get('products', []):
                    if isinstance(product, str):
                        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        product_info = {
                            'product_url': product,
                            'specifications': product_specs.get(product, []),
                            'spec_count': len(product_specs.get(product, []))
                        }
                    else:
                        # æ›´æ–°ç°æœ‰å­—å…¸
                        product['specifications'] = product_specs.get(product['product_url'], [])
                        product['spec_count'] = len(product['specifications'])
                        product_info = product
                    updated_products.append(product_info)
                node['products'] = updated_products
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘ï¼ˆå¦‚æœå­˜åœ¨rootï¼‰
        if 'root' in data:
            update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            updated_products = []
            for product in leaf.get('products', []):
                if isinstance(product, str):
                    product_info = {
                        'product_url': product,
                        'specifications': product_specs.get(product, []),
                        'spec_count': len(product_specs.get(product, []))
                    }
                else:
                    product['specifications'] = product_specs.get(product['product_url'], [])
                    product['spec_count'] = len(product['specifications'])
                    product_info = product
                updated_products.append(product_info)
            leaf['products'] = updated_products
    
    def run_progressive_cache(self, target_level: CacheLevel = CacheLevel.SPECIFICATIONS, force_refresh: bool = False, retry_failed_only: bool = False):
        """è¿è¡Œæ¸è¿›å¼ç¼“å­˜æ„å»º"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ TraceParts æ¸è¿›å¼ç¼“å­˜ç³»ç»Ÿ")
        self.logger.info("="*60)
        
        # è·å–å½“å‰ç¼“å­˜çº§åˆ«
        current_level, data = self.get_cache_level()
        
        if force_refresh:
            self.logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°æ„å»ºæ‰€æœ‰ç¼“å­˜")
            current_level = CacheLevel.NONE
            data = None
        else:
            self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜çº§åˆ«: {current_level.name}")
            self.logger.info(f"ğŸ¯ ç›®æ ‡ç¼“å­˜çº§åˆ«: {target_level.name}")
        
        # é€çº§æ„å»ºç¼“å­˜
        if current_level.value < CacheLevel.CLASSIFICATION.value:
            self.logger.info("\n[é˜¶æ®µ 1/3] æ„å»ºåˆ†ç±»æ ‘ç¼“å­˜")
            self.logger.info("-" * 50)
            
            root, leaves = self.classification_crawler.crawl_full_tree_enhanced()
            data = {'root': root, 'leaves': leaves}
            self.save_cache(data, CacheLevel.CLASSIFICATION)
            current_level = CacheLevel.CLASSIFICATION
            
            if target_level == CacheLevel.CLASSIFICATION:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        
        if current_level.value < CacheLevel.PRODUCTS.value and target_level.value >= CacheLevel.PRODUCTS.value:
            self.logger.info("\n[é˜¶æ®µ 2/3] æ‰©å±•äº§å“é“¾æ¥ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_products(data)
            self.save_cache(data, CacheLevel.PRODUCTS)
            current_level = CacheLevel.PRODUCTS
            
            if target_level == CacheLevel.PRODUCTS:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        elif current_level.value >= CacheLevel.PRODUCTS.value and target_level.value >= CacheLevel.PRODUCTS.value:
            # å³ä½¿å·²ç»åˆ°è¾¾PRODUCTSçº§åˆ«ï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„äº§å“é“¾æ¥éœ€è¦é‡è¯•
            failed_products_db = self._load_failed_products_from_error_logs(max_retry_times=3)
            if failed_products_db:
                self.logger.info("\n[é˜¶æ®µ 2/3] é‡æ–°å¤„ç†å¤±è´¥çš„äº§å“é“¾æ¥")
                self.logger.info("-" * 50)
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ° {len(failed_products_db)} ä¸ªå¤±è´¥çš„å¶èŠ‚ç‚¹ï¼Œéœ€è¦é‡æ–°çˆ¬å–äº§å“é“¾æ¥")
                
                data = self.extend_to_products(data)
                self.save_cache(data, CacheLevel.PRODUCTS)
                
                if target_level == CacheLevel.PRODUCTS:
                    self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                    return data
        
        if current_level.value < CacheLevel.SPECIFICATIONS.value and target_level.value >= CacheLevel.SPECIFICATIONS.value:
            self.logger.info("\n[é˜¶æ®µ 3/3] æ‰©å±•äº§å“è§„æ ¼ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_specifications(data, retry_failed_only=retry_failed_only)
            self.save_cache(data, CacheLevel.SPECIFICATIONS)
            
            self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
        
        return data

    # === å¤±è´¥è§„æ ¼å¢é‡è®°å½• ===
    def _load_failed_specs(self) -> Dict[str, Dict]:
        """åŠ è½½å¤±è´¥è§„æ ¼è®°å½•ï¼Œè¿”å› url->record å­—å…¸"""
        failed = {}
        if self.failed_specs_file.exists():
            with open(self.failed_specs_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        rec = json.loads(line.strip())
                        failed[rec.get('url')] = rec
                    except:
                        continue
        return failed

    def _append_failed_spec(self, record: Dict):
        """çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°å¤±è´¥è®°å½•ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰"""
        with self.failed_lock:
            # è¯»å–ç°æœ‰è®°å½•
            existing_records = {}
            if self.failed_specs_file.exists():
                try:
                    with open(self.failed_specs_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            try:
                                existing_record = json.loads(line.strip())
                                url = existing_record.get('url')
                                if url:
                                    existing_records[url] = existing_record
                            except:
                                continue
                except Exception as e:
                    self.logger.warning(f"è¯»å–å¤±è´¥è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
            
            # æ›´æ–°æˆ–æ·»åŠ æ–°è®°å½•
            url = record.get('url')
            if url:
                existing_records[url] = record
            
            # é‡å†™æ•´ä¸ªæ–‡ä»¶ä»¥ç¡®ä¿å”¯ä¸€æ€§
            try:
                with open(self.failed_specs_file, 'w', encoding='utf-8') as f:
                    for record_data in existing_records.values():
                        f.write(json.dumps(record_data, ensure_ascii=False) + "\n")
            except Exception as e:
                self.logger.error(f"å†™å…¥å¤±è´¥è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
                # å¦‚æœå†™å…¥å¤±è´¥ï¼Œå°è¯•è¿½åŠ æ¨¡å¼ä½œä¸ºå¤‡ä»½
                try:
                    with open(self.failed_specs_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(record, ensure_ascii=False) + "\n")
                except:
                    pass
    
    def _is_product_cached(self, product_url: str, leaf_code: str = None) -> bool:
        """æ£€æŸ¥äº§å“è§„æ ¼æ˜¯å¦å·²ç»ç¼“å­˜"""
        try:
            import hashlib
            
            # å¦‚æœæ²¡æœ‰leaf_codeï¼Œå°è¯•ä»URLæ¨æ–­æˆ–ä½¿ç”¨unknown
            if not leaf_code:
                leaf_code = 'unknown'
            
            # ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
            url_hash = hashlib.md5(product_url.encode()).hexdigest()[:12]
            base_name = f"{leaf_code}_{url_hash}"
            
            # æ£€æŸ¥å¤šç§æ ¼å¼çš„ç¼“å­˜æ–‡ä»¶ï¼ˆä¼˜å…ˆæ£€æŸ¥æ–°æ ¼å¼ï¼‰
            cache_files_to_check = [
                self.specs_cache_dir / f"{base_name}_complete.json",  # æ–°æ ¼å¼ï¼šå®Œæ•´JSON
                self.specs_cache_dir / f"{base_name}.json",           # åŸå§‹æ ¼å¼ï¼šå…¼å®¹æ€§
            ]
            
            for cache_file in cache_files_to_check:
                if cache_file.exists():
                    file_size = cache_file.stat().st_size
                    if file_size > 10:  # è‡³å°‘10å­—èŠ‚ï¼Œé¿å…ç©ºæ–‡ä»¶
                        # å¿«é€ŸéªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                        try:
                            with open(cache_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                
                                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…è§„æ ¼æ•°æ®
                                if cache_file.name.endswith('_complete.json'):
                                    # æ–°æ ¼å¼ï¼šæ£€æŸ¥ product_specifications å­—æ®µ
                                    specs = data.get('product_specifications', [])
                                    if isinstance(specs, list) and len(specs) > 0:
                                        self.logger.debug(f"âœ… æ‰¾åˆ°æ–°æ ¼å¼ç¼“å­˜: {cache_file.name} ({len(specs)} specs)")
                                        return True
                                else:
                                    # åŸå§‹æ ¼å¼ï¼šç›´æ¥æ£€æŸ¥æ•°æ®
                                    if isinstance(data, list) and len(data) > 0:
                                        self.logger.debug(f"âœ… æ‰¾åˆ°åŸæ ¼å¼ç¼“å­˜: {cache_file.name} ({len(data)} specs)")
                                        return True
                        except:
                            # å¦‚æœæ–‡ä»¶æŸåï¼Œè®¤ä¸ºæœªç¼“å­˜
                            self.logger.debug(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œå°†é‡æ–°çˆ¬å–: {cache_file}")
                            continue
            
            return False
            
        except Exception as e:
            self.logger.debug(f"æ£€æŸ¥ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _remove_from_failed_specs(self, product_url: str):
        """ä»å¤±è´¥è®°å½•ä¸­ç§»é™¤æˆåŠŸçš„äº§å“"""
        try:
            if not self.failed_specs_file.exists():
                return
            
            # è¯»å–æ‰€æœ‰è®°å½•
            failed_records = []
            with open(self.failed_specs_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line.strip())
                        # åªä¿ç•™ä¸æ˜¯å½“å‰URLçš„è®°å½•
                        if record.get('url') != product_url:
                            failed_records.append(record)
                    except:
                        continue
            
            # é‡å†™æ–‡ä»¶
            with self.failed_lock:
                with open(self.failed_specs_file, 'w', encoding='utf-8') as f:
                    for record in failed_records:
                        f.write(json.dumps(record, ensure_ascii=False) + "\n")
                        
            self.logger.debug(f"âœ… å·²ä»å¤±è´¥è®°å½•ä¸­ç§»é™¤: {product_url}")
            
        except Exception as e:
            self.logger.warning(f"ç§»é™¤å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
    
    def _get_cached_specs_count(self) -> int:
        """è·å–å·²ç¼“å­˜çš„è§„æ ¼æ–‡ä»¶æ•°é‡ï¼ˆæŒ‰äº§å“è®¡ç®—ï¼Œä¸æŒ‰æ–‡ä»¶æ ¼å¼ï¼‰"""
        try:
            if not self.specs_cache_dir.exists():
                return 0
            
            # ç»Ÿè®¡å”¯ä¸€çš„äº§å“ï¼ˆé€šè¿‡base_nameå»é‡ï¼‰
            base_names = set()
            
            # æ£€æŸ¥å®Œæ•´æ ¼å¼æ–‡ä»¶
            for complete_file in self.specs_cache_dir.glob("*_complete.json"):
                base_name = complete_file.name.replace('_complete.json', '')
                base_names.add(base_name)
            
            # æ£€æŸ¥åŸå§‹æ ¼å¼æ–‡ä»¶ï¼ˆæ’é™¤å·²æœ‰å®Œæ•´æ ¼å¼çš„ï¼‰
            for json_file in self.specs_cache_dir.glob("*.json"):
                if not json_file.name.endswith(('_complete.json', '_list.json')):
                    base_name = json_file.name.replace('.json', '')
                    base_names.add(base_name)
            
            return len(base_names)
        except:
            return 0
    
    def _cleanup_duplicate_failed_specs(self):
        """æ¸…ç†é‡å¤çš„å¤±è´¥è®°å½•ï¼ˆåˆå§‹åŒ–æ—¶æ‰§è¡Œï¼‰"""
        if not self.failed_specs_file.exists():
            return
        
        try:
            # è¯»å–æ‰€æœ‰è®°å½•ï¼ŒæŒ‰URLå»é‡
            unique_records = {}
            total_lines = 0
            
            with open(self.failed_specs_file, 'r', encoding='utf-8') as f:
                for line in f:
                    total_lines += 1
                    try:
                        record = json.loads(line.strip())
                        url = record.get('url')
                        if url:
                            # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒæ—¶é—´æˆ³ï¼Œä¿ç•™æœ€æ–°çš„
                            if url in unique_records:
                                existing_ts = unique_records[url].get('ts', '')
                                new_ts = record.get('ts', '')
                                if new_ts > existing_ts:
                                    unique_records[url] = record
                            else:
                                unique_records[url] = record
                    except:
                        continue
            
            # å¦‚æœæœ‰é‡å¤ï¼Œé‡å†™æ–‡ä»¶
            if len(unique_records) < total_lines:
                duplicate_count = total_lines - len(unique_records)
                self.logger.info(f"ğŸ§¹ æ¸…ç†å¤±è´¥è®°å½•é‡å¤é¡¹: {duplicate_count} ä¸ªï¼Œä¿ç•™ {len(unique_records)} ä¸ªå”¯ä¸€è®°å½•")
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_file = self.failed_specs_file.with_suffix('.jsonl.backup')
                if backup_file.exists():
                    backup_file.unlink()  # åˆ é™¤æ—§å¤‡ä»½
                self.failed_specs_file.rename(backup_file)
                
                # é‡å†™å»é‡åçš„è®°å½•
                with open(self.failed_specs_file, 'w', encoding='utf-8') as f:
                    for record in unique_records.values():
                        f.write(json.dumps(record, ensure_ascii=False) + "\n")
                        
        except Exception as e:
            self.logger.warning(f"æ¸…ç†é‡å¤å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
    
    def close(self):
        """å…³é—­ç¼“å­˜ç®¡ç†å™¨ï¼Œæ¸…ç†èµ„æº"""
        self.logger.info("ğŸ›‘ å…³é—­ç¼“å­˜ç®¡ç†å™¨...")
        
        # æ¸…ç†è§„æ ¼çˆ¬å–å™¨èµ„æºï¼ˆå¦‚æœéœ€è¦ï¼‰
        # åŸç‰ˆè§„æ ¼çˆ¬å–å™¨ä¸éœ€è¦ç‰¹æ®Šå…³é—­ï¼Œè¿™é‡Œé¢„ç•™ç»™å°†æ¥æ‰©å±•
        
        self.logger.info("âœ… ç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    # ----- New Method for Single URL Testing -----
    def run_single_url_test(self, test_url: str, target_level: CacheLevel, force_refresh: bool = False) -> Dict[str, Any]:
        """Process a single URL through the caching stages for testing purposes."""
        self.logger.info(f"ğŸ§ª [CacheManager] Running single URL test for: {test_url} (Target: {target_level.name}, Refresh: {force_refresh})")
        
        tp_code = self._extract_tp_code_from_url(test_url)
        if not tp_code:
            self.logger.error(f"âŒ [CacheManager] Could not extract TP_CODE from test URL: {test_url}")
            return {"error": "Could not extract TP_CODE", "url": test_url, "tp_code": None}
        
        node_name = f"TestNode-{tp_code}"
        node = {
            'url': test_url,
            'name': node_name,
            'code': tp_code,
            'level': 4, 
            'children': [],
            'is_leaf': None, 
            'is_potential_leaf': True, 
            'products_on_page': 0,
            # Construct paths similar to how they would be for actual caching, though we might not save all steps
            'product_links_file': self.products_cache_dir / f"{tp_code}_products.json",
            'specification_dir': self.specs_cache_dir / tp_code
        }
        self.logger.info(f"ğŸ§¬ [CacheManager] Created test node: Code={tp_code}, Name={node_name}")

        test_results = {
            "url": test_url,
            "tp_code": tp_code,
            "node_name": node_name,
            "stages_processed": [],
            "is_leaf_node": None,
            "target_product_count": 0,
            "product_links_count": 0,
            "product_links_data": None, # To store actual links if fetched
            "specification_summary": None
        }

        # Stage 1: Classification (Leaf Node Verification)
        self.logger.info(f"â¡ï¸ [CacheManager] Stage 1: Verifying leaf status for {tp_code}")
        self.classification_crawler.debug_mode = self.debug_mode
        is_leaf_status, target_count, details_dict = self.classification_crawler._check_single_leaf_node(node)
        node['is_leaf'] = is_leaf_status
        node['products_on_page'] = target_count # This is the target_count from classification
        test_results["is_leaf_node"] = is_leaf_status
        test_results["target_product_count"] = target_count
        test_results["stages_processed"].append("classification_check")
        self.logger.info(f"   ğŸ“„ Leaf status: {is_leaf_status}, Target products (from classification): {target_count}")

        if not is_leaf_status:
            self.logger.info(f"ğŸ›‘ [CacheManager] Node {tp_code} is not a leaf. Test processing for this URL ends here.")
            return test_results

        # Stage 2: Product Links
        fetched_links_data = None
        if target_level.value >= CacheLevel.PRODUCTS.value:
            self.logger.info(f"â¡ï¸ [CacheManager] Stage 2: Fetching product links for {tp_code} (Force refresh: {force_refresh})")
            try:
                # The method in ultimate_products_v2.py is collect_all_product_links(self, leaf_url: str, tp_code: str, ...)
                # It does not take current_target_count; it determines target_count internally.
                fetched_links_data_tuple = self.products_crawler.collect_all_product_links(
                    leaf_url=node['url'], 
                    tp_code=node['code']
                    # Removed current_target_count and force_refresh from this direct call
                    # force_refresh for the product links stage in a single test would imply that
                    # collect_all_product_links itself should not use any of its own potential caching,
                    # which is usually the case as it fetches live data.
                )
                
                # collect_all_product_links returns a tuple: (links_list, progress_info_dict)
                if fetched_links_data_tuple and isinstance(fetched_links_data_tuple, tuple) and len(fetched_links_data_tuple) == 2:
                    links_list, progress_info_dict = fetched_links_data_tuple
                    fetched_links_data = {
                        "leaf_url": node['url'],
                        "tp_code": node['code'],
                        "links": links_list,
                        "progress_info": progress_info_dict,
                        "timestamp": progress_info_dict.get("timestamp", time.strftime("%Y-%m-%d %H:%M:%S"))
                    }
                    test_results["product_links_count"] = len(links_list)
                    test_results["product_links_data"] = fetched_links_data
                    
                    if "target_count_on_page" in progress_info_dict:
                        # Update the overall test_results target_product_count if product crawler found one
                        # This might differ from the one found by classification_crawler if logic varies
                        test_results["target_product_count"] = progress_info_dict["target_count_on_page"]
                        self.logger.info(f"   ğŸ¯ Target products (from product links crawler): {progress_info_dict['target_count_on_page']}")

                    self.logger.info(f"   ğŸ”— Product links fetched: {test_results['product_links_count']}")
                else:
                    self.logger.warning(f"   âš ï¸ No product links fetched for {tp_code} or data was empty/malformed. Response from crawler: {fetched_links_data_tuple}")
                    fetched_links_data = None # Ensure it's None if fetching failed

            except Exception as e_prod:
                self.logger.error(f"   âŒ Error fetching product links for {tp_code}: {e_prod}", exc_info=self.debug_mode)
            test_results["stages_processed"].append("product_links_fetch")
        else:
            self.logger.info(f"âšªï¸ [CacheManager] Stage 2: Product links (skipped by target_level: {target_level.name})")

        # Stage 3: Specifications
        if target_level.value >= CacheLevel.SPECIFICATIONS.value:
            if fetched_links_data and fetched_links_data.get('links'):
                product_links_for_specs = [
                    f"https://www.traceparts.cn{link}" if link.startswith('/') else link
                    for link in fetched_links_data['links']
                ]
                self.logger.info(f"â¡ï¸ [CacheManager] Stage 3: Fetching specifications for {tp_code} (Products: {len(product_links_for_specs)}, Force refresh: {force_refresh})")
                try:
                    # Changed to use extract_batch_specifications
                    # The method returns a dict like: {'results': [], 'summary': {}}
                    specs_data_dict = self.specifications_crawler.extract_batch_specifications(
                        product_urls=product_links_for_specs
                        # force_refresh is not directly passed; it's handled by --no-cache in Makefile for the crawler
                    )
                    
                    # Store the summary or a relevant part of specs_data_dict
                    test_results["specification_summary"] = {
                        "processed_count": len(product_links_for_specs),
                        "successful_count": specs_data_dict.get('summary', {}).get('success_cnt', 0),
                        "total_specs_found": specs_data_dict.get('summary', {}).get('total_specs', 0),
                        # Optionally, store all detailed results if small enough or needed for debug
                        # "details": specs_data_dict.get('results', []) 
                    }
                    test_results["stages_processed"].append("specifications_fetch")
                    self.logger.info(f"   ğŸ”© Specifications fetched for {tp_code}: {test_results['specification_summary']}")
                except Exception as e:
                    self.logger.error(f"âŒ Error fetching specifications for {tp_code}: {e}", exc_info=self.debug_mode)
                    test_results["specification_summary"] = {"error": str(e)}
            else:
                self.logger.info(f"âšªï¸ [CacheManager] Stage 3: Specifications (skipped, no product links found for {tp_code})")
        else:
             self.logger.info(f"âšªï¸ [CacheManager] Stage 3: Specifications (skipped by target_level: {target_level.name})")

        self.logger.info(f"ğŸ [CacheManager] Single URL test finished for {tp_code}. Results: {test_results}")
        return test_results

    def _extract_tp_code_from_url(self, url: str) -> Optional[str]:
        """Extracts the TP code from a URL's CatalogPath query parameter."""
        from urllib.parse import urlparse, parse_qs
        try:
            qs_part = urlparse(url).query
            params = parse_qs(qs_part)
            cp = params.get('CatalogPath', [''])[0]
            if cp.startswith('TRACEPARTS:'):
                cp = cp.split(':',1)[1]
            return cp if cp else None
        except Exception as e:
            self.logger.error(f"Error extracting TP_CODE from URL '{url}': {e}")
            return None

    def _get_cache_path(self, tp_code: str, level: CacheLevel, is_dir: bool = False) -> Path:
        """Returns the cache path for a given TP code and level."""
        # Implementation of _get_cache_path method
        pass

    def _classification_crawler(self):
        """Returns the classification crawler."""
        # Implementation of _classification_crawler method
        pass

    def _product_links_crawler(self):
        """Returns the product links crawler."""
        # Implementation of _product_links_crawler method
        pass

    def _spec_crawler(self):
        """Returns the specification crawler."""
        # Implementation of _spec_crawler method
        pass

    def _debug_mode(self):
        """Returns the debug mode."""
        # Implementation of _debug_mode method
        pass

    def _cache_base_dir(self):
        """Returns the cache base directory."""
        # Implementation of _cache_base_dir method
        pass

    def _classification_crawler(self):
        """Returns the classification crawler."""
        # Implementation of _classification_crawler method
        pass

    def _product_links_crawler(self):
        """Returns the product links crawler."""
        # Implementation of _product_links_crawler method
        pass

    def _spec_crawler(self):
        """Returns the specification crawler."""
        # Implementation of _spec_crawler method
        pass

    def _debug_mode(self):
        """Returns the debug mode."""
        # Implementation of _debug_mode method
        pass

    def _cache_base_dir(self):
        """Returns the cache base directory."""
        # Implementation of _cache_base_dir method
        pass 

    def _ensure_absolute_urls(self, links, base="https://www.traceparts.cn"):
        return [link if link.startswith("http") else base + link for link in links]
    
    # === å¤±è´¥äº§å“å¢é‡è®°å½•ç®¡ç†ï¼ˆä½¿ç”¨ç°æœ‰é”™è¯¯æ—¥å¿—ç³»ç»Ÿï¼‰===
    def _load_failed_products_from_error_logs(self, max_retry_times: int = 3) -> Dict[str, Dict]:
        """ä»é”™è¯¯æ—¥å¿—ä¸­åŠ è½½å¤±è´¥äº§å“è®°å½•ï¼Œè‡ªåŠ¨éªŒè¯å’Œå‰”é™¤å·²ä¿®å¤çš„è®°å½•"""
        failed_products = {}
        
        if not self.error_logs_dir.exists():
            return failed_products
        
        # æ‰¾åˆ°æœ€æ–°çš„é”™è¯¯æ—¥å¿—æ–‡ä»¶
        error_log_files = list(self.error_logs_dir.glob('error_log_v*.json'))
        if not error_log_files:
            return failed_products
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œå–æœ€æ–°çš„
        latest_error_log = sorted(error_log_files, key=lambda x: x.name)[-1]
        
        try:
            with open(latest_error_log, 'r', encoding='utf-8') as f:
                error_data = json.load(f)
            
            product_errors = error_data.get('details', {}).get('products', [])
            
            # ç»Ÿè®¡æ¯ä¸ªå¶èŠ‚ç‚¹çš„å¤±è´¥æ¬¡æ•°ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
            failure_counts = {}
            for error in product_errors:
                leaf_code = error.get('leaf_code')
                if leaf_code:
                    failure_counts[leaf_code] = failure_counts.get(leaf_code, 0) + 1
            
            # æ”¶é›†å¯é‡è¯•çš„å¤±è´¥è®°å½•
            candidate_failures = []
            for error in product_errors:
                leaf_code = error.get('leaf_code')
                error_type = error.get('error_type', '')
                
                if leaf_code and failure_counts.get(leaf_code, 0) <= max_retry_times:
                    if error_type in ['zero_products_no_exception', 'product_extraction_failed', 'zero_products']:
                        candidate_failures.append({
                            'leaf_code': leaf_code,
                            'leaf_name': error.get('leaf_name', ''),
                            'leaf_url': error.get('leaf_url', ''),
                            'error_type': error_type,
                            'tries': failure_counts[leaf_code],
                            'last_timestamp': error.get('timestamp', ''),
                            'reason': error.get('note', error.get('exception', 'æœªçŸ¥åŸå› '))
                        })
            
            # æ™ºèƒ½éªŒè¯ï¼šå¹¶è¡Œæ£€æŸ¥å¤±è´¥è®°å½•æ˜¯å¦å·²ä¿®å¤ï¼Œå¹¶æ›´æ–°é”™è¯¯æ—¥å¿—
            verified_failures = {}
            recovered_codes = []
            
            if candidate_failures:
                self.logger.info(f"ğŸ” å¹¶è¡Œæ™ºèƒ½éªŒè¯å¤±è´¥è®°å½•...")
                sample_size = min(20, len(candidate_failures))  # å¢åŠ åˆ°å‰20ä¸ªï¼Œå¹¶è¡Œå¤„ç†é€Ÿåº¦å¿«
                
                # å¹¶è¡ŒéªŒè¯ç¼“å­˜æ–‡ä»¶
                verification_results = self._parallel_verify_cache_files(candidate_failures[:sample_size])
                
                for failure, (is_recovered, product_count) in zip(candidate_failures[:sample_size], verification_results):
                    leaf_code = failure['leaf_code']
                    
                    if is_recovered:
                        self.logger.info(f"âœ… æ£€æµ‹åˆ°å·²ä¿®å¤: {leaf_code} (ç°æœ‰ {product_count} ä¸ªäº§å“)")
                        recovered_codes.append(leaf_code)
                    else:
                        # ä»ç„¶æ˜¯å¤±è´¥çŠ¶æ€
                        verified_failures[leaf_code] = failure
                
                # å¯¹äºæœªéªŒè¯çš„å¤±è´¥è®°å½•ï¼Œç›´æ¥åŠ å…¥ï¼ˆé¿å…éªŒè¯æ—¶é—´è¿‡é•¿ï¼‰
                for failure in candidate_failures[sample_size:]:
                    verified_failures[failure['leaf_code']] = failure
                
                # å¦‚æœæœ‰å·²ä¿®å¤çš„è®°å½•ï¼Œæ›´æ–°é”™è¯¯æ—¥å¿—æ–‡ä»¶
                if recovered_codes:
                    self._update_error_log_file(latest_error_log, error_data, recovered_codes)
            
            failed_products = verified_failures
            
            self.logger.info(f"ğŸ“Š ä» {latest_error_log.name} æ™ºèƒ½åŠ è½½å¤±è´¥äº§å“è®°å½•")
            self.logger.info(f"   â€¢ æ€»é”™è¯¯è®°å½•: {len(product_errors)} ä¸ª")
            self.logger.info(f"   â€¢ å€™é€‰é‡è¯•: {len(candidate_failures)} ä¸ª")
            self.logger.info(f"   â€¢ å¹¶è¡ŒéªŒè¯: {min(20, len(candidate_failures))} ä¸ª")
            self.logger.info(f"   â€¢ è‡ªåŠ¨å‰”é™¤: {len(recovered_codes)} ä¸ª")
            self.logger.info(f"   â€¢ ç¡®è®¤å¤±è´¥: {len(failed_products)} ä¸ª")
            self.logger.info(f"   â€¢ è¶…é™è·³è¿‡: {len([k for k, v in failure_counts.items() if v > max_retry_times])} ä¸ª")
            
        except Exception as e:
            self.logger.error(f"è¯»å–é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")
        
        return failed_products

    def _is_leaf_in_current_error_batch(self, leaf_code: str) -> bool:
        """æ£€æŸ¥å¶èŠ‚ç‚¹æ˜¯å¦åœ¨å½“å‰æ‰¹æ¬¡çš„é”™è¯¯è®°å½•ä¸­ï¼ˆé¿å…é‡å¤è®°å½•ï¼‰"""
        for error in self.error_records.get('products', []):
            if error.get('leaf_code') == leaf_code:
                return True
        return False
    
    def _verify_single_cache_file(self, failure_record: Dict) -> Tuple[bool, int]:
        """éªŒè¯å•ä¸ªç¼“å­˜æ–‡ä»¶æ˜¯å¦å·²ä¿®å¤ï¼Œè¿”å›(æ˜¯å¦å·²ä¿®å¤, äº§å“æ•°é‡)"""
        leaf_code = failure_record['leaf_code']
        
        try:
            cache_file = self.products_cache_dir / f"{leaf_code}.json"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_products = json.load(f)
                
                if cached_products and len(cached_products) > 0:
                    return True, len(cached_products)  # å·²ä¿®å¤
            
            return False, 0  # ä»ç„¶å¤±è´¥
            
        except Exception as e:
            # ç¼“å­˜æ–‡ä»¶æœ‰é—®é¢˜ï¼Œç»§ç»­å½“ä½œå¤±è´¥å¤„ç†
            return False, 0
    
    def _parallel_verify_cache_files(self, failure_records: List[Dict]) -> List[Tuple[bool, int]]:
        """å¹¶è¡ŒéªŒè¯å¤šä¸ªç¼“å­˜æ–‡ä»¶æ˜¯å¦å·²ä¿®å¤"""
        if not failure_records:
            return []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ£€æŸ¥ç¼“å­˜æ–‡ä»¶ï¼ˆIOå¯†é›†å‹ä»»åŠ¡ï¼Œé€‚åˆç”¨çº¿ç¨‹ï¼‰
        max_workers = min(8, len(failure_records))  # æœ€å¤š8ä¸ªçº¿ç¨‹ï¼Œé¿å…è¿‡å¤šIOç«äº‰
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰éªŒè¯ä»»åŠ¡
            future_to_record = {
                executor.submit(self._verify_single_cache_file, record): record 
                for record in failure_records
            }
            
            # æ”¶é›†ç»“æœï¼Œä¿æŒåŸå§‹é¡ºåº
            results = []
            for record in failure_records:
                # æ‰¾åˆ°å¯¹åº”çš„future
                future = next(f for f, r in future_to_record.items() if r == record)
                try:
                    result = future.result(timeout=5)  # 5ç§’è¶…æ—¶
                    results.append(result)
                except Exception as e:
                    # éªŒè¯å¤±è´¥ï¼Œå½“ä½œä»ç„¶å¤±è´¥å¤„ç†
                    self.logger.debug(f"éªŒè¯ç¼“å­˜æ–‡ä»¶å¤±è´¥ {record['leaf_code']}: {e}")
                    results.append((False, 0))
        
        return results

    def _update_error_log_file(self, error_log_path: Path, error_data: Dict, recovered_codes: List[str]):
        """æ›´æ–°é”™è¯¯æ—¥å¿—æ–‡ä»¶ï¼Œåˆ é™¤å·²ä¿®å¤çš„è®°å½•å¹¶æ›´æ–°ç»Ÿè®¡"""
        try:
            self.logger.info(f"ğŸ“ æ›´æ–°é”™è¯¯æ—¥å¿—æ–‡ä»¶ï¼Œå‰”é™¤ {len(recovered_codes)} ä¸ªå·²ä¿®å¤è®°å½•...")
            
            # è·å–åŸå§‹äº§å“é”™è¯¯åˆ—è¡¨
            original_product_errors = error_data.get('details', {}).get('products', [])
            
            # è¿‡æ»¤æ‰å·²ä¿®å¤çš„è®°å½•
            updated_product_errors = [
                error for error in original_product_errors 
                if error.get('leaf_code') not in recovered_codes
            ]
            
            # æ›´æ–°error_data
            updated_error_data = error_data.copy()
            updated_error_data['details']['products'] = updated_product_errors
            
            # æ›´æ–°summaryç»Ÿè®¡
            old_summary = updated_error_data.get('summary', {})
            new_summary = old_summary.copy()
            new_summary.update({
                'total_product_errors': len(updated_product_errors),
                'auto_cleanup_info': {
                    'last_cleanup_at': datetime.now().isoformat(),
                    'recovered_count': len(recovered_codes),
                    'recovered_codes': recovered_codes,
                    'original_count': len(original_product_errors)
                }
            })
            updated_error_data['summary'] = new_summary
            
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = error_log_path.with_suffix('.json.bak')
            if backup_path.exists():
                backup_path.unlink()
            error_log_path.rename(backup_path)
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            with open(error_log_path, 'w', encoding='utf-8') as f:
                json.dump(updated_error_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… é”™è¯¯æ—¥å¿—å·²æ›´æ–°: {error_log_path.name}")
            self.logger.info(f"   â€¢ åŸå§‹é”™è¯¯: {len(original_product_errors)} ä¸ª")
            self.logger.info(f"   â€¢ å·²ä¿®å¤å‰”é™¤: {len(recovered_codes)} ä¸ª")
            self.logger.info(f"   â€¢ å‰©ä½™é”™è¯¯: {len(updated_product_errors)} ä¸ª")
            self.logger.info(f"   â€¢ å¤‡ä»½æ–‡ä»¶: {backup_path.name}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°é”™è¯¯æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œå°è¯•æ¢å¤å¤‡ä»½
            try:
                if backup_path.exists():
                    backup_path.rename(error_log_path)
                    self.logger.info("ğŸ”„ å·²æ¢å¤åŸå§‹é”™è¯¯æ—¥å¿—æ–‡ä»¶")
            except Exception as restore_e:
                self.logger.error(f"âŒ æ¢å¤å¤‡ä»½ä¹Ÿå¤±è´¥: {restore_e}")