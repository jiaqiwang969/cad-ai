#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
=============
æ”¯æŒä¸‰é˜¶æ®µç¼“å­˜ï¼š
1. åˆ†ç±»æ ‘ï¼ˆclassification treeï¼‰
2. äº§å“é“¾æ¥ï¼ˆproduct linksï¼‰
3. äº§å“è§„æ ¼ï¼ˆproduct specificationsï¼‰

æ¯ä¸ªé˜¶æ®µå¯ä»¥ç‹¬ç«‹æ›´æ–°ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ç¼“å­˜çº§åˆ«
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.crawler.classification_optimized import OptimizedClassificationCrawler
from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger, ProgressTracker


class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«æšä¸¾"""
    NONE = 0
    CLASSIFICATION = 1  # ä»…åˆ†ç±»æ ‘
    PRODUCTS = 2       # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥
    SPECIFICATIONS = 3  # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥ + äº§å“è§„æ ¼


class CacheManager:
    """ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = 'results/cache', max_workers: int = 16):
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers
        self.logger = ThreadSafeLogger("cache-manager", logging.INFO)
        self.progress_tracker = ProgressTracker(self.logger)
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„ - æ–°çš„å‘½åè§„èŒƒ
        self.cache_index_file = self.cache_dir / 'cache_index.json'  # ç´¢å¼•æ–‡ä»¶ï¼Œè®°å½•æœ€æ–°ç‰ˆæœ¬
        self.products_cache_dir = self.cache_dir / 'products'
        self.specs_cache_dir = self.cache_dir / 'specifications'
        self.error_logs_dir = self.cache_dir / 'error_logs'  # å¼‚å¸¸è®°å½•ç›®å½•
        
        # åˆ›å»ºç‰ˆæœ¬åŒ–çš„ç¼“å­˜æ–‡ä»¶å
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.classification_file = self.cache_dir / f'classification_tree_v{self.timestamp}.json'
        self.products_file = self.cache_dir / f'products_links_v{self.timestamp}.json'  
        self.specifications_file = self.cache_dir / f'specifications_v{self.timestamp}.json'
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.products_cache_dir.mkdir(parents=True, exist_ok=True)
        self.specs_cache_dir.mkdir(parents=True, exist_ok=True)
        self.error_logs_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–çˆ¬å–å™¨
        self.classification_crawler = OptimizedClassificationCrawler()
        self.products_crawler = UltimateProductLinksCrawler()
        self.specifications_crawler = OptimizedSpecificationsCrawler(log_level=logging.INFO)
        
        # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        self.cache_ttl = {
            CacheLevel.CLASSIFICATION: 24 * 7,  # åˆ†ç±»æ ‘ï¼š7å¤©
            CacheLevel.PRODUCTS: 24 * 3,       # äº§å“é“¾æ¥ï¼š3å¤©
            CacheLevel.SPECIFICATIONS: 24      # äº§å“è§„æ ¼ï¼š1å¤©
        }
        
        # å¼‚å¸¸è®°å½•
        self.error_records = {
            'products': [],      # äº§å“é“¾æ¥çˆ¬å–å¤±è´¥è®°å½•
            'specifications': [] # äº§å“è§„æ ¼çˆ¬å–å¤±è´¥è®°å½•
        }
    
    def get_cache_level(self) -> Tuple[CacheLevel, Optional[Dict]]:
        """è·å–å½“å‰ç¼“å­˜çº§åˆ«å’Œç¼“å­˜æ•°æ®"""
        # è¯»å–ç¼“å­˜ç´¢å¼•æ–‡ä»¶
        if not self.cache_index_file.exists():
            # å¦‚æœæ²¡æœ‰ç´¢å¼•æ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ—§ç‰ˆæœ¬çš„ç¼“å­˜æ–‡ä»¶
            old_cache_file = self.cache_dir / 'classification_tree_full.json'
            if old_cache_file.exists():
                self.logger.info("ğŸ”„ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬ç¼“å­˜æ–‡ä»¶ï¼Œå°†è¿›è¡Œè¿ç§»")
                try:
                    with open(old_cache_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    metadata = data.get('metadata', {})
                    cache_level = CacheLevel(metadata.get('cache_level', 1))
                    return cache_level, data
                except Exception as e:
                    self.logger.error(f"è¯»å–æ—§ç‰ˆæœ¬ç¼“å­˜å¤±è´¥: {e}")
            return CacheLevel.NONE, None
        
        try:
            with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            latest_files = index_data.get('latest_files', {})
            current_level = CacheLevel.NONE
            data = None
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if 'specifications' in latest_files:
                specs_file = self.cache_dir / latest_files['specifications']
                if specs_file.exists():
                    current_level = CacheLevel.SPECIFICATIONS
                    with open(specs_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
            elif 'products' in latest_files:
                products_file = self.cache_dir / latest_files['products']
                if products_file.exists():
                    current_level = CacheLevel.PRODUCTS
                    with open(products_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
            elif 'classification' in latest_files:
                class_file = self.cache_dir / latest_files['classification']
                if class_file.exists():
                    current_level = CacheLevel.CLASSIFICATION
                    with open(class_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if data and 'metadata' in data:
                metadata = data['metadata']
                if 'generated' in metadata:
                    generated_time = datetime.fromisoformat(metadata['generated'])
                    age_hours = (datetime.now() - generated_time).total_seconds() / 3600
                    
                    if age_hours > self.cache_ttl.get(current_level, 24):
                        self.logger.warning(f"ç¼“å­˜å·²è¿‡æœŸ (å¹´é¾„: {age_hours:.1f}å°æ—¶)")
                        return CacheLevel.NONE, None
            
            return current_level, data
            
        except Exception as e:
            self.logger.error(f"è¯»å–ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")
            return CacheLevel.NONE, None
    
    def _update_cache_index(self, level: CacheLevel, filename: str):
        """æ›´æ–°ç¼“å­˜ç´¢å¼•æ–‡ä»¶"""
        index_data = {}
        if self.cache_index_file.exists():
            try:
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
            except Exception:
                pass
        
        if 'latest_files' not in index_data:
            index_data['latest_files'] = {}
        if 'version_history' not in index_data:
            index_data['version_history'] = []
        
        # æ›´æ–°æœ€æ–°æ–‡ä»¶è®°å½•
        level_name = level.name.lower()
        index_data['latest_files'][level_name] = filename
        
        # æ·»åŠ ç‰ˆæœ¬å†å²
        version_record = {
            'level': level.name,
            'filename': filename,
            'timestamp': datetime.now().isoformat(),
            'version': self.timestamp
        }
        index_data['version_history'].append(version_record)
        
        # åªä¿ç•™æœ€è¿‘50ä¸ªç‰ˆæœ¬è®°å½•
        if len(index_data['version_history']) > 50:
            index_data['version_history'] = index_data['version_history'][-50:]
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        with open(self.cache_index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“‡ å·²æ›´æ–°ç¼“å­˜ç´¢å¼•: {level.name} -> {filename}")
    
    def save_cache(self, data: Dict, level: CacheLevel):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        # é€‰æ‹©å¯¹åº”çš„æ–‡ä»¶å
        if level == CacheLevel.CLASSIFICATION:
            cache_file = self.classification_file
        elif level == CacheLevel.PRODUCTS:
            cache_file = self.products_file
        elif level == CacheLevel.SPECIFICATIONS:
            cache_file = self.specifications_file
        else:
            raise ValueError(f"æœªçŸ¥çš„ç¼“å­˜çº§åˆ«: {level}")
        
        # å¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if cache_file.exists():
            backup_file = cache_file.with_suffix('.json.bak')
            cache_file.rename(backup_file)
            self.logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        
        try:
            # è®¡ç®—è§„æ ¼æ€»æ•°ï¼ˆåªæœ‰åœ¨SPECIFICATIONSçº§åˆ«æ‰æœ‰è§„æ ¼æ•°æ®ï¼‰
            total_specifications = 0
            if level == CacheLevel.SPECIFICATIONS:
                for leaf in data.get('leaves', []):
                    for product in leaf.get('products', []):
                        if isinstance(product, dict):
                            total_specifications += len(product.get('specifications', []))
            
            # æ›´æ–°å…ƒæ•°æ®
            data['metadata'] = {
                'generated': datetime.now().isoformat(),
                'cache_level': level.value,
                'cache_level_name': level.name,
                'version': f'v{self.timestamp}',
                'total_leaves': len(data.get('leaves', [])),
                'total_products': sum(leaf.get('product_count', 0) for leaf in data.get('leaves', [])),
                'total_specifications': total_specifications
            }
            
            # ä¿å­˜æ–‡ä»¶
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            file_size_mb = cache_file.stat().st_size / 1024 / 1024
            self.logger.info(f"ğŸ’¾ å·²ä¿å­˜ç¼“å­˜åˆ°: {cache_file}")
            self.logger.info(f"   ç¼“å­˜çº§åˆ«: {level.name}")
            self.logger.info(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
            self.logger.info(f"   ç‰ˆæœ¬å·: v{self.timestamp}")
            
            # æ›´æ–°ç´¢å¼•æ–‡ä»¶
            self._update_cache_index(level, cache_file.name)
            
            # æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘5ä¸ªç‰ˆæœ¬ï¼‰
            self._cleanup_old_versions(level)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _cleanup_old_versions(self, level: CacheLevel, keep_versions: int = 5):
        """æ¸…ç†æ—§ç‰ˆæœ¬çš„ç¼“å­˜æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ ä¸ªç‰ˆæœ¬"""
        try:
            # æ ¹æ®çº§åˆ«ç¡®å®šæ–‡ä»¶æ¨¡å¼
            if level == CacheLevel.CLASSIFICATION:
                pattern = "classification_tree_v*.json"
            elif level == CacheLevel.PRODUCTS:
                pattern = "products_links_v*.json"
            elif level == CacheLevel.SPECIFICATIONS:
                pattern = "specifications_v*.json"
            else:
                return
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
            cache_files = list(self.cache_dir.glob(pattern))
            if len(cache_files) <= keep_versions:
                return
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤è¾ƒæ—§çš„æ–‡ä»¶
            cache_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            files_to_delete = cache_files[keep_versions:]
            
            for file_path in files_to_delete:
                try:
                    file_path.unlink()
                    self.logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§ç‰ˆæœ¬æ–‡ä»¶: {file_path.name}")
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            
            if files_to_delete:
                self.logger.info(f"ğŸ§¹ å·²æ¸…ç† {len(files_to_delete)} ä¸ªæ—§ç‰ˆæœ¬æ–‡ä»¶")
                
        except Exception as e:
            self.logger.warning(f"æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_version_history(self, level: Optional[CacheLevel] = None) -> List[Dict]:
        """è·å–ç‰ˆæœ¬å†å²è®°å½•"""
        try:
            if not self.cache_index_file.exists():
                return []
            
            with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            history = index_data.get('version_history', [])
            
            # å¦‚æœæŒ‡å®šäº†çº§åˆ«ï¼Œåªè¿”å›è¯¥çº§åˆ«çš„å†å²
            if level:
                history = [h for h in history if h.get('level') == level.name]
            
            # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            history.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
            return history
            
        except Exception as e:
            self.logger.error(f"è·å–ç‰ˆæœ¬å†å²å¤±è´¥: {e}")
            return []
    
    def get_cache_status(self) -> Dict:
        """è·å–è¯¦ç»†çš„ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        current_level, data = self.get_cache_level()
        
        status = {
            'current_level': current_level.name,
            'current_level_value': current_level.value,
            'cache_directory': str(self.cache_dir),
            'latest_files': {},
            'file_sizes': {},
            'metadata': {}
        }
        
        # è¯»å–ç´¢å¼•æ–‡ä»¶è·å–æœ€æ–°æ–‡ä»¶ä¿¡æ¯
        try:
            if self.cache_index_file.exists():
                with open(self.cache_index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                latest_files = index_data.get('latest_files', {})
                for level_name, filename in latest_files.items():
                    file_path = self.cache_dir / filename
                    if file_path.exists():
                        status['latest_files'][level_name] = filename
                        status['file_sizes'][level_name] = f"{file_path.stat().st_size / 1024 / 1024:.1f} MB"
        except Exception as e:
            self.logger.warning(f"è¯»å–ç¼“å­˜ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
        
        # å¦‚æœæœ‰å½“å‰æ•°æ®ï¼Œæå–å…ƒæ•°æ®
        if data and 'metadata' in data:
            status['metadata'] = data['metadata']
        
        return status
    
    def _record_error(self, error_type: str, error_info: Dict):
        """è®°å½•é”™è¯¯ä¿¡æ¯"""
        error_record = {
            'timestamp': datetime.now().isoformat(),
            'version': f'v{self.timestamp}',
            **error_info
        }
        
        if error_type in self.error_records:
            self.error_records[error_type].append(error_record)
    
    def _save_error_logs(self):
        """ä¿å­˜å¼‚å¸¸è®°å½•åˆ°æ–‡ä»¶"""
        if not any(self.error_records.values()):
            return  # æ²¡æœ‰é”™è¯¯è®°å½•ï¼Œä¸éœ€è¦ä¿å­˜
        
        error_log_file = self.error_logs_dir / f'error_log_v{self.timestamp}.json'
        
        # ç»Ÿè®¡ä¿¡æ¯
        error_summary = {
            'generated': datetime.now().isoformat(),
            'version': f'v{self.timestamp}',
            'summary': {
                'total_product_errors': len(self.error_records['products']),
                'total_specification_errors': len(self.error_records['specifications']),
                'zero_specs_count': len([e for e in self.error_records['specifications'] if e.get('spec_count', 0) == 0]),
                'exception_count': len([e for e in self.error_records['specifications'] if 'exception' in e])
            },
            'details': self.error_records
        }
        
        # ä¿å­˜é”™è¯¯æ—¥å¿—
        with open(error_log_file, 'w', encoding='utf-8') as f:
            json.dump(error_summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“ å¼‚å¸¸è®°å½•å·²ä¿å­˜: {error_log_file}")
        self.logger.info(f"   â€¢ äº§å“é“¾æ¥å¤±è´¥: {error_summary['summary']['total_product_errors']} ä¸ª")
        self.logger.info(f"   â€¢ è§„æ ¼çˆ¬å–å¤±è´¥: {error_summary['summary']['total_specification_errors']} ä¸ª")
        self.logger.info(f"   â€¢ å…¶ä¸­é›¶è§„æ ¼: {error_summary['summary']['zero_specs_count']} ä¸ª")
    
    def extend_to_products(self, data: Dict) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“é“¾æ¥çº§åˆ«"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“¦ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“é“¾æ¥")
        self.logger.info("="*60)
        
        leaves = data['leaves']
        self.progress_tracker.register_task("äº§å“é“¾æ¥æ‰©å±•", len(leaves))
        
        # å¹¶è¡Œçˆ¬å–äº§å“é“¾æ¥
        leaf_products = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_leaf = {
                executor.submit(self._crawl_products_for_leaf, leaf): leaf
                for leaf in leaves
            }
            
            for future in as_completed(future_to_leaf):
                leaf = future_to_leaf[future]
                try:
                    products = future.result()
                    leaf_products[leaf['code']] = products
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=True)
                    
                    # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼ˆåŒ…å«URLï¼‰
                    if products:
                        self.logger.info(f"âœ… å¶èŠ‚ç‚¹ {leaf['code']} äº§å“æ•°: {len(products)}")
                        self.logger.info(f"   åœ°å€: {leaf['url']}")
                    else:
                        self.logger.warning(f"âš ï¸  å¶èŠ‚ç‚¹ {leaf['code']} æ— äº§å“")
                        self.logger.warning(f"   åœ°å€: {leaf['url']}")
                        
                        # è®°å½•é›¶äº§å“æƒ…å†µ
                        self._record_error('products', {
                            'error_type': 'zero_products',
                            'leaf_code': leaf['code'],
                            'leaf_name': leaf.get('name', ''),
                            'leaf_url': leaf['url'],
                            'product_count': 0,
                            'note': 'é¡µé¢è®¿é—®æ­£å¸¸ä½†æœªæ‰¾åˆ°äº§å“'
                        })
                        
                except Exception as e:
                    self.logger.error(f"å¶èŠ‚ç‚¹ {leaf['code']} å¤„ç†å¤±è´¥: {e}")
                    self.logger.error(f"   åœ°å€: {leaf['url']}")
                    
                    # è®°å½•äº§å“é“¾æ¥çˆ¬å–å¤±è´¥
                    self._record_error('products', {
                        'error_type': 'product_extraction_failed',
                        'leaf_code': leaf['code'],
                        'leaf_name': leaf.get('name', ''),
                        'leaf_url': leaf['url'],
                        'exception': str(e),
                        'exception_type': type(e).__name__
                    })
                    
                    leaf_products[leaf['code']] = []
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=False)
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_products(data, leaf_products)
        
        # ç»Ÿè®¡
        total_products = sum(len(products) for products in leaf_products.values())
        self.logger.info(f"\nâœ… äº§å“é“¾æ¥æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†å¶èŠ‚ç‚¹: {len(leaves)} ä¸ª")
        self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {total_products} ä¸ª")
        
        # ä¿å­˜å¼‚å¸¸è®°å½•ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if self.error_records['products']:
            self._save_error_logs()
        
        return data
    
    def extend_to_specifications(self, data: Dict) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“è§„æ ¼çº§åˆ«"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“‹ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“è§„æ ¼")
        self.logger.info("="*60)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦çˆ¬å–è§„æ ¼çš„äº§å“
        all_products = []
        for leaf in data['leaves']:
            leaf_code = leaf['code']
            for product_url in leaf.get('products', []):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²URLï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if isinstance(product_url, str):
                    product_info = {
                        'product_url': product_url,
                        'leaf_code': leaf_code
                    }
                else:
                    product_info = product_url
                    product_info['leaf_code'] = leaf_code
                all_products.append(product_info)
        
        self.logger.info(f"å‡†å¤‡çˆ¬å– {len(all_products)} ä¸ªäº§å“çš„è§„æ ¼...")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„çº¿ç¨‹æ•°é…ç½®
        max_workers = min(len(all_products), 12)
        
        # æå–æ‰€æœ‰äº§å“URL
        product_urls = [p['product_url'] if isinstance(p, dict) else p for p in all_products]
        
        # æ‰¹é‡çˆ¬å–
        batch_result = self.specifications_crawler.extract_batch_specifications(
            product_urls, 
            max_workers=max_workers
        )
        
        # å¤„ç†ç»“æœ
        product_specs = {}
        success_count = 0
        total_specs = 0
        
        for result in batch_result.get('results', []):
            product_url = result['product_url']
            specs = result.get('specifications', [])
            product_specs[product_url] = specs
            
            if result.get('success', False):
                success_count += 1
                total_specs += len(specs)
                
                # è®°å½•é›¶è§„æ ¼æƒ…å†µï¼ˆæˆåŠŸè®¿é—®ä½†æ— è§„æ ¼ï¼‰
                if len(specs) == 0:
                    # æ‰¾åˆ°å¯¹åº”çš„äº§å“ä¿¡æ¯
                    product_info = next((p for p in all_products if (p['product_url'] if isinstance(p, dict) else p) == product_url), None)
                    
                    self._record_error('specifications', {
                        'error_type': 'zero_specifications',
                        'product_url': product_url,
                        'leaf_code': product_info.get('leaf_code', 'unknown') if isinstance(product_info, dict) else 'unknown',
                        'spec_count': 0,
                        'success': True,
                        'note': 'é¡µé¢è®¿é—®æˆåŠŸä½†æœªæå–åˆ°äº§å“è§„æ ¼'
                    })
                    
                # è®°å½•è§„æ ¼æ•°é‡è¾ƒå°‘çš„æƒ…å†µï¼ˆå¯èƒ½çš„é—®é¢˜ï¼‰
                elif len(specs) == 1:
                    product_info = next((p for p in all_products if (p['product_url'] if isinstance(p, dict) else p) == product_url), None)
                    
                    self._record_error('specifications', {
                        'error_type': 'low_specification_count',
                        'product_url': product_url,
                        'leaf_code': product_info.get('leaf_code', 'unknown') if isinstance(product_info, dict) else 'unknown',
                        'spec_count': len(specs),
                        'success': True,
                        'note': 'è§„æ ¼æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½å­˜åœ¨æå–é—®é¢˜',
                        'specifications': specs  # åŒ…å«å…·ä½“çš„è§„æ ¼å†…å®¹ç”¨äºè°ƒè¯•
                    })
            else:
                # è®°å½•å®Œå…¨å¤±è´¥çš„æƒ…å†µ
                product_info = next((p for p in all_products if (p['product_url'] if isinstance(p, dict) else p) == product_url), None)
                error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                
                self.logger.warning(f"âš ï¸ äº§å“è§„æ ¼çˆ¬å–å¤±è´¥: {product_url}")
                self.logger.warning(f"   é”™è¯¯: {error_msg}")
                
                self._record_error('specifications', {
                    'error_type': 'specification_extraction_failed',
                    'product_url': product_url,
                    'leaf_code': product_info.get('leaf_code', 'unknown') if isinstance(product_info, dict) else 'unknown',
                    'spec_count': 0,
                    'success': False,
                    'exception': error_msg,
                    'note': 'äº§å“è§„æ ¼çˆ¬å–å®Œå…¨å¤±è´¥'
                })
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_specifications(data, product_specs)
        
        # ç»Ÿè®¡
        self.logger.info(f"\nâœ… äº§å“è§„æ ¼æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†äº§å“: {len(all_products)} ä¸ª")
        self.logger.info(f"   â€¢ æˆåŠŸçˆ¬å–: {success_count} ä¸ª")
        self.logger.info(f"   â€¢ æ€»è§„æ ¼æ•°: {total_specs} ä¸ª")
        
        # ä¿å­˜å¼‚å¸¸è®°å½•
        self._save_error_logs()
        
        return data
    
    def _crawl_products_for_leaf(self, leaf: Dict) -> List[str]:
        """ä¸ºå¶èŠ‚ç‚¹çˆ¬å–äº§å“é“¾æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        code = leaf['code']
        cache_file = self.products_cache_dir / f"{code}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.PRODUCTS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                self.logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {code} ({len(products)} ä¸ªäº§å“)")
                return products
        
        # çˆ¬å–æ–°æ•°æ®
        self.logger.info(f"ğŸŒ çˆ¬å–äº§å“: {code}")
        try:
            products = self.products_crawler.extract_product_links(leaf['url'])
            
            # è®°å½•ç©ºäº§å“åˆ—è¡¨çš„æƒ…å†µï¼ˆæ²¡æœ‰å¼‚å¸¸ä½†ç»“æœä¸ºç©ºï¼‰
            if not products:
                self._record_error('products', {
                    'error_type': 'zero_products_no_exception',
                    'leaf_code': code,
                    'leaf_name': leaf.get('name', ''),
                    'leaf_url': leaf['url'],
                    'product_count': 0,
                    'note': 'çˆ¬å–å®Œæˆä½†è¿”å›ç©ºäº§å“åˆ—è¡¨'
                })
            
            # ä¿å­˜ç¼“å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            return products
        except Exception as e:
            self.logger.error(f"âŒ å¤±è´¥: {code} - {e}")
            
            # è®°å½•çˆ¬å–å¼‚å¸¸
            self._record_error('products', {
                'error_type': 'product_extraction_exception',
                'leaf_code': code,
                'leaf_name': leaf.get('name', ''),
                'leaf_url': leaf['url'],
                'exception': str(e),
                'exception_type': type(e).__name__,
                'note': 'äº§å“é“¾æ¥çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸'
            })
            
            return []
    
    def _crawl_specs_for_product(self, product: Any) -> List[Dict]:
        """ä¸ºäº§å“çˆ¬å–è§„æ ¼ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if isinstance(product, dict):
            product_url = product['product_url']
            leaf_code = product.get('leaf_code', 'unknown')
        else:
            product_url = product
            leaf_code = 'unknown'
        
        # ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆä½¿ç”¨URLçš„hashï¼‰
        import hashlib
        url_hash = hashlib.md5(product_url.encode()).hexdigest()[:12]
        cache_file = self.specs_cache_dir / f"{leaf_code}_{url_hash}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.SPECIFICATIONS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    specs = json.load(f)
                return specs
        
        # çˆ¬å–æ–°æ•°æ®
        try:
            result = self.specifications_crawler.extract_specifications(product_url)
            specs = result.get('specifications', [])
            # ä¿å­˜ç¼“å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(specs, f, ensure_ascii=False, indent=2)
            return specs
        except Exception as e:
            self.logger.error(f"è§„æ ¼çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def _update_tree_with_products(self, data: Dict, leaf_products: Dict[str, List[str]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“é“¾æ¥"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                code = node.get('code', '')
                products = leaf_products.get(code, [])
                node['products'] = products
                node['product_count'] = len(products)
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘
        update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            code = leaf['code']
            products = leaf_products.get(code, [])
            leaf['products'] = products
            leaf['product_count'] = len(products)
    
    def _update_tree_with_specifications(self, data: Dict, product_specs: Dict[str, List[Dict]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“è§„æ ¼"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                # æ›´æ–°äº§å“åˆ—è¡¨æ ¼å¼
                updated_products = []
                for product in node.get('products', []):
                    if isinstance(product, str):
                        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        product_info = {
                            'product_url': product,
                            'specifications': product_specs.get(product, []),
                            'spec_count': len(product_specs.get(product, []))
                        }
                    else:
                        # æ›´æ–°ç°æœ‰å­—å…¸
                        product['specifications'] = product_specs.get(product['product_url'], [])
                        product['spec_count'] = len(product['specifications'])
                        product_info = product
                    updated_products.append(product_info)
                node['products'] = updated_products
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘
        update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            updated_products = []
            for product in leaf.get('products', []):
                if isinstance(product, str):
                    product_info = {
                        'product_url': product,
                        'specifications': product_specs.get(product, []),
                        'spec_count': len(product_specs.get(product, []))
                    }
                else:
                    product['specifications'] = product_specs.get(product['product_url'], [])
                    product['spec_count'] = len(product['specifications'])
                    product_info = product
                updated_products.append(product_info)
            leaf['products'] = updated_products
    
    def run_progressive_cache(self, target_level: CacheLevel = CacheLevel.SPECIFICATIONS, force_refresh: bool = False):
        """è¿è¡Œæ¸è¿›å¼ç¼“å­˜æ„å»º"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ TraceParts æ¸è¿›å¼ç¼“å­˜ç³»ç»Ÿ")
        self.logger.info("="*60)
        
        # è·å–å½“å‰ç¼“å­˜çº§åˆ«
        current_level, data = self.get_cache_level()
        
        if force_refresh:
            self.logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°æ„å»ºæ‰€æœ‰ç¼“å­˜")
            current_level = CacheLevel.NONE
            data = None
        else:
            self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜çº§åˆ«: {current_level.name}")
            self.logger.info(f"ğŸ¯ ç›®æ ‡ç¼“å­˜çº§åˆ«: {target_level.name}")
        
        # é€çº§æ„å»ºç¼“å­˜
        if current_level.value < CacheLevel.CLASSIFICATION.value:
            self.logger.info("\n[é˜¶æ®µ 1/3] æ„å»ºåˆ†ç±»æ ‘ç¼“å­˜")
            self.logger.info("-" * 50)
            
            root, leaves = self.classification_crawler.crawl_full_tree()
            data = {'root': root, 'leaves': leaves}
            self.save_cache(data, CacheLevel.CLASSIFICATION)
            current_level = CacheLevel.CLASSIFICATION
            
            if target_level == CacheLevel.CLASSIFICATION:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        
        if current_level.value < CacheLevel.PRODUCTS.value and target_level.value >= CacheLevel.PRODUCTS.value:
            self.logger.info("\n[é˜¶æ®µ 2/3] æ‰©å±•äº§å“é“¾æ¥ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_products(data)
            self.save_cache(data, CacheLevel.PRODUCTS)
            current_level = CacheLevel.PRODUCTS
            
            if target_level == CacheLevel.PRODUCTS:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        
        if current_level.value < CacheLevel.SPECIFICATIONS.value and target_level.value >= CacheLevel.SPECIFICATIONS.value:
            self.logger.info("\n[é˜¶æ®µ 3/3] æ‰©å±•äº§å“è§„æ ¼ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_specifications(data)
            self.save_cache(data, CacheLevel.SPECIFICATIONS)
            
            self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
        
        return data 