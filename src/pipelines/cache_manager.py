#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
=============
æ”¯æŒä¸‰é˜¶æ®µç¼“å­˜ï¼š
1. åˆ†ç±»æ ‘ï¼ˆclassification treeï¼‰
2. äº§å“é“¾æ¥ï¼ˆproduct linksï¼‰
3. äº§å“è§„æ ¼ï¼ˆproduct specificationsï¼‰

æ¯ä¸ªé˜¶æ®µå¯ä»¥ç‹¬ç«‹æ›´æ–°ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ç¼“å­˜çº§åˆ«
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.crawler.classification_optimized import OptimizedClassificationCrawler
from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger, ProgressTracker


class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«æšä¸¾"""
    NONE = 0
    CLASSIFICATION = 1  # ä»…åˆ†ç±»æ ‘
    PRODUCTS = 2       # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥
    SPECIFICATIONS = 3  # åˆ†ç±»æ ‘ + äº§å“é“¾æ¥ + äº§å“è§„æ ¼


class CacheManager:
    """ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = 'results/cache', max_workers: int = 16):
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers
        self.logger = ThreadSafeLogger("cache-manager", logging.INFO)
        self.progress_tracker = ProgressTracker(self.logger)
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.main_cache_file = self.cache_dir / 'classification_tree_full.json'
        self.products_cache_dir = self.cache_dir / 'products'
        self.specs_cache_dir = self.cache_dir / 'specifications'
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.products_cache_dir.mkdir(parents=True, exist_ok=True)
        self.specs_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–çˆ¬å–å™¨
        self.classification_crawler = OptimizedClassificationCrawler()
        self.products_crawler = UltimateProductLinksCrawler()
        self.specifications_crawler = OptimizedSpecificationsCrawler()
        
        # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        self.cache_ttl = {
            CacheLevel.CLASSIFICATION: 24 * 7,  # åˆ†ç±»æ ‘ï¼š7å¤©
            CacheLevel.PRODUCTS: 24 * 3,       # äº§å“é“¾æ¥ï¼š3å¤©
            CacheLevel.SPECIFICATIONS: 24      # äº§å“è§„æ ¼ï¼š1å¤©
        }
    
    def get_cache_level(self) -> Tuple[CacheLevel, Optional[Dict]]:
        """è·å–å½“å‰ç¼“å­˜çº§åˆ«å’Œç¼“å­˜æ•°æ®"""
        if not self.main_cache_file.exists():
            return CacheLevel.NONE, None
        
        try:
            with open(self.main_cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata = data.get('metadata', {})
            cache_level = CacheLevel(metadata.get('cache_level', 0))
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if 'generated' in metadata:
                generated_time = datetime.fromisoformat(metadata['generated'])
                age_hours = (datetime.now() - generated_time).total_seconds() / 3600
                
                if age_hours > self.cache_ttl.get(cache_level, 24):
                    self.logger.warning(f"ç¼“å­˜å·²è¿‡æœŸ (å¹´é¾„: {age_hours:.1f}å°æ—¶)")
                    return CacheLevel.NONE, None
            
            return cache_level, data
            
        except Exception as e:
            self.logger.error(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return CacheLevel.NONE, None
    
    def save_cache(self, data: Dict, level: CacheLevel):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        # å¤‡ä»½ç°æœ‰æ–‡ä»¶
        if self.main_cache_file.exists():
            backup_file = self.main_cache_file.with_suffix('.json.bak')
            self.main_cache_file.rename(backup_file)
            self.logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        
        # è®¡ç®—è§„æ ¼æ€»æ•°ï¼ˆåªæœ‰åœ¨SPECIFICATIONSçº§åˆ«æ‰æœ‰è§„æ ¼æ•°æ®ï¼‰
        total_specifications = 0
        if level == CacheLevel.SPECIFICATIONS:
            for leaf in data.get('leaves', []):
                for product in leaf.get('products', []):
                    if isinstance(product, dict):
                        total_specifications += len(product.get('specifications', []))
        
        # æ›´æ–°å…ƒæ•°æ®
        data['metadata'] = {
            'generated': datetime.now().isoformat(),
            'cache_level': level.value,
            'cache_level_name': level.name,
            'version': f'3.0-{level.name.lower()}',
            'total_leaves': len(data.get('leaves', [])),
            'total_products': sum(leaf.get('product_count', 0) for leaf in data.get('leaves', [])),
            'total_specifications': total_specifications
        }
        
        # ä¿å­˜æ–‡ä»¶
        with open(self.main_cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        file_size_mb = self.main_cache_file.stat().st_size / 1024 / 1024
        self.logger.info(f"ğŸ’¾ å·²ä¿å­˜ç¼“å­˜åˆ°: {self.main_cache_file}")
        self.logger.info(f"   ç¼“å­˜çº§åˆ«: {level.name}")
        self.logger.info(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
    
    def extend_to_products(self, data: Dict) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“é“¾æ¥çº§åˆ«"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“¦ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“é“¾æ¥")
        self.logger.info("="*60)
        
        leaves = data['leaves']
        self.progress_tracker.register_task("äº§å“é“¾æ¥æ‰©å±•", len(leaves))
        
        # å¹¶è¡Œçˆ¬å–äº§å“é“¾æ¥
        leaf_products = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_leaf = {
                executor.submit(self._crawl_products_for_leaf, leaf): leaf
                for leaf in leaves
            }
            
            for future in as_completed(future_to_leaf):
                leaf = future_to_leaf[future]
                try:
                    products = future.result()
                    leaf_products[leaf['code']] = products
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=True)
                    
                    # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼ˆåŒ…å«URLï¼‰
                    if products:
                        self.logger.info(f"âœ… å¶èŠ‚ç‚¹ {leaf['code']} äº§å“æ•°: {len(products)}")
                        self.logger.info(f"   åœ°å€: {leaf['url']}")
                    else:
                        self.logger.warning(f"âš ï¸  å¶èŠ‚ç‚¹ {leaf['code']} æ— äº§å“")
                        self.logger.warning(f"   åœ°å€: {leaf['url']}")
                        
                except Exception as e:
                    self.logger.error(f"å¶èŠ‚ç‚¹ {leaf['code']} å¤„ç†å¤±è´¥: {e}")
                    self.logger.error(f"   åœ°å€: {leaf['url']}")
                    leaf_products[leaf['code']] = []
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=False)
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_products(data, leaf_products)
        
        # ç»Ÿè®¡
        total_products = sum(len(products) for products in leaf_products.values())
        self.logger.info(f"\nâœ… äº§å“é“¾æ¥æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†å¶èŠ‚ç‚¹: {len(leaves)} ä¸ª")
        self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {total_products} ä¸ª")
        
        return data
    
    def extend_to_specifications(self, data: Dict) -> Dict:
        """æ‰©å±•ç¼“å­˜åˆ°äº§å“è§„æ ¼çº§åˆ«"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“‹ æ‰©å±•ç¼“å­˜ï¼šæ·»åŠ äº§å“è§„æ ¼")
        self.logger.info("="*60)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦çˆ¬å–è§„æ ¼çš„äº§å“
        all_products = []
        for leaf in data['leaves']:
            leaf_code = leaf['code']
            for product_url in leaf.get('products', []):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²URLï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if isinstance(product_url, str):
                    product_info = {
                        'product_url': product_url,
                        'leaf_code': leaf_code
                    }
                else:
                    product_info = product_url
                    product_info['leaf_code'] = leaf_code
                all_products.append(product_info)
        
        self.logger.info(f"å‡†å¤‡çˆ¬å– {len(all_products)} ä¸ªäº§å“çš„è§„æ ¼...")
        self.progress_tracker.register_task("äº§å“è§„æ ¼æ‰©å±•", len(all_products))
        
        # å¹¶è¡Œçˆ¬å–äº§å“è§„æ ¼
        product_specs = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_product = {
                executor.submit(self._crawl_specs_for_product, p): p
                for p in all_products
            }
            
            for future in as_completed(future_to_product):
                product = future_to_product[future]
                try:
                    specs = future.result()
                    product_url = product['product_url'] if isinstance(product, dict) else product
                    product_specs[product_url] = specs
                    self.progress_tracker.update_task("äº§å“è§„æ ¼æ‰©å±•", success=True)
                except Exception as e:
                    self.logger.error(f"äº§å“è§„æ ¼çˆ¬å–å¤±è´¥: {e}")
                    product_url = product['product_url'] if isinstance(product, dict) else product
                    product_specs[product_url] = []
                    self.progress_tracker.update_task("äº§å“è§„æ ¼æ‰©å±•", success=False)
        
        # æ›´æ–°æ•°æ®ç»“æ„
        self._update_tree_with_specifications(data, product_specs)
        
        # ç»Ÿè®¡
        total_specs = sum(len(specs) for specs in product_specs.values())
        self.logger.info(f"\nâœ… äº§å“è§„æ ¼æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†äº§å“: {len(all_products)} ä¸ª")
        self.logger.info(f"   â€¢ æ€»è§„æ ¼æ•°: {total_specs} ä¸ª")
        
        return data
    
    def _crawl_products_for_leaf(self, leaf: Dict) -> List[str]:
        """ä¸ºå¶èŠ‚ç‚¹çˆ¬å–äº§å“é“¾æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        code = leaf['code']
        cache_file = self.products_cache_dir / f"{code}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.PRODUCTS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                self.logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {code} ({len(products)} ä¸ªäº§å“)")
                return products
        
        # çˆ¬å–æ–°æ•°æ®
        self.logger.info(f"ğŸŒ çˆ¬å–äº§å“: {code}")
        try:
            products = self.products_crawler.extract_product_links(leaf['url'])
            # ä¿å­˜ç¼“å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            return products
        except Exception as e:
            self.logger.error(f"âŒ å¤±è´¥: {code} - {e}")
            return []
    
    def _crawl_specs_for_product(self, product: Any) -> List[Dict]:
        """ä¸ºäº§å“çˆ¬å–è§„æ ¼ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if isinstance(product, dict):
            product_url = product['product_url']
            leaf_code = product.get('leaf_code', 'unknown')
        else:
            product_url = product
            leaf_code = 'unknown'
        
        # ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆä½¿ç”¨URLçš„hashï¼‰
        import hashlib
        url_hash = hashlib.md5(product_url.encode()).hexdigest()[:12]
        cache_file = self.specs_cache_dir / f"{leaf_code}_{url_hash}.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.cache_ttl[CacheLevel.SPECIFICATIONS] * 3600:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    specs = json.load(f)
                return specs
        
        # çˆ¬å–æ–°æ•°æ®
        try:
            result = self.specifications_crawler.extract_specifications(product_url)
            specs = result.get('specifications', [])
            # ä¿å­˜ç¼“å­˜
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(specs, f, ensure_ascii=False, indent=2)
            return specs
        except Exception as e:
            self.logger.error(f"è§„æ ¼çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def _update_tree_with_products(self, data: Dict, leaf_products: Dict[str, List[str]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“é“¾æ¥"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                code = node.get('code', '')
                products = leaf_products.get(code, [])
                node['products'] = products
                node['product_count'] = len(products)
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘
        update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            code = leaf['code']
            products = leaf_products.get(code, [])
            leaf['products'] = products
            leaf['product_count'] = len(products)
    
    def _update_tree_with_specifications(self, data: Dict, product_specs: Dict[str, List[Dict]]):
        """æ›´æ–°æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“è§„æ ¼"""
        def update_node(node: Dict):
            if node.get('is_leaf', False):
                # æ›´æ–°äº§å“åˆ—è¡¨æ ¼å¼
                updated_products = []
                for product in node.get('products', []):
                    if isinstance(product, str):
                        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                        product_info = {
                            'product_url': product,
                            'specifications': product_specs.get(product, []),
                            'spec_count': len(product_specs.get(product, []))
                        }
                    else:
                        # æ›´æ–°ç°æœ‰å­—å…¸
                        product['specifications'] = product_specs.get(product['product_url'], [])
                        product['spec_count'] = len(product['specifications'])
                        product_info = product
                    updated_products.append(product_info)
                node['products'] = updated_products
            
            for child in node.get('children', []):
                update_node(child)
        
        # æ›´æ–°æ ‘
        update_node(data['root'])
        
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨
        for leaf in data['leaves']:
            updated_products = []
            for product in leaf.get('products', []):
                if isinstance(product, str):
                    product_info = {
                        'product_url': product,
                        'specifications': product_specs.get(product, []),
                        'spec_count': len(product_specs.get(product, []))
                    }
                else:
                    product['specifications'] = product_specs.get(product['product_url'], [])
                    product['spec_count'] = len(product['specifications'])
                    product_info = product
                updated_products.append(product_info)
            leaf['products'] = updated_products
    
    def run_progressive_cache(self, target_level: CacheLevel = CacheLevel.SPECIFICATIONS, force_refresh: bool = False):
        """è¿è¡Œæ¸è¿›å¼ç¼“å­˜æ„å»º"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ TraceParts æ¸è¿›å¼ç¼“å­˜ç³»ç»Ÿ")
        self.logger.info("="*60)
        
        # è·å–å½“å‰ç¼“å­˜çº§åˆ«
        current_level, data = self.get_cache_level()
        
        if force_refresh:
            self.logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°æ„å»ºæ‰€æœ‰ç¼“å­˜")
            current_level = CacheLevel.NONE
            data = None
        else:
            self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜çº§åˆ«: {current_level.name}")
            self.logger.info(f"ğŸ¯ ç›®æ ‡ç¼“å­˜çº§åˆ«: {target_level.name}")
        
        # é€çº§æ„å»ºç¼“å­˜
        if current_level.value < CacheLevel.CLASSIFICATION.value:
            self.logger.info("\n[é˜¶æ®µ 1/3] æ„å»ºåˆ†ç±»æ ‘ç¼“å­˜")
            self.logger.info("-" * 50)
            
            root, leaves = self.classification_crawler.crawl_full_tree()
            data = {'root': root, 'leaves': leaves}
            self.save_cache(data, CacheLevel.CLASSIFICATION)
            current_level = CacheLevel.CLASSIFICATION
            
            if target_level == CacheLevel.CLASSIFICATION:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        
        if current_level.value < CacheLevel.PRODUCTS.value and target_level.value >= CacheLevel.PRODUCTS.value:
            self.logger.info("\n[é˜¶æ®µ 2/3] æ‰©å±•äº§å“é“¾æ¥ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_products(data)
            self.save_cache(data, CacheLevel.PRODUCTS)
            current_level = CacheLevel.PRODUCTS
            
            if target_level == CacheLevel.PRODUCTS:
                self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
                return data
        
        if current_level.value < CacheLevel.SPECIFICATIONS.value and target_level.value >= CacheLevel.SPECIFICATIONS.value:
            self.logger.info("\n[é˜¶æ®µ 3/3] æ‰©å±•äº§å“è§„æ ¼ç¼“å­˜")
            self.logger.info("-" * 50)
            
            data = self.extend_to_specifications(data)
            self.save_cache(data, CacheLevel.SPECIFICATIONS)
            
            self.logger.info("\nâœ… å·²è¾¾åˆ°ç›®æ ‡ç¼“å­˜çº§åˆ«")
        
        return data 