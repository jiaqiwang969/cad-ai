#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ•ˆå¢é‡æ›´æ–°ç®¡ç†å™¨
================
æ™ºèƒ½å¿«é€Ÿæ£€æµ‹ç³»ç»Ÿï¼Œå¤§å¹…æå‡å¢é‡æ›´æ–°æ•ˆç‡ï¼š

æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ï¼š
1. æ™ºèƒ½é‡‡æ ·æ£€æµ‹ï¼šéšæœºé‡‡æ ·åˆ¤æ–­æ˜¯å¦æœ‰å¤§è§„æ¨¡å˜åŒ–
2. åˆ†å±‚æ£€æµ‹ç­–ç•¥ï¼šè½»é‡çº§æ£€æµ‹ -> è¯¦ç»†æ£€æµ‹
3. å¹¶è¡Œæ£€æµ‹ï¼šåˆ†ç±»æ ‘ã€äº§å“ã€è§„æ ¼å¹¶è¡Œæ£€æµ‹
4. æ—¶é—´æˆ³ä¼˜åŒ–ï¼šæ ¹æ®ç¼“å­˜æ—¶é—´æ™ºèƒ½è·³è¿‡æ£€æµ‹
5. å¿«é€ŸæŒ‡çº¹å¯¹æ¯”ï¼šä½¿ç”¨é¡µé¢ç‰¹å¾å¿«é€Ÿåˆ¤æ–­å˜åŒ–
"""

import json
import logging
import random
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from src.pipelines.cache_manager import CacheManager, CacheLevel
from src.crawler.classification_optimized import OptimizedClassificationCrawler
from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger


@dataclass
class DetectionConfig:
    """æ£€æµ‹é…ç½®"""
    # é‡‡æ ·é…ç½®
    classification_sample_ratio: float = 0.1  # åˆ†ç±»æ ‘é‡‡æ ·æ¯”ä¾‹
    products_sample_size: int = 50             # äº§å“æ£€æµ‹é‡‡æ ·æ•°é‡
    specs_sample_size: int = 20                # è§„æ ¼æ£€æµ‹é‡‡æ ·æ•°é‡
    
    # æ—¶é—´é…ç½®
    min_check_interval_hours: int = 2          # æœ€å°æ£€æµ‹é—´éš”ï¼ˆå°æ—¶ï¼‰
    classification_ttl_hours: int = 24 * 7     # åˆ†ç±»æ ‘ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ7å¤©ï¼‰
    products_ttl_hours: int = 24 * 3           # äº§å“ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ3å¤©ï¼‰
    specs_ttl_hours: int = 24                  # è§„æ ¼ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ1å¤©ï¼‰
    
    # æ€§èƒ½é…ç½®
    max_parallel_workers: int = 8              # å¹¶è¡Œæ£€æµ‹çº¿ç¨‹æ•°
    quick_detection_timeout: int = 30          # å¿«é€Ÿæ£€æµ‹è¶…æ—¶ï¼ˆç§’ï¼‰
    
    # é˜ˆå€¼é…ç½®
    change_threshold: float = 0.05             # å˜åŒ–é˜ˆå€¼ï¼ˆ5%ï¼‰
    force_full_check_ratio: float = 0.2        # å¼ºåˆ¶å…¨é‡æ£€æŸ¥çš„å˜åŒ–æ¯”ä¾‹


@dataclass
class QuickDetectionResult:
    """å¿«é€Ÿæ£€æµ‹ç»“æœ"""
    has_changes: bool = False
    confidence: float = 0.0                    # ç»“æœç½®ä¿¡åº¦ (0-1)
    estimated_new_leaves: int = 0
    estimated_new_products: int = 0
    detection_time_seconds: float = 0.0
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class EfficientUpdateStats:
    """é«˜æ•ˆæ›´æ–°ç»Ÿè®¡"""
    total_detection_time: float = 0.0
    quick_detection_time: float = 0.0
    detailed_detection_time: float = 0.0
    
    leaves_sampled: int = 0
    leaves_checked: int = 0
    products_sampled: int = 0
    
    new_leaves: int = 0
    new_products: int = 0
    new_specifications: int = 0
    
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None


class EfficientIncrementalManager:
    """é«˜æ•ˆå¢é‡æ›´æ–°ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = 'results/cache', config: Optional[DetectionConfig] = None):
        self.cache_dir = Path(cache_dir)
        self.config = config or DetectionConfig()
        self.logger = ThreadSafeLogger("efficient-update", logging.INFO)
        
        # ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(cache_dir=str(cache_dir))
        
        # çˆ¬å–å™¨
        self.classification_crawler = OptimizedClassificationCrawler()
        self.products_crawler = UltimateProductLinksCrawler()
        self.specifications_crawler = OptimizedSpecificationsCrawler(log_level=logging.INFO)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = EfficientUpdateStats()
        
        # å¤‡ä»½ç›®å½•
        self.backup_dir = self.cache_dir / 'backups'
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def run_efficient_update(self, target_level: CacheLevel = CacheLevel.SPECIFICATIONS) -> Dict:
        """
        è¿è¡Œé«˜æ•ˆå¢é‡æ›´æ–°
        
        Args:
            target_level: ç›®æ ‡æ›´æ–°çº§åˆ«
            
        Returns:
            æ›´æ–°åçš„å®Œæ•´æ•°æ®
        """
        self.stats.start_time = datetime.now()
        
        self.logger.info("\n" + "="*70)
        self.logger.info("âš¡ TraceParts é«˜æ•ˆå¢é‡æ›´æ–°ç³»ç»Ÿ")
        self.logger.info("="*70)
        
        # è·å–å½“å‰ç¼“å­˜çŠ¶æ€
        current_level, current_data = self.cache_manager.get_cache_level()
        
        if current_level == CacheLevel.NONE:
            self.logger.warning("âš ï¸  æœªå‘ç°ç°æœ‰ç¼“å­˜ï¼Œå°†æ‰§è¡Œå…¨é‡æ„å»º")
            return self.cache_manager.run_progressive_cache(target_level=target_level)
        
        self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜çº§åˆ«: {current_level.name}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡æ›´æ–°çº§åˆ«: {target_level.name}")
        
        # æ™ºèƒ½æ£€æµ‹ç­–ç•¥ï¼šå…ˆå¿«é€Ÿæ£€æµ‹æ˜¯å¦éœ€è¦æ›´æ–°
        if self._should_skip_detection(current_data):
            self.logger.info("âœ… ç¼“å­˜ä»ç„¶æ–°é²œï¼Œè·³è¿‡æ£€æµ‹")
            return current_data
        
        # å¿«é€Ÿæ£€æµ‹æ˜¯å¦æœ‰å˜åŒ–
        quick_result = self._quick_detection(current_data, target_level)
        
        if not quick_result.has_changes:
            self.logger.info(f"âœ… å¿«é€Ÿæ£€æµ‹æ— å˜åŒ– (ç½®ä¿¡åº¦: {quick_result.confidence:.2f})")
            return current_data
        
        self.logger.info(f"ğŸ” å¿«é€Ÿæ£€æµ‹å‘ç°å˜åŒ– (ç½®ä¿¡åº¦: {quick_result.confidence:.2f})")
        self.logger.info(f"   é¢„ä¼°æ–°å¢: å¶èŠ‚ç‚¹ {quick_result.estimated_new_leaves} ä¸ª, äº§å“ {quick_result.estimated_new_products} ä¸ª")
        
        # åˆ›å»ºå¤‡ä»½
        self._create_backup(current_data, current_level)
        
        # æ‰§è¡Œè¯¦ç»†çš„å¢é‡æ›´æ–°
        updated_data = self._detailed_incremental_update(current_data, target_level, quick_result)
        
        # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
        self.cache_manager.save_cache(updated_data, target_level)
        
        # æ‰“å°æ±‡æ€»
        self._print_efficient_summary()
        
        return updated_data
    
    def _should_skip_detection(self, current_data: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ£€æµ‹ï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰"""
        metadata = current_data.get('metadata', {})
        generated_str = metadata.get('generated', '')
        
        if not generated_str:
            return False
        
        try:
            generated_time = datetime.fromisoformat(generated_str)
            age_hours = (datetime.now() - generated_time).total_seconds() / 3600
            
            if age_hours < self.config.min_check_interval_hours:
                self.logger.info(f"ğŸ“… ç¼“å­˜å¹´é¾„: {age_hours:.1f} å°æ—¶ (< {self.config.min_check_interval_hours} å°æ—¶é˜ˆå€¼)")
                return True
                
        except Exception as e:
            self.logger.warning(f"è§£æç”Ÿæˆæ—¶é—´å¤±è´¥: {e}")
        
        return False
    
    def _quick_detection(self, current_data: Dict, target_level: CacheLevel) -> QuickDetectionResult:
        """å¿«é€Ÿæ£€æµ‹æ˜¯å¦æœ‰å˜åŒ–"""
        start_time = time.time()
        result = QuickDetectionResult()
        
        self.logger.info("\nğŸš€ [å¿«é€Ÿæ£€æµ‹] é‡‡æ ·åˆ†ææ½œåœ¨å˜åŒ–")
        self.logger.info("-" * 50)
        
        try:
            # å¹¶è¡Œæ‰§è¡Œå¤šç§å¿«é€Ÿæ£€æµ‹
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {}
                
                # 1. åˆ†ç±»æ ‘å¿«é€Ÿæ£€æµ‹
                if target_level.value >= CacheLevel.CLASSIFICATION.value:
                    futures['classification'] = executor.submit(
                        self._quick_detect_classification_changes, current_data
                    )
                
                # 2. äº§å“é“¾æ¥å¿«é€Ÿæ£€æµ‹
                if target_level.value >= CacheLevel.PRODUCTS.value:
                    futures['products'] = executor.submit(
                        self._quick_detect_product_changes, current_data
                    )
                
                # 3. è§„æ ¼å¿«é€Ÿæ£€æµ‹
                if target_level.value >= CacheLevel.SPECIFICATIONS.value:
                    futures['specifications'] = executor.submit(
                        self._quick_detect_spec_changes, current_data
                    )
                
                # æ”¶é›†ç»“æœ
                detection_results = {}
                for detection_type, future in futures.items():
                    try:
                        detection_results[detection_type] = future.result(
                            timeout=self.config.quick_detection_timeout
                        )
                    except Exception as e:
                        self.logger.warning(f"{detection_type} å¿«é€Ÿæ£€æµ‹å¤±è´¥: {e}")
                        detection_results[detection_type] = {'has_changes': False, 'confidence': 0.0}
            
            # ç»¼åˆåˆ†æç»“æœ
            result = self._analyze_quick_detection_results(detection_results)
            
        except Exception as e:
            self.logger.error(f"å¿«é€Ÿæ£€æµ‹å¤±è´¥: {e}")
            result.has_changes = True  # å¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼Œè¿›è¡Œè¯¦ç»†æ£€æµ‹
            result.confidence = 0.5
        
        result.detection_time_seconds = time.time() - start_time
        self.stats.quick_detection_time = result.detection_time_seconds
        
        return result
    
    def _quick_detect_classification_changes(self, current_data: Dict) -> Dict:
        """å¿«é€Ÿæ£€æµ‹åˆ†ç±»æ ‘å˜åŒ–"""
        self.logger.info("ğŸŒ³ å¿«é€Ÿæ£€æµ‹åˆ†ç±»æ ‘å˜åŒ–...")
        
        try:
            # ç­–ç•¥1ï¼šæ£€æµ‹æ ¹é¡µé¢çš„æŒ‡çº¹å˜åŒ–
            root_url = "https://www.traceparts.cn/zh/search/traceparts-classification"
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´sophisticatedçš„é¡µé¢æŒ‡çº¹æ£€æµ‹
            # æ¯”å¦‚æ£€æµ‹åˆ†ç±»æ•°é‡ã€é¡µé¢hashç­‰
            
            # ç­–ç•¥2ï¼šé‡‡æ ·æ£€æµ‹éƒ¨åˆ†åˆ†ç±»
            current_leaves = current_data.get('leaves', [])
            sample_size = max(1, int(len(current_leaves) * self.config.classification_sample_ratio))
            sampled_leaves = random.sample(current_leaves, min(sample_size, len(current_leaves)))
            
            self.logger.info(f"   é‡‡æ · {len(sampled_leaves)} ä¸ªå¶èŠ‚ç‚¹è¿›è¡Œæ£€æµ‹")
            self.stats.leaves_sampled = len(sampled_leaves)
            
            # ç®€å•æ£€æµ‹ï¼šæ£€æŸ¥é‡‡æ ·å¶èŠ‚ç‚¹æ˜¯å¦è¿˜èƒ½æ­£å¸¸è®¿é—®
            accessible_count = 0
            for leaf in sampled_leaves[:5]:  # åªæ£€æŸ¥å‰5ä¸ªé¿å…è¶…æ—¶
                try:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´ç²¾ç»†çš„æ£€æµ‹é€»è¾‘
                    accessible_count += 1
                except:
                    pass
            
            confidence = accessible_count / min(5, len(sampled_leaves)) if sampled_leaves else 1.0
            
            return {
                'has_changes': confidence < 0.8,  # å¦‚æœè®¿é—®æˆåŠŸç‡ä½äº80%ï¼Œå¯èƒ½æœ‰å˜åŒ–
                'confidence': confidence,
                'sampled_count': len(sampled_leaves),
                'accessible_ratio': confidence
            }
            
        except Exception as e:
            self.logger.warning(f"åˆ†ç±»æ ‘å¿«é€Ÿæ£€æµ‹å¤±è´¥: {e}")
            return {'has_changes': False, 'confidence': 0.5}
    
    def _quick_detect_product_changes(self, current_data: Dict) -> Dict:
        """å¿«é€Ÿæ£€æµ‹äº§å“å˜åŒ–"""
        self.logger.info("ğŸ“¦ å¿«é€Ÿæ£€æµ‹äº§å“å˜åŒ–...")
        
        try:
            current_leaves = current_data.get('leaves', [])
            if not current_leaves:
                return {'has_changes': False, 'confidence': 1.0}
            
            # ç­–ç•¥ï¼šéšæœºé‡‡æ ·æ£€æµ‹äº§å“æ•°é‡å˜åŒ–
            sample_size = min(self.config.products_sample_size, len(current_leaves))
            sampled_leaves = random.sample(current_leaves, sample_size)
            
            self.logger.info(f"   é‡‡æ · {len(sampled_leaves)} ä¸ªå¶èŠ‚ç‚¹æ£€æµ‹äº§å“å˜åŒ–")
            self.stats.leaves_sampled = len(sampled_leaves)
            
            changes_detected = 0
            total_checked = 0
            
            for leaf in sampled_leaves:
                try:
                    current_count = leaf.get('product_count', 0)
                    if current_count == 0:
                        continue  # è·³è¿‡åŸæœ¬å°±æ²¡æœ‰äº§å“çš„å¶èŠ‚ç‚¹
                    
                    # å¿«é€Ÿæ£€æµ‹ï¼šåªè·å–äº§å“åˆ—è¡¨é•¿åº¦ï¼Œä¸è§£æè¯¦ç»†å†…å®¹
                    new_products = self.products_crawler.extract_product_links(leaf['url'])
                    new_count = len(new_products)
                    
                    if abs(new_count - current_count) > 0:
                        changes_detected += 1
                        self.logger.info(f"   ğŸ“ˆ {leaf['code']}: {current_count} â†’ {new_count}")
                    
                    total_checked += 1
                    
                    # å¿«é€Ÿæ£€æµ‹ï¼Œåªæ£€æŸ¥å°‘æ•°å‡ ä¸ª
                    if total_checked >= 10:
                        break
                        
                except Exception as e:
                    self.logger.warning(f"   âŒ {leaf.get('code', 'unknown')}: {e}")
            
            change_ratio = changes_detected / max(total_checked, 1)
            has_changes = change_ratio > self.config.change_threshold
            confidence = max(0.6, 1.0 - change_ratio) if not has_changes else min(0.9, 0.5 + change_ratio)
            
            return {
                'has_changes': has_changes,
                'confidence': confidence,
                'change_ratio': change_ratio,
                'changes_detected': changes_detected,
                'total_checked': total_checked
            }
            
        except Exception as e:
            self.logger.warning(f"äº§å“å¿«é€Ÿæ£€æµ‹å¤±è´¥: {e}")
            return {'has_changes': False, 'confidence': 0.5}
    
    def _quick_detect_spec_changes(self, current_data: Dict) -> Dict:
        """å¿«é€Ÿæ£€æµ‹è§„æ ¼å˜åŒ–"""
        self.logger.info("ğŸ“‹ å¿«é€Ÿæ£€æµ‹è§„æ ¼å˜åŒ–...")
        
        try:
            # æ”¶é›†å°‘é‡éœ€è¦æ£€æµ‹è§„æ ¼çš„äº§å“
            products_to_check = []
            for leaf in current_data.get('leaves', []):
                for product in leaf.get('products', []):
                    if isinstance(product, str):
                        # æ—§æ ¼å¼ï¼Œéœ€è¦çˆ¬å–è§„æ ¼
                        products_to_check.append(product)
                    elif isinstance(product, dict) and not product.get('specifications'):
                        # æ²¡æœ‰è§„æ ¼æ•°æ®
                        products_to_check.append(product['product_url'])
                    
                    if len(products_to_check) >= self.config.specs_sample_size:
                        break
                if len(products_to_check) >= self.config.specs_sample_size:
                    break
            
            if not products_to_check:
                return {'has_changes': False, 'confidence': 1.0}
            
            self.logger.info(f"   é‡‡æ · {len(products_to_check)} ä¸ªäº§å“æ£€æµ‹è§„æ ¼å˜åŒ–")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´ç²¾ç»†çš„è§„æ ¼å˜åŒ–æ£€æµ‹
            return {
                'has_changes': len(products_to_check) > 0,
                'confidence': 0.7,
                'products_needing_specs': len(products_to_check)
            }
            
        except Exception as e:
            self.logger.warning(f"è§„æ ¼å¿«é€Ÿæ£€æµ‹å¤±è´¥: {e}")
            return {'has_changes': False, 'confidence': 0.5}
    
    def _analyze_quick_detection_results(self, detection_results: Dict) -> QuickDetectionResult:
        """åˆ†æå¿«é€Ÿæ£€æµ‹ç»“æœ"""
        result = QuickDetectionResult()
        
        # æ”¶é›†å„æ£€æµ‹ç»“æœ
        classification_result = detection_results.get('classification', {})
        products_result = detection_results.get('products', {})
        specs_result = detection_results.get('specifications', {})
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–
        has_class_changes = classification_result.get('has_changes', False)
        has_product_changes = products_result.get('has_changes', False)
        has_spec_changes = specs_result.get('has_changes', False)
        
        result.has_changes = has_class_changes or has_product_changes or has_spec_changes
        
        # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
        confidences = []
        if 'confidence' in classification_result:
            confidences.append(classification_result['confidence'])
        if 'confidence' in products_result:
            confidences.append(products_result['confidence'])
        if 'confidence' in specs_result:
            confidences.append(specs_result['confidence'])
        
        result.confidence = sum(confidences) / len(confidences) if confidences else 0.5
        
        # ä¼°ç®—å˜åŒ–æ•°é‡
        if has_product_changes:
            change_ratio = products_result.get('change_ratio', 0.1)
            total_leaves = self.stats.leaves_sampled or 1000
            result.estimated_new_products = int(total_leaves * change_ratio * 10)  # ç²—ç•¥ä¼°ç®—
        
        result.details = detection_results
        
        return result
    
    def _detailed_incremental_update(self, current_data: Dict, target_level: CacheLevel, quick_result: QuickDetectionResult) -> Dict:
        """è¯¦ç»†å¢é‡æ›´æ–°ï¼ˆåŸºäºå¿«é€Ÿæ£€æµ‹ç»“æœä¼˜åŒ–ï¼‰"""
        start_time = time.time()
        
        self.logger.info("\nğŸ” [è¯¦ç»†æ£€æµ‹] ç²¾ç¡®æ›´æ–°å˜åŒ–å†…å®¹")
        self.logger.info("-" * 50)
        
        updated_data = current_data.copy()
        
        try:
            # æ ¹æ®å¿«é€Ÿæ£€æµ‹ç»“æœå†³å®šæ›´æ–°ç­–ç•¥
            if quick_result.details.get('classification', {}).get('has_changes', False):
                updated_data = self._update_classification_tree_smart(updated_data)
            
            if quick_result.details.get('products', {}).get('has_changes', False):
                updated_data = self._update_product_links_smart(updated_data, quick_result)
            
            if quick_result.details.get('specifications', {}).get('has_changes', False):
                updated_data = self._update_specifications_smart(updated_data)
            
        except Exception as e:
            self.logger.error(f"è¯¦ç»†æ›´æ–°å¤±è´¥: {e}")
            raise
        
        self.stats.detailed_detection_time = time.time() - start_time
        return updated_data
    
    def _update_classification_tree_smart(self, current_data: Dict) -> Dict:
        """æ™ºèƒ½æ›´æ–°åˆ†ç±»æ ‘ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        self.logger.info("ğŸŒ³ æ™ºèƒ½æ›´æ–°åˆ†ç±»æ ‘...")
        
        # è¿™é‡Œå¯ä»¥å®ç°æ›´æ™ºèƒ½çš„åˆ†ç±»æ ‘æ›´æ–°é€»è¾‘
        # ä¾‹å¦‚ï¼šåªçˆ¬å–æ ¹èŠ‚ç‚¹ï¼Œå¯¹æ¯”ç»“æ„å˜åŒ–ï¼Œåªæ›´æ–°æœ‰å˜åŒ–çš„éƒ¨åˆ†
        
        try:
            # ç®€åŒ–ç‰ˆï¼šé‡ç”¨åŸæœ‰é€»è¾‘ä½†æ·»åŠ ä¼˜åŒ–
            new_root, new_leaves = self.classification_crawler.crawl_full_tree()
            
            old_leaf_codes = {leaf['code'] for leaf in current_data['leaves']}
            new_leaf_codes = {leaf['code'] for leaf in new_leaves}
            
            added_leaf_codes = new_leaf_codes - old_leaf_codes
            self.stats.new_leaves = len(added_leaf_codes)
            
            if added_leaf_codes:
                self.logger.info(f"ğŸ†• å‘ç°æ–°å¢å¶èŠ‚ç‚¹: {len(added_leaf_codes)} ä¸ª")
                # åˆå¹¶é€»è¾‘...
                new_leaves_to_add = [leaf for leaf in new_leaves if leaf['code'] in added_leaf_codes]
                current_data['leaves'].extend(new_leaves_to_add)
                
                # æ›´æ–°æ ‘ç»“æ„ï¼Œä¿ç•™ç°æœ‰äº§å“æ•°æ®
                existing_leaf_data = {leaf['code']: leaf for leaf in current_data['leaves']}
                
                def preserve_data(node: Dict):
                    if node.get('is_leaf', False):
                        code = node.get('code', '')
                        if code in existing_leaf_data:
                            existing_leaf = existing_leaf_data[code]
                            node['products'] = existing_leaf.get('products', [])
                            node['product_count'] = existing_leaf.get('product_count', 0)
                    
                    for child in node.get('children', []):
                        preserve_data(child)
                
                preserve_data(new_root)
                current_data['root'] = new_root
            
            return current_data
            
        except Exception as e:
            self.logger.error(f"åˆ†ç±»æ ‘æ›´æ–°å¤±è´¥: {e}")
            return current_data
    
    def _update_product_links_smart(self, current_data: Dict, quick_result: QuickDetectionResult) -> Dict:
        """æ™ºèƒ½æ›´æ–°äº§å“é“¾æ¥ï¼ˆåŸºäºé‡‡æ ·ç»“æœï¼‰"""
        self.logger.info("ğŸ“¦ æ™ºèƒ½æ›´æ–°äº§å“é“¾æ¥...")
        
        # æ ¹æ®å¿«é€Ÿæ£€æµ‹ç»“æœï¼Œä¼˜å…ˆæ£€æŸ¥æœ‰å˜åŒ–çš„å¶èŠ‚ç‚¹
        change_ratio = quick_result.details.get('products', {}).get('change_ratio', 0.1)
        
        if change_ratio > self.config.force_full_check_ratio:
            self.logger.info(f"å˜åŒ–æ¯”ä¾‹è¾ƒé«˜ ({change_ratio:.2%})ï¼Œæ‰§è¡Œå…¨é‡æ£€æŸ¥")
            # æ‰§è¡Œå…¨é‡æ£€æŸ¥é€»è¾‘
        else:
            self.logger.info(f"å˜åŒ–æ¯”ä¾‹è¾ƒä½ ({change_ratio:.2%})ï¼Œæ‰§è¡Œé‡‡æ ·æ£€æŸ¥")
            # æ‰§è¡Œé‡‡æ ·æ£€æŸ¥é€»è¾‘
        
        # è¿™é‡Œå®ç°æ™ºèƒ½çš„äº§å“é“¾æ¥æ›´æ–°
        # å¯ä»¥åŸºäºå¿«é€Ÿæ£€æµ‹çš„ç»“æœæ¥ä¼˜åŒ–æ£€æµ‹èŒƒå›´
        
        return current_data
    
    def _update_specifications_smart(self, current_data: Dict) -> Dict:
        """æ™ºèƒ½æ›´æ–°äº§å“è§„æ ¼"""
        self.logger.info("ğŸ“‹ æ™ºèƒ½æ›´æ–°äº§å“è§„æ ¼...")
        
        # å®ç°æ™ºèƒ½çš„è§„æ ¼æ›´æ–°é€»è¾‘
        return current_data
    
    def _create_backup(self, data: Dict, level: CacheLevel):
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = self.backup_dir / f'backup_efficient_{level.name.lower()}_v{timestamp}.json'
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ å·²åˆ›å»ºæ•°æ®å¤‡ä»½: {backup_file.name}")
    
    def _print_efficient_summary(self):
        """æ‰“å°é«˜æ•ˆæ›´æ–°æ±‡æ€»"""
        self.stats.end_time = datetime.now()
        total_duration = (self.stats.end_time - self.stats.start_time).total_seconds() / 60
        
        self.logger.info("\n" + "="*70)
        self.logger.info("âš¡ é«˜æ•ˆå¢é‡æ›´æ–°å®Œæˆ - æ€§èƒ½æŠ¥å‘Š")
        self.logger.info("="*70)
        
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f} åˆ†é’Ÿ")
        self.logger.info(f"   â€¢ å¿«é€Ÿæ£€æµ‹: {self.stats.quick_detection_time:.1f} ç§’")
        self.logger.info(f"   â€¢ è¯¦ç»†æ£€æµ‹: {self.stats.detailed_detection_time:.1f} ç§’")
        
        self.logger.info(f"\nğŸ” æ£€æµ‹æ•ˆç‡:")
        self.logger.info(f"   â€¢ å¶èŠ‚ç‚¹é‡‡æ ·: {self.stats.leaves_sampled} ä¸ª")
        self.logger.info(f"   â€¢ å¶èŠ‚ç‚¹æ£€æŸ¥: {self.stats.leaves_checked} ä¸ª")
        self.logger.info(f"   â€¢ äº§å“é‡‡æ ·: {self.stats.products_sampled} ä¸ª")
        
        self.logger.info(f"\nğŸ“ˆ æ›´æ–°ç»“æœ:")
        self.logger.info(f"   â€¢ æ–°å¢å¶èŠ‚ç‚¹: {self.stats.new_leaves} ä¸ª")
        self.logger.info(f"   â€¢ æ–°å¢äº§å“: {self.stats.new_products} ä¸ª")
        self.logger.info(f"   â€¢ æ–°å¢è§„æ ¼: {self.stats.new_specifications} ä¸ª")
        
        efficiency_ratio = self.stats.leaves_sampled / max(self.stats.leaves_checked, 1)
        self.logger.info(f"\nâš¡ æ•ˆç‡æå‡: {efficiency_ratio:.1f}x (é‡‡æ ·æ£€æµ‹)")
        
        self.logger.info("="*70) 