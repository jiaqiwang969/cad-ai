#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰©å±•åˆ†ç±»æ ‘ç¼“å­˜ - æ·»åŠ äº§å“é“¾æ¥
==============================
å°†ç¬¬äºŒé˜¶æ®µçˆ¬å–çš„äº§å“é“¾æ¥åˆå¹¶åˆ°classification_tree.jsonä¸­ï¼Œ
ä½¿å¶èŠ‚ç‚¹åŒ…å«productså­—æ®µï¼Œå½¢æˆå®Œæ•´çš„ç¼“å­˜æ–‡ä»¶
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger, ProgressTracker


class CacheExtender:
    """æ‰©å±•åˆ†ç±»æ ‘ç¼“å­˜ï¼Œæ·»åŠ äº§å“é“¾æ¥"""
    
    def __init__(self, max_workers: int = 16):
        self.max_workers = max_workers
        self.logger = ThreadSafeLogger("cache-extender", logging.INFO)
        self.progress_tracker = ProgressTracker(self.logger)
        self.products_crawler = UltimateProductLinksCrawler()
        
        # ç¼“å­˜è·¯å¾„
        self.cache_dir = Path('results/cache')
        self.cache_file = self.cache_dir / 'classification_tree.json'
        self.products_cache_dir = self.cache_dir / 'products'
        self.products_cache_dir.mkdir(parents=True, exist_ok=True)
        
    def load_classification_tree(self) -> Dict[str, Any]:
        """åŠ è½½ç°æœ‰çš„åˆ†ç±»æ ‘ç¼“å­˜"""
        if not self.cache_file.exists():
            self.logger.error(f"åˆ†ç±»æ ‘ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {self.cache_file}")
            raise FileNotFoundError(f"Classification tree cache not found: {self.cache_file}")
            
        self.logger.info(f"ğŸ“‚ åŠ è½½åˆ†ç±»æ ‘ç¼“å­˜: {self.cache_file}")
        with open(self.cache_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.logger.info(f"âœ… å·²åŠ è½½åˆ†ç±»æ ‘ï¼ŒåŒ…å« {len(data.get('leaves', []))} ä¸ªå¶èŠ‚ç‚¹")
        return data
    
    def get_cached_products(self, leaf_code: str) -> List[str]:
        """è·å–å¶èŠ‚ç‚¹çš„ç¼“å­˜äº§å“é“¾æ¥"""
        cache_file = self.products_cache_dir / f"{leaf_code}.json"
        
        if cache_file.exists():
            # æ£€æŸ¥ç¼“å­˜æ—¶æ•ˆï¼ˆ24å°æ—¶ï¼‰
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < 86400:  # 24å°æ—¶
                with open(cache_file, 'r', encoding='utf-8') as f:
                    products = json.load(f)
                return products
        
        return None
    
    def save_products_cache(self, leaf_code: str, products: List[str]):
        """ä¿å­˜äº§å“é“¾æ¥åˆ°ç¼“å­˜"""
        cache_file = self.products_cache_dir / f"{leaf_code}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
    
    def crawl_products_for_leaf(self, leaf: Dict) -> List[str]:
        """ä¸ºå•ä¸ªå¶èŠ‚ç‚¹çˆ¬å–äº§å“é“¾æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        code = leaf['code']
        
        # å…ˆæ£€æŸ¥ç¼“å­˜
        cached_products = self.get_cached_products(code)
        if cached_products is not None:
            self.logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜: {code} ({len(cached_products)} ä¸ªäº§å“)")
            return cached_products
        
        # çˆ¬å–æ–°æ•°æ®
        self.logger.info(f"ğŸŒ çˆ¬å–äº§å“: {code} - {leaf['url']}")
        try:
            products = self.products_crawler.extract_product_links(leaf['url'])
            # ä¿å­˜åˆ°ç¼“å­˜
            self.save_products_cache(code, products)
            self.logger.info(f"âœ… æˆåŠŸçˆ¬å– {code}: {len(products)} ä¸ªäº§å“")
            return products
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–å¤±è´¥ {code}: {e}")
            return []
    
    def extend_tree_with_products(self, tree_data: Dict) -> Dict:
        """ä¸ºæ ‘çš„æ‰€æœ‰å¶èŠ‚ç‚¹æ·»åŠ äº§å“é“¾æ¥"""
        root = tree_data['root']
        leaves = tree_data['leaves']
        
        self.logger.info(f"\nå¼€å§‹æ‰©å±•åˆ†ç±»æ ‘ï¼Œä¸º {len(leaves)} ä¸ªå¶èŠ‚ç‚¹æ·»åŠ äº§å“é“¾æ¥...")
        
        # æ³¨å†Œè¿›åº¦è·Ÿè¸ª
        self.progress_tracker.register_task("äº§å“é“¾æ¥æ‰©å±•", len(leaves))
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¶èŠ‚ç‚¹
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_leaf = {
                executor.submit(self.crawl_products_for_leaf, leaf): leaf 
                for leaf in leaves
            }
            
            # æ”¶é›†ç»“æœ
            leaf_products = {}
            for future in as_completed(future_to_leaf):
                leaf = future_to_leaf[future]
                try:
                    products = future.result()
                    leaf_products[leaf['code']] = products
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=True)
                except Exception as e:
                    self.logger.error(f"å¤„ç†å¶èŠ‚ç‚¹ {leaf['code']} å¤±è´¥: {e}")
                    leaf_products[leaf['code']] = []
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æ‰©å±•", success=False)
        
        # æ›´æ–°æ ‘ç»“æ„ï¼Œä¸ºæ¯ä¸ªå¶èŠ‚ç‚¹æ·»åŠ productså­—æ®µ
        def update_node(node: Dict):
            """é€’å½’æ›´æ–°èŠ‚ç‚¹ï¼Œä¸ºå¶èŠ‚ç‚¹æ·»åŠ äº§å“é“¾æ¥"""
            if node.get('is_leaf', False):
                code = node.get('code', '')
                node['products'] = leaf_products.get(code, [])
                node['product_count'] = len(node['products'])
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            for child in node.get('children', []):
                update_node(child)
        
        # ä»æ ¹èŠ‚ç‚¹å¼€å§‹æ›´æ–°
        update_node(root)
        
        # åŒæ—¶æ›´æ–°leavesåˆ—è¡¨
        for leaf in leaves:
            code = leaf['code']
            leaf['products'] = leaf_products.get(code, [])
            leaf['product_count'] = len(leaf['products'])
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_products = sum(len(products) for products in leaf_products.values())
        self.logger.info(f"\nâœ… æ‰©å±•å®Œæˆ:")
        self.logger.info(f"   â€¢ å¤„ç†å¶èŠ‚ç‚¹: {len(leaves)} ä¸ª")
        self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {total_products} ä¸ª")
        
        return tree_data
    
    def save_extended_cache(self, tree_data: Dict):
        """ä¿å­˜æ‰©å±•åçš„ç¼“å­˜"""
        # å¤‡ä»½åŸæ–‡ä»¶
        if self.cache_file.exists():
            backup_file = self.cache_file.with_suffix('.json.bak')
            self.cache_file.rename(backup_file)
            self.logger.info(f"ğŸ“‹ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
        
        # æ·»åŠ å…ƒæ•°æ®
        tree_data['metadata'] = {
            'generated': datetime.now().isoformat(),
            'extended_at': datetime.now().isoformat(),
            'version': '2.0-with-products',
            'total_leaves': len(tree_data.get('leaves', [])),
            'total_products': sum(leaf.get('product_count', 0) for leaf in tree_data.get('leaves', []))
        }
        
        # ä¿å­˜æ–°æ–‡ä»¶
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(tree_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ å·²ä¿å­˜æ‰©å±•åçš„ç¼“å­˜åˆ°: {self.cache_file}")
        self.logger.info(f"   æ–‡ä»¶å¤§å°: {self.cache_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    def run(self):
        """è¿è¡Œæ‰©å±•æµç¨‹"""
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ åˆ†ç±»æ ‘ç¼“å­˜æ‰©å±•å·¥å…·")
        self.logger.info("="*60)
        
        try:
            # 1. åŠ è½½ç°æœ‰åˆ†ç±»æ ‘
            tree_data = self.load_classification_tree()
            
            # 2. æ‰©å±•æ ‘ç»“æ„ï¼Œæ·»åŠ äº§å“é“¾æ¥
            extended_tree = self.extend_tree_with_products(tree_data)
            
            # 3. ä¿å­˜æ‰©å±•åçš„ç¼“å­˜
            self.save_extended_cache(extended_tree)
            
            self.logger.info("\nâœ… æ‰©å±•å®Œæˆï¼ä¸‹æ¬¡è¿è¡Œæ—¶å°†ç›´æ¥ä½¿ç”¨å®Œæ•´ç¼“å­˜ã€‚")
            
        except Exception as e:
            self.logger.error(f"æ‰©å±•å¤±è´¥: {e}", exc_info=True)
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰©å±•åˆ†ç±»æ ‘ç¼“å­˜ï¼Œæ·»åŠ äº§å“é“¾æ¥')
    parser.add_argument('--workers', type=int, default=16, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 16)')
    
    args = parser.parse_args()
    
    extender = CacheExtender(max_workers=args.workers)
    extender.run()


if __name__ == '__main__':
    main() 