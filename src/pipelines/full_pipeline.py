#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æµæ°´çº¿æ¨¡å—
=============
æ•´åˆåˆ†ç±»æ ‘ã€äº§å“é“¾æ¥ã€äº§å“è§„æ ¼çš„å®Œæ•´çˆ¬å–æµç¨‹
"""

import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

from config.settings import Settings
from config.logging_config import get_logger
from src.utils.browser_manager import create_browser_manager
from src.utils.net_guard import get_network_stats
from src.crawler.classification import ClassificationCrawler
from src.crawler.products import ProductLinksCrawler
from src.crawler.specifications import SpecificationsCrawler


logger = get_logger(__name__)


class FullPipeline:
    """å®Œæ•´çˆ¬å–æµæ°´çº¿"""
    
    def __init__(self, max_workers: int = None, browser_type: str = None):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘æ•°
            browser_type: æµè§ˆå™¨ç±»å‹
        """
        self.max_workers = max_workers or Settings.CRAWLER['max_workers']
        # åˆ›å»ºå…±äº«çš„æµè§ˆå™¨ç®¡ç†å™¨ï¼Œæ± å¤§å°ä¸å¹¶å‘æ•°åŒ¹é…
        self.browser_manager = create_browser_manager(browser_type, pool_size=self.max_workers)
        
        # åˆå§‹åŒ–å„ä¸ªçˆ¬å–å™¨ï¼Œä½¿ç”¨å…±äº«çš„æµè§ˆå™¨ç®¡ç†å™¨
        self.classification_crawler = ClassificationCrawler(self.browser_manager)
        self.products_crawler = ProductLinksCrawler(self.browser_manager)
        self.specifications_crawler = SpecificationsCrawler(self.browser_manager)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'duration': None,
            'total_leaves': 0,
            'total_products': 0,
            'total_specifications': 0,
            'success_leaves': 0,
            'success_products': 0,
            'failed_leaves': [],
            'failed_products': []
        }
    
    def run(self, output_file: str = None, cache_enabled: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        self.stats['start_time'] = datetime.now()
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çˆ¬å–æµæ°´çº¿")
        
        try:
            # 1. çˆ¬å–åˆ†ç±»æ ‘
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šçˆ¬å–åˆ†ç±»æ ‘")
            logger.info("=" * 60)
            
            root, leaves = self._crawl_classification_tree(cache_enabled)
            if not root or not leaves:
                logger.error("åˆ†ç±»æ ‘çˆ¬å–å¤±è´¥ï¼Œç»ˆæ­¢æµæ°´çº¿")
                return None
            
            self.stats['total_leaves'] = len(leaves)
            
            # 2. çˆ¬å–äº§å“é“¾æ¥
            logger.info("=" * 60)
            logger.info("ğŸ“¦ ç¬¬äºŒæ­¥ï¼šçˆ¬å–äº§å“é“¾æ¥")
            logger.info("=" * 60)
            
            leaf_products = self._crawl_product_links(leaves)
            
            # 3. çˆ¬å–äº§å“è§„æ ¼
            logger.info("=" * 60)
            logger.info("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šçˆ¬å–äº§å“è§„æ ¼")
            logger.info("=" * 60)
            
            final_data = self._crawl_specifications(leaves, leaf_products)
            
            # 4. ä¿å­˜ç»“æœ
            result = self._save_results(root, final_data, output_file)
            
            # ç»Ÿè®¡
            self._calculate_stats()
            self._print_summary()
            
            return result
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
        finally:
            # æ¸…ç†èµ„æº
            self.browser_manager.shutdown()
    
    def _crawl_classification_tree(self, use_cache: bool) -> tuple:
        """çˆ¬å–åˆ†ç±»æ ‘"""
        cache_file = Path(Settings.STORAGE['cache_dir']) / 'classification_tree.json'
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < Settings.STORAGE['cache_ttl']:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„åˆ†ç±»æ ‘ (å¹´é¾„: {cache_age/3600:.1f}å°æ—¶)")
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data['root'], data['leaves']
        
        # çˆ¬å–æ–°æ•°æ®
        root, leaves = self.classification_crawler.crawl_full_tree()
        
        # ä¿å­˜ç¼“å­˜
        if use_cache and root:
            cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({'root': root, 'leaves': leaves}, f, ensure_ascii=False, indent=2)
        
        return root, leaves
    
    def _crawl_product_links(self, leaves: List[Dict]) -> Dict[str, List[str]]:
        """æ‰¹é‡çˆ¬å–äº§å“é“¾æ¥"""
        # å¯ä»¥æŒ‰æ‰¹æ¬¡å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤š
        batch_size = 50
        all_results = {}
        
        for i in range(0, len(leaves), batch_size):
            batch = leaves[i:i+batch_size]
            logger.info(f"å¤„ç†å¶èŠ‚ç‚¹æ‰¹æ¬¡ {i//batch_size + 1}/{(len(leaves)-1)//batch_size + 1}")
            
            batch_results = self.products_crawler.extract_batch_product_links(
                batch, 
                self.max_workers
            )
            all_results.update(batch_results)
            
            # æ‰“å°è¿›åº¦
            total_so_far = sum(len(links) for links in all_results.values())
            logger.info(f"å½“å‰ç´¯è®¡äº§å“é“¾æ¥æ•°: {total_so_far}")
        
        return all_results
    
    def _crawl_specifications(self, 
                            leaves: List[Dict], 
                            leaf_products: Dict[str, List[str]]) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–äº§å“è§„æ ¼"""
        final_leaves = []
        
        for leaf in leaves:
            code = leaf['code']
            product_urls = leaf_products.get(code, [])
            
            leaf_info = {
                'name': leaf['name'],
                'code': code,
                'url': leaf['url'],
                'level': leaf['level'],
                'products': []
            }
            
            if product_urls:
                logger.info(f"å¤„ç†å¶èŠ‚ç‚¹ {code} çš„ {len(product_urls)} ä¸ªäº§å“")
                
                # æ‰¹é‡æå–è§„æ ¼
                spec_results = self.specifications_crawler.extract_batch_specifications(
                    product_urls,
                    self.max_workers
                )
                
                # æ•´ç†æ•°æ®
                for spec_result in spec_results:
                    product_info = {
                        'product_url': spec_result['product_url'],
                        'specifications': spec_result['specifications'],
                        'success': spec_result['success']
                    }
                    if not spec_result['success']:
                        product_info['error'] = spec_result.get('error', 'unknown')
                        self.stats['failed_products'].append(spec_result['product_url'])
                    
                    leaf_info['products'].append(product_info)
                
                self.stats['success_leaves'] += 1
            else:
                logger.warning(f"å¶èŠ‚ç‚¹ {code} æ²¡æœ‰äº§å“")
                self.stats['failed_leaves'].append(code)
            
            final_leaves.append(leaf_info)
            
            # ç»Ÿè®¡è§„æ ¼æ•°
            self.stats['total_specifications'] += sum(
                len(p['specifications']) for p in leaf_info['products']
            )
        
        return final_leaves
    
    def _save_results(self, root: Dict, leaves: List[Dict], output_file: str = None) -> Dict:
        """ä¿å­˜ç»“æœ"""
        if not output_file:
            timestamp = int(time.time())
            output_file = Settings.STORAGE['products_dir'] + f'/full_pipeline_{timestamp}.json'
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        result = {
            'metadata': {
                'generated': datetime.now().isoformat(),
                'version': '2.0',
                'stats': self.stats
            },
            'root': root,
            'leaves': leaves
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path.resolve()}")
        return result
    
    def _calculate_stats(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['end_time'] = datetime.now()
        self.stats['duration'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        # è®¡ç®—äº§å“æ€»æ•°
        self.stats['total_products'] = len(set(
            url for failed_list in self.stats['failed_products'] 
            for url in (failed_list if isinstance(failed_list, list) else [failed_list])
        )) + self.stats['success_products']
        
        # è·å–ç½‘ç»œç»Ÿè®¡
        self.stats['network'] = get_network_stats()
    
    def _print_summary(self):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š çˆ¬å–æ±‡æ€»")
        logger.info("=" * 60)
        
        duration_min = self.stats['duration'] / 60
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration_min:.1f} åˆ†é’Ÿ")
        logger.info(f"ğŸŒ³ å¶èŠ‚ç‚¹: {self.stats['total_leaves']} ä¸ª")
        logger.info(f"ğŸ“¦ äº§å“æ•°: {self.stats['total_products']} ä¸ª")
        logger.info(f"ğŸ“‹ è§„æ ¼æ•°: {self.stats['total_specifications']} ä¸ª")
        logger.info(f"âœ… æˆåŠŸç‡: {self.stats['success_leaves']}/{self.stats['total_leaves']} å¶èŠ‚ç‚¹")
        
        if self.stats['network']:
            logger.info(f"ğŸŒ ç½‘ç»œç»Ÿè®¡:")
            logger.info(f"   - æˆåŠŸè¯·æ±‚: {self.stats['network']['total_successes']}")
            logger.info(f"   - å¤±è´¥è¯·æ±‚: {self.stats['network']['total_failures']}")
            logger.info(f"   - æˆåŠŸç‡: {self.stats['network']['success_rate']:.1%}")
            logger.info(f"   - æš‚åœæ¬¡æ•°: {self.stats['network']['total_pauses']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TraceParts å®Œæ•´çˆ¬å–æµæ°´çº¿')
    parser.add_argument('--workers', type=int, default=None, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--browser', choices=['selenium', 'playwright'], 
                       default='selenium', help='æµè§ˆå™¨ç±»å‹')
    
    args = parser.parse_args()
    
    # è¿è¡Œæµæ°´çº¿
    pipeline = FullPipeline(
        max_workers=args.workers,
        browser_type=args.browser
    )
    
    pipeline.run(
        output_file=args.output,
        cache_enabled=not args.no_cache
    )


if __name__ == '__main__':
    main() 