#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡æ›´æ–°ç®¡ç†å™¨
=============
æ™ºèƒ½å·®å¼‚æ£€æµ‹ç³»ç»Ÿï¼Œæ”¯æŒä¸‰çº§å¢é‡æ›´æ–°ï¼š
1. æ–°å¢åˆ†ç±»æ ‘èŠ‚ç‚¹ï¼ˆæ–°ç›®å½•+å¶èŠ‚ç‚¹ï¼‰
2. å¶èŠ‚ç‚¹äº§å“æ‰©å……ï¼ˆæ–°äº§å“é“¾æ¥ï¼‰
3. äº§å“è§„æ ¼æ‰©å……ï¼ˆæ–°äº§å“è§„æ ¼ï¼‰

æ ¸å¿ƒåŸåˆ™ï¼šå®Œå…¨ä¿ç•™æ—§æ•°æ®ï¼Œåªæ·»åŠ æ–°å¢å†…å®¹
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass

from src.pipelines.cache_manager import CacheManager, CacheLevel
from src.crawler.classification_optimized import OptimizedClassificationCrawler
from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger


@dataclass
class UpdateStats:
    """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    new_leaves: int = 0
    updated_leaves: int = 0  # æœ‰æ–°äº§å“çš„å¶èŠ‚ç‚¹
    new_products: int = 0
    new_specifications: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    def get_duration_minutes(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds() / 60
        return 0.0


@dataclass 
class LeafComparison:
    """å¶èŠ‚ç‚¹å¯¹æ¯”ç»“æœ"""
    code: str
    name: str
    url: str
    old_product_count: int
    new_product_count: int
    is_new_leaf: bool
    has_new_products: bool
    new_product_urls: List[str]


class IncrementalUpdateManager:
    """å¢é‡æ›´æ–°ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = 'results/cache', max_workers: int = 16):
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers
        self.logger = ThreadSafeLogger("incremental-update", logging.INFO)
        
        # ä½¿ç”¨ç°æœ‰çš„ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(cache_dir=str(cache_dir), max_workers=max_workers)
        
        # ç‹¬ç«‹çš„çˆ¬å–å™¨ï¼ˆç”¨äºå·®å¼‚æ£€æµ‹ï¼‰
        self.classification_crawler = OptimizedClassificationCrawler()
        self.products_crawler = UltimateProductLinksCrawler()
        self.specifications_crawler = OptimizedSpecificationsCrawler(log_level=logging.INFO)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = UpdateStats()
        
        # å¤‡ä»½ç›®å½•
        self.backup_dir = self.cache_dir / 'backups'
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def run_incremental_update(self, target_level: CacheLevel = CacheLevel.SPECIFICATIONS) -> Dict:
        """
        è¿è¡Œå¢é‡æ›´æ–°
        
        Args:
            target_level: ç›®æ ‡æ›´æ–°çº§åˆ«
            
        Returns:
            æ›´æ–°åçš„å®Œæ•´æ•°æ®
        """
        self.stats.start_time = datetime.now()
        
        self.logger.info("\n" + "="*70)
        self.logger.info("ğŸ”„ TraceParts æ™ºèƒ½å¢é‡æ›´æ–°ç³»ç»Ÿ")
        self.logger.info("="*70)
        
        # æ£€æŸ¥å½“å‰ç¼“å­˜çŠ¶æ€
        current_level, current_data = self.cache_manager.get_cache_level()
        
        if current_level == CacheLevel.NONE:
            self.logger.warning("âš ï¸  æœªå‘ç°ç°æœ‰ç¼“å­˜ï¼Œå°†æ‰§è¡Œå…¨é‡æ„å»º")
            return self.cache_manager.run_progressive_cache(target_level=target_level)
        
        self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜çº§åˆ«: {current_level.name}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡æ›´æ–°çº§åˆ«: {target_level.name}")
        
        # åˆ›å»ºå¤‡ä»½
        self._create_backup(current_data, current_level)
        
        # æ‰§è¡Œå¢é‡æ›´æ–°
        updated_data = current_data.copy()
        
        try:
            # é˜¶æ®µ1ï¼šæ£€æµ‹å’Œæ›´æ–°åˆ†ç±»æ ‘
            if target_level.value >= CacheLevel.CLASSIFICATION.value:
                updated_data = self._update_classification_tree(updated_data)
            
            # é˜¶æ®µ2ï¼šæ£€æµ‹å’Œæ›´æ–°äº§å“é“¾æ¥
            if target_level.value >= CacheLevel.PRODUCTS.value:
                updated_data = self._update_product_links(updated_data)
            
            # é˜¶æ®µ3ï¼šæ£€æµ‹å’Œæ›´æ–°äº§å“è§„æ ¼
            if target_level.value >= CacheLevel.SPECIFICATIONS.value:
                updated_data = self._update_specifications(updated_data)
            
            # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
            self.cache_manager.save_cache(updated_data, target_level)
            
            # æ‰“å°æ›´æ–°æ±‡æ€»
            self._print_update_summary()
            
            return updated_data
            
        except Exception as e:
            self.logger.error(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}", exc_info=True)
            self.logger.info("ğŸ”„ å¯ä½¿ç”¨å¤‡ä»½æ–‡ä»¶æ¢å¤")
            raise
    
    def _create_backup(self, data: Dict, level: CacheLevel):
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = self.backup_dir / f'backup_{level.name.lower()}_v{timestamp}.json'
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ å·²åˆ›å»ºæ•°æ®å¤‡ä»½: {backup_file.name}")
    
    def _update_classification_tree(self, current_data: Dict) -> Dict:
        """æ›´æ–°åˆ†ç±»æ ‘ - æ£€æµ‹æ–°å¢å¶èŠ‚ç‚¹"""
        self.logger.info("\n" + "ğŸŒ³ [é˜¶æ®µ 1/3] æ£€æµ‹åˆ†ç±»æ ‘å˜åŒ–")
        self.logger.info("-" * 60)
        
        # è·å–æœ€æ–°çš„åˆ†ç±»æ ‘
        self.logger.info("ğŸŒ çˆ¬å–æœ€æ–°åˆ†ç±»æ ‘ç»“æ„...")
        new_root, new_leaves = self.classification_crawler.crawl_full_tree()
        
        # å¯¹æ¯”å¶èŠ‚ç‚¹
        old_leaf_codes = {leaf['code'] for leaf in current_data['leaves']}
        new_leaf_codes = {leaf['code'] for leaf in new_leaves}
        
        added_leaf_codes = new_leaf_codes - old_leaf_codes
        self.stats.new_leaves = len(added_leaf_codes)
        
        if added_leaf_codes:
            self.logger.info(f"ğŸ†• å‘ç°æ–°å¢å¶èŠ‚ç‚¹: {len(added_leaf_codes)} ä¸ª")
            for code in sorted(added_leaf_codes):
                new_leaf = next(leaf for leaf in new_leaves if leaf['code'] == code)
                self.logger.info(f"   â€¢ {code}: {new_leaf.get('name', '')}")
                self.logger.info(f"     URL: {new_leaf['url']}")
            
            # åˆå¹¶æ–°å¶èŠ‚ç‚¹åˆ°ç°æœ‰æ•°æ®
            updated_data = self._merge_new_leaves(current_data, new_leaves, added_leaf_codes)
            self.logger.info(f"âœ… å·²åˆå¹¶ {len(added_leaf_codes)} ä¸ªæ–°å¶èŠ‚ç‚¹")
            
        else:
            self.logger.info("âœ… åˆ†ç±»æ ‘æ— å˜åŒ–ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            updated_data = current_data
        
        return updated_data
    
    def _merge_new_leaves(self, current_data: Dict, new_leaves: List[Dict], new_leaf_codes: Set[str]) -> Dict:
        """åˆå¹¶æ–°å¢å¶èŠ‚ç‚¹åˆ°ç°æœ‰æ•°æ®"""
        updated_data = current_data.copy()
        
        # æ·»åŠ æ–°å¶èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹åˆ—è¡¨
        new_leaves_to_add = [leaf for leaf in new_leaves if leaf['code'] in new_leaf_codes]
        updated_data['leaves'].extend(new_leaves_to_add)
        
        # æ›´æ–°åˆ†ç±»æ ‘ç»“æ„ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ ‘åˆå¹¶é€»è¾‘ï¼‰
        # ç”±äºæ–°çš„åˆ†ç±»æ ‘åŒ…å«äº†å®Œæ•´ç»“æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°çš„æ ¹èŠ‚ç‚¹
        # ä½†ä¿ç•™ç°æœ‰å¶èŠ‚ç‚¹çš„äº§å“æ•°æ®
        self.logger.info("ğŸ”„ æ­£åœ¨æ›´æ–°åˆ†ç±»æ ‘ç»“æ„...")
        
        # è¿™é‡Œéœ€è¦æ™ºèƒ½åˆå¹¶æ ‘ç»“æ„ï¼Œä¿ç•™ç°æœ‰æ•°æ®
        # æš‚æ—¶ä½¿ç”¨ç®€å•ç­–ç•¥ï¼šä½¿ç”¨æ–°çš„æ ‘ç»“æ„ï¼Œä½†ä¼šåœ¨åç»­æ­¥éª¤ä¸­ä¿ç•™äº§å“æ•°æ®
        updated_data['root'] = self._crawl_new_tree_with_preserved_data(current_data)
        
        return updated_data
    
    def _crawl_new_tree_with_preserved_data(self, current_data: Dict) -> Dict:
        """çˆ¬å–æ–°çš„æ ‘ç»“æ„ï¼Œä½†ä¿ç•™ç°æœ‰çš„äº§å“æ•°æ®"""
        # è·å–æœ€æ–°çš„å®Œæ•´æ ‘ç»“æ„
        new_root, _ = self.classification_crawler.crawl_full_tree()
        
        # åˆ›å»ºç°æœ‰æ•°æ®çš„æ˜ å°„
        existing_leaf_data = {
            leaf['code']: leaf for leaf in current_data['leaves']
        }
        
        # é€’å½’æ›´æ–°æ ‘èŠ‚ç‚¹ï¼Œä¿ç•™ç°æœ‰çš„äº§å“æ•°æ®
        def preserve_existing_data(node: Dict):
            if node.get('is_leaf', False):
                code = node.get('code', '')
                if code in existing_leaf_data:
                    # ä¿ç•™ç°æœ‰çš„äº§å“æ•°æ®
                    existing_leaf = existing_leaf_data[code]
                    node['products'] = existing_leaf.get('products', [])
                    node['product_count'] = existing_leaf.get('product_count', 0)
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            for child in node.get('children', []):
                preserve_existing_data(child)
        
        preserve_existing_data(new_root)
        return new_root
    
    def _update_product_links(self, current_data: Dict) -> Dict:
        """æ›´æ–°äº§å“é“¾æ¥ - æ£€æµ‹æ–°å¢äº§å“"""
        self.logger.info("\n" + "ğŸ“¦ [é˜¶æ®µ 2/3] æ£€æµ‹äº§å“é“¾æ¥å˜åŒ–")
        self.logger.info("-" * 60)
        
        # å¯¹æ¯”æ¯ä¸ªå¶èŠ‚ç‚¹çš„äº§å“æ•°é‡
        leaf_comparisons = self._compare_leaf_products(current_data['leaves'])
        
        # ç»Ÿè®¡éœ€è¦æ›´æ–°çš„å¶èŠ‚ç‚¹
        leaves_to_update = [comp for comp in leaf_comparisons if comp.is_new_leaf or comp.has_new_products]
        
        if not leaves_to_update:
            self.logger.info("âœ… æ‰€æœ‰å¶èŠ‚ç‚¹äº§å“æ— å˜åŒ–ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return current_data
        
        self.logger.info(f"ğŸ“Š äº§å“é“¾æ¥å˜åŒ–ç»Ÿè®¡:")
        self.logger.info(f"   â€¢ æ–°å¢å¶èŠ‚ç‚¹: {sum(1 for c in leaf_comparisons if c.is_new_leaf)} ä¸ª")
        self.logger.info(f"   â€¢ æœ‰æ–°äº§å“çš„å¶èŠ‚ç‚¹: {sum(1 for c in leaf_comparisons if c.has_new_products and not c.is_new_leaf)} ä¸ª")
        
        # æ›´æ–°æœ‰å˜åŒ–çš„å¶èŠ‚ç‚¹
        updated_data = self._update_changed_leaves(current_data, leaves_to_update)
        
        return updated_data
    
    def _compare_leaf_products(self, current_leaves: List[Dict]) -> List[LeafComparison]:
        """å¯¹æ¯”å¶èŠ‚ç‚¹çš„äº§å“æ•°é‡å˜åŒ–"""
        comparisons = []
        
        for leaf in current_leaves:
            self.logger.info(f"ğŸ” æ£€æŸ¥å¶èŠ‚ç‚¹: {leaf['code']}")
            
            try:
                # è·å–å½“å‰äº§å“é“¾æ¥æ•°é‡
                current_product_urls = leaf.get('products', [])
                if isinstance(current_product_urls, list) and current_product_urls:
                    if isinstance(current_product_urls[0], dict):
                        # æ–°æ ¼å¼ï¼šåŒ…å«è§„æ ¼çš„å­—å…¸
                        current_urls = [p['product_url'] for p in current_product_urls]
                    else:
                        # æ—§æ ¼å¼ï¼šç›´æ¥çš„URLåˆ—è¡¨
                        current_urls = current_product_urls
                else:
                    current_urls = []
                
                old_count = len(current_urls)
                
                # çˆ¬å–æœ€æ–°çš„äº§å“é“¾æ¥
                latest_urls = self.products_crawler.extract_product_links(leaf['url'])
                new_count = len(latest_urls)
                
                # æ£€æµ‹æ–°å¢äº§å“
                current_url_set = set(current_urls)
                latest_url_set = set(latest_urls)
                new_product_urls = list(latest_url_set - current_url_set)
                
                is_new_leaf = old_count == 0  # å¦‚æœåŸæ¥æ²¡æœ‰äº§å“ï¼Œè®¤ä¸ºæ˜¯æ–°å¶èŠ‚ç‚¹
                has_new_products = len(new_product_urls) > 0
                
                comparison = LeafComparison(
                    code=leaf['code'],
                    name=leaf.get('name', ''),
                    url=leaf['url'],
                    old_product_count=old_count,
                    new_product_count=new_count,
                    is_new_leaf=is_new_leaf,
                    has_new_products=has_new_products,
                    new_product_urls=new_product_urls
                )
                
                if has_new_products or is_new_leaf:
                    self.logger.info(f"   ğŸ“ˆ {leaf['code']}: {old_count} â†’ {new_count} (+{len(new_product_urls)})")
                else:
                    self.logger.info(f"   âœ… {leaf['code']}: {old_count} (æ— å˜åŒ–)")
                
                comparisons.append(comparison)
                
            except Exception as e:
                self.logger.error(f"   âŒ {leaf['code']}: æ£€æŸ¥å¤±è´¥ - {e}")
                # åˆ›å»ºä¸€ä¸ªå¤±è´¥çš„å¯¹æ¯”ç»“æœ
                comparison = LeafComparison(
                    code=leaf['code'],
                    name=leaf.get('name', ''),
                    url=leaf['url'],
                    old_product_count=len(leaf.get('products', [])),
                    new_product_count=0,
                    is_new_leaf=False,
                    has_new_products=False,
                    new_product_urls=[]
                )
                comparisons.append(comparison)
        
        return comparisons
    
    def _update_changed_leaves(self, current_data: Dict, leaves_to_update: List[LeafComparison]) -> Dict:
        """æ›´æ–°æœ‰å˜åŒ–çš„å¶èŠ‚ç‚¹"""
        updated_data = current_data.copy()
        
        for comparison in leaves_to_update:
            try:
                self.logger.info(f"ğŸ”„ æ›´æ–°å¶èŠ‚ç‚¹: {comparison.code}")
                
                if comparison.is_new_leaf:
                    # æ–°å¶èŠ‚ç‚¹ï¼šçˆ¬å–æ‰€æœ‰äº§å“
                    new_urls = self.products_crawler.extract_product_links(comparison.url)
                    self.logger.info(f"   ğŸ†• æ–°å¶èŠ‚ç‚¹ï¼Œçˆ¬å–åˆ° {len(new_urls)} ä¸ªäº§å“")
                    self.stats.new_products += len(new_urls)
                else:
                    # ç°æœ‰å¶èŠ‚ç‚¹ï¼šåªæ·»åŠ æ–°äº§å“
                    new_urls = comparison.new_product_urls
                    self.logger.info(f"   ğŸ“ˆ æ–°å¢ {len(new_urls)} ä¸ªäº§å“")
                    self.stats.new_products += len(new_urls)
                
                # æ›´æ–°å¶èŠ‚ç‚¹æ•°æ®
                self._merge_new_products_to_leaf(updated_data, comparison, new_urls)
                self.stats.updated_leaves += 1
                
            except Exception as e:
                self.logger.error(f"âŒ æ›´æ–°å¶èŠ‚ç‚¹ {comparison.code} å¤±è´¥: {e}")
        
        return updated_data
    
    def _merge_new_products_to_leaf(self, data: Dict, comparison: LeafComparison, new_urls: List[str]):
        """å°†æ–°äº§å“åˆå¹¶åˆ°å¶èŠ‚ç‚¹"""
        # æ›´æ–°å¶èŠ‚ç‚¹åˆ—è¡¨ä¸­çš„æ•°æ®
        for leaf in data['leaves']:
            if leaf['code'] == comparison.code:
                existing_products = leaf.get('products', [])
                
                if comparison.is_new_leaf:
                    # æ–°å¶èŠ‚ç‚¹ï¼šç›´æ¥è®¾ç½®äº§å“åˆ—è¡¨
                    leaf['products'] = new_urls
                    leaf['product_count'] = len(new_urls)
                else:
                    # ç°æœ‰å¶èŠ‚ç‚¹ï¼šåˆå¹¶æ–°äº§å“
                    if isinstance(existing_products, list) and existing_products:
                        if isinstance(existing_products[0], dict):
                            # æ–°æ ¼å¼ï¼šä¿æŒå­—å…¸æ ¼å¼
                            existing_urls = {p['product_url'] for p in existing_products}
                            for url in new_urls:
                                if url not in existing_urls:
                                    existing_products.append({'product_url': url})
                        else:
                            # æ—§æ ¼å¼ï¼šURLåˆ—è¡¨
                            existing_url_set = set(existing_products)
                            for url in new_urls:
                                if url not in existing_url_set:
                                    existing_products.append(url)
                    else:
                        # ç©ºåˆ—è¡¨æˆ–None
                        leaf['products'] = new_urls
                    
                    leaf['product_count'] = len(leaf['products'])
                break
        
        # åŒæ—¶æ›´æ–°æ ‘ç»“æ„ä¸­çš„æ•°æ®
        def update_tree_node(node: Dict):
            if node.get('is_leaf', False) and node.get('code') == comparison.code:
                # ä¸å¶èŠ‚ç‚¹åˆ—è¡¨ä¿æŒåŒæ­¥
                leaf_data = next(leaf for leaf in data['leaves'] if leaf['code'] == comparison.code)
                node['products'] = leaf_data['products']
                node['product_count'] = leaf_data['product_count']
            
            for child in node.get('children', []):
                update_tree_node(child)
        
        update_tree_node(data['root'])
    
    def _update_specifications(self, current_data: Dict) -> Dict:
        """æ›´æ–°äº§å“è§„æ ¼ - æ£€æµ‹æ–°å¢äº§å“è§„æ ¼"""
        self.logger.info("\n" + "ğŸ“‹ [é˜¶æ®µ 3/3] æ£€æµ‹äº§å“è§„æ ¼å˜åŒ–")
        self.logger.info("-" * 60)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦çˆ¬å–è§„æ ¼çš„æ–°äº§å“
        products_needing_specs = self._identify_products_needing_specs(current_data)
        
        if not products_needing_specs:
            self.logger.info("âœ… æ— æ–°äº§å“éœ€è¦çˆ¬å–è§„æ ¼ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return current_data
        
        self.logger.info(f"ğŸ“Š éœ€è¦çˆ¬å–è§„æ ¼çš„äº§å“: {len(products_needing_specs)} ä¸ª")
        
        # æ‰¹é‡çˆ¬å–æ–°äº§å“è§„æ ¼
        updated_data = self._crawl_specifications_for_new_products(current_data, products_needing_specs)
        
        return updated_data
    
    def _identify_products_needing_specs(self, current_data: Dict) -> List[Dict]:
        """è¯†åˆ«éœ€è¦çˆ¬å–è§„æ ¼çš„äº§å“"""
        products_needing_specs = []
        
        for leaf in current_data['leaves']:
            for product in leaf.get('products', []):
                if isinstance(product, str):
                    # æ—§æ ¼å¼çš„URLï¼Œéœ€è¦çˆ¬å–è§„æ ¼
                    products_needing_specs.append({
                        'product_url': product,
                        'leaf_code': leaf['code'],
                        'is_new': True
                    })
                elif isinstance(product, dict):
                    if 'specifications' not in product or not product.get('specifications'):
                        # å­—å…¸æ ¼å¼ä½†æ²¡æœ‰è§„æ ¼æ•°æ®
                        products_needing_specs.append({
                            'product_url': product['product_url'],
                            'leaf_code': leaf['code'],
                            'is_new': True
                        })
        
        return products_needing_specs
    
    def _crawl_specifications_for_new_products(self, current_data: Dict, products_needing_specs: List[Dict]) -> Dict:
        """ä¸ºæ–°äº§å“çˆ¬å–è§„æ ¼"""
        # æå–URLåˆ—è¡¨
        product_urls = [p['product_url'] for p in products_needing_specs]
        
        # æ‰¹é‡çˆ¬å–è§„æ ¼
        self.logger.info(f"ğŸŒ å¼€å§‹æ‰¹é‡çˆ¬å– {len(product_urls)} ä¸ªäº§å“çš„è§„æ ¼...")
        batch_result = self.specifications_crawler.extract_batch_specifications(
            product_urls, 
            max_workers=min(len(product_urls), 12)
        )
        
        # å¤„ç†ç»“æœ
        product_specs = {}
        success_count = 0
        
        for result in batch_result.get('results', []):
            product_url = result['product_url']
            specs = result.get('specifications', [])
            product_specs[product_url] = specs
            
            if result.get('success', False):
                success_count += 1
                self.stats.new_specifications += len(specs)
        
        self.logger.info(f"âœ… è§„æ ¼çˆ¬å–å®Œæˆ: {success_count}/{len(product_urls)} æˆåŠŸ")
        self.logger.info(f"ğŸ“Š æ–°å¢è§„æ ¼æ€»æ•°: {self.stats.new_specifications}")
        
        # æ›´æ–°æ•°æ®ç»“æ„
        updated_data = self._merge_specifications_to_data(current_data, product_specs)
        
        return updated_data
    
    def _merge_specifications_to_data(self, current_data: Dict, product_specs: Dict[str, List[Dict]]) -> Dict:
        """å°†è§„æ ¼æ•°æ®åˆå¹¶åˆ°ç°æœ‰æ•°æ®"""
        updated_data = current_data.copy()
        
        # æ›´æ–°å¶èŠ‚ç‚¹ä¸­çš„äº§å“æ ¼å¼
        for leaf in updated_data['leaves']:
            updated_products = []
            for product in leaf.get('products', []):
                if isinstance(product, str):
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼å¹¶æ·»åŠ è§„æ ¼
                    product_info = {
                        'product_url': product,
                        'specifications': product_specs.get(product, []),
                        'spec_count': len(product_specs.get(product, []))
                    }
                else:
                    # æ›´æ–°ç°æœ‰å­—å…¸
                    if 'specifications' not in product or not product.get('specifications'):
                        product['specifications'] = product_specs.get(product['product_url'], [])
                        product['spec_count'] = len(product['specifications'])
                    product_info = product
                
                updated_products.append(product_info)
            
            leaf['products'] = updated_products
        
        # åŒæ­¥æ›´æ–°æ ‘ç»“æ„
        def update_tree_node(node: Dict):
            if node.get('is_leaf', False):
                # æ‰¾åˆ°å¯¹åº”çš„å¶èŠ‚ç‚¹æ•°æ®
                leaf_data = next((leaf for leaf in updated_data['leaves'] if leaf['code'] == node.get('code')), None)
                if leaf_data:
                    node['products'] = leaf_data['products']
                    node['product_count'] = leaf_data.get('product_count', 0)
            
            for child in node.get('children', []):
                update_tree_node(child)
        
        update_tree_node(updated_data['root'])
        
        return updated_data
    
    def _print_update_summary(self):
        """æ‰“å°æ›´æ–°æ±‡æ€»"""
        self.stats.end_time = datetime.now()
        duration = self.stats.get_duration_minutes()
        
        self.logger.info("\n" + "="*70)
        self.logger.info("ğŸ“Š å¢é‡æ›´æ–°å®Œæˆ - æ±‡æ€»æŠ¥å‘Š")
        self.logger.info("="*70)
        
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f} åˆ†é’Ÿ")
        self.logger.info(f"\nğŸ“ˆ æ›´æ–°ç»Ÿè®¡:")
        self.logger.info(f"   â€¢ æ–°å¢å¶èŠ‚ç‚¹: {self.stats.new_leaves} ä¸ª")
        self.logger.info(f"   â€¢ æ›´æ–°å¶èŠ‚ç‚¹: {self.stats.updated_leaves} ä¸ª")
        self.logger.info(f"   â€¢ æ–°å¢äº§å“: {self.stats.new_products} ä¸ª")
        self.logger.info(f"   â€¢ æ–°å¢è§„æ ¼: {self.stats.new_specifications} ä¸ª")
        
        self.logger.info(f"\nğŸ’¾ æ•°æ®ä¿å­˜:")
        self.logger.info(f"   â€¢ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        self.logger.info(f"   â€¢ å¤‡ä»½ç›®å½•: {self.backup_dir}")
        
        self.logger.info("="*70) 