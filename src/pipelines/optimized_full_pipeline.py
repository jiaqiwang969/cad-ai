#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆå®Œæ•´æµæ°´çº¿æ¨¡å—
==================
åº”ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–ç»éªŒçš„å®Œæ•´çˆ¬å–æµç¨‹
"""

import json
import time
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¯¼å…¥ä¼˜åŒ–ç‰ˆçˆ¬å–å™¨
from src.crawler.classification_optimized import OptimizedClassificationCrawler
from src.crawler.ultimate_products import UltimateProductLinksCrawler
from src.crawler.specifications_optimized import OptimizedSpecificationsCrawler
from src.utils.thread_safe_logger import ThreadSafeLogger, ProgressTracker


class OptimizedFullPipeline:
    """ä¼˜åŒ–ç‰ˆå®Œæ•´çˆ¬å–æµæ°´çº¿"""
    
    # é¢„ç¼–è¯‘é…ç½®å¸¸é‡
    DEFAULT_MAX_WORKERS = 32
    BATCH_SIZE = 50
    CACHE_TTL = 86400  # 24å°æ—¶
    
    def __init__(self, max_workers: int = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆæµæ°´çº¿
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘æ•°
        """
        self.max_workers = max_workers or self.DEFAULT_MAX_WORKERS
        
        # ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ—¥å¿—å™¨
        self.logger = ThreadSafeLogger("opt-pipeline", logging.INFO)
        self.progress_tracker = ProgressTracker(self.logger)
        
        # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆçˆ¬å–å™¨ (ä¸ä½¿ç”¨æµè§ˆå™¨æ± )
        self.classification_crawler = OptimizedClassificationCrawler()
        self.products_crawler = UltimateProductLinksCrawler()
        self.specifications_crawler = OptimizedSpecificationsCrawler()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'duration': None,
            'total_leaves': 0,
            'total_products': 0,
            'total_specifications': 0,
            'success_leaves': 0,
            'success_products': 0,
            'failed_leaves': [],
            'failed_products': []
        }
    
    def run(self, output_file: str = None, cache_enabled: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œä¼˜åŒ–ç‰ˆå®Œæ•´æµæ°´çº¿
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        self.stats['start_time'] = datetime.now()
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ TraceParts äº§å“æ•°æ®çˆ¬å–ç³»ç»Ÿ v3.0")
        self.logger.info("="*60)
        self.logger.info("ğŸ“‹ çˆ¬å–æµç¨‹ï¼š")
        self.logger.info("   1. çˆ¬å–äº§å“åˆ†ç±»æ ‘ â†’ è·å–æ‰€æœ‰å¶èŠ‚ç‚¹")
        self.logger.info("   2. æå–äº§å“é“¾æ¥ â†’ ä»æ¯ä¸ªå¶èŠ‚ç‚¹è·å–äº§å“URL")
        self.logger.info("   3. è·å–äº§å“è§„æ ¼ â†’ çˆ¬å–æ¯ä¸ªäº§å“çš„è¯¦ç»†å‚æ•°")
        self.logger.info("")
        self.logger.info(f"âš™ï¸  è¿è¡Œé…ç½®:")
        self.logger.info(f"   â€¢ å¹¶å‘æ•°: {self.max_workers} ä¸ªçº¿ç¨‹")
        self.logger.info(f"   â€¢ ç¼“å­˜çŠ¶æ€: {'âœ… å¯ç”¨' if cache_enabled else 'âŒ ç¦ç”¨'}")
        if cache_enabled:
            self.logger.info(f"   â€¢ ç¼“å­˜æ—¶æ•ˆ: 24 å°æ—¶")
        self.logger.info("="*60)
        
        try:
            # 1. çˆ¬å–åˆ†ç±»æ ‘
            self.logger.info("\n[æ­¥éª¤ 1/3] çˆ¬å–äº§å“åˆ†ç±»æ ‘")
            self.logger.info("-" * 50)
            
            root, leaves = self._crawl_classification_tree(cache_enabled)
            if not root or not leaves:
                self.logger.error("âŒ åˆ†ç±»æ ‘çˆ¬å–å¤±è´¥ï¼Œç»ˆæ­¢æµæ°´çº¿")
                return None
            
            self.stats['total_leaves'] = len(leaves)
            self.logger.info(f"âœ… åˆ†ç±»æ ‘çˆ¬å–å®Œæˆ: å…±å‘ç° {len(leaves)} ä¸ªå¶èŠ‚ç‚¹")
            
            # 2. çˆ¬å–äº§å“é“¾æ¥
            self.logger.info("\n[æ­¥éª¤ 2/3] ä»å¶èŠ‚ç‚¹æå–äº§å“é“¾æ¥")
            self.logger.info("-" * 50)
            self.logger.info(f"å‡†å¤‡ä» {len(leaves)} ä¸ªå¶èŠ‚ç‚¹å¹¶è¡Œæå–äº§å“é“¾æ¥...")
            
            leaf_products = self._crawl_product_links_parallel(leaves)
            
            # 3. çˆ¬å–äº§å“è§„æ ¼
            self.logger.info("\n[æ­¥éª¤ 3/3] çˆ¬å–äº§å“è¯¦ç»†è§„æ ¼")
            self.logger.info("-" * 50)
            self.logger.info(f"å‡†å¤‡çˆ¬å– {self.stats['total_products']} ä¸ªäº§å“çš„è¯¦ç»†è§„æ ¼å‚æ•°...")
            
            final_data = self._crawl_specifications_parallel(leaves, leaf_products)
            
            # 4. ä¿å­˜ç»“æœ
            result = self._save_results(root, final_data, output_file)
            
            # ç»Ÿè®¡
            self._calculate_stats()
            self._print_summary()
            
            return result
            
        except Exception as e:
            self.logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
    
    def _crawl_classification_tree(self, use_cache: bool) -> tuple:
        """çˆ¬å–åˆ†ç±»æ ‘ (ä¼˜åŒ–ç‰ˆ)"""
        # ç®€åŒ–çš„ç¼“å­˜è·¯å¾„
        cache_file = Path('results/cache/classification_tree.json')
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and cache_file.exists():
            cache_age = time.time() - cache_file.stat().st_mtime
            if cache_age < self.CACHE_TTL:
                self.logger.info(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„åˆ†ç±»æ ‘")
                self.logger.info(f"   ç¼“å­˜æ–‡ä»¶: {cache_file.resolve()}")
                self.logger.info(f"   ç¼“å­˜å¹´é¾„: {cache_age/3600:.1f} å°æ—¶")
                
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•ç‰ˆæœ¬ï¼ˆåŒ…å«äº§å“é“¾æ¥ï¼‰
                metadata = data.get('metadata', {})
                version = metadata.get('version', '1.0')
                
                if 'with-products' in version:
                    self.logger.info(f"   âœ¨ ç¼“å­˜ç‰ˆæœ¬: {version} (åŒ…å«äº§å“é“¾æ¥)")
                    self.logger.info(f"   äº§å“æ€»æ•°: {metadata.get('total_products', 0)} ä¸ª")
                    self._has_cached_products = True
                else:
                    self.logger.info(f"   ç¼“å­˜ç‰ˆæœ¬: {version} (ä»…åˆ†ç±»æ ‘)")
                    self._has_cached_products = False
                
                self.logger.info(f"   ğŸ’¡ æç¤º: ä½¿ç”¨ --no-cache å‚æ•°å¼ºåˆ¶é‡æ–°çˆ¬å–")
                return data['root'], data['leaves']
            else:
                self.logger.info(f"âš ï¸  ç¼“å­˜å·²è¿‡æœŸ (å¹´é¾„: {cache_age/3600:.1f} å°æ—¶ > 24å°æ—¶)")
                self.logger.info(f"   å°†é‡æ–°çˆ¬å–åˆ†ç±»æ ‘...")
        
        # çˆ¬å–æ–°æ•°æ®
        self._has_cached_products = False
        if not use_cache:
            self.logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°çˆ¬å–åˆ†ç±»æ ‘ (--no-cache å‚æ•°)")
        elif not cache_file.exists():
            self.logger.info(f"ğŸ“‚ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜")
            self.logger.info(f"   ç¼“å­˜è·¯å¾„: {cache_file.resolve()}")
        
        self.logger.info(f"ğŸŒ å¼€å§‹çˆ¬å–åˆ†ç±»æ ‘...")
        root, leaves = self.classification_crawler.crawl_full_tree()
        
        # ä¿å­˜ç¼“å­˜
        if use_cache and root:
            cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({'root': root, 'leaves': leaves}, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ’¾ åˆ†ç±»æ ‘å·²ç¼“å­˜åˆ°: {cache_file.resolve()}")
        
        return root, leaves
    
    def _crawl_product_links_parallel(self, leaves: List[Dict]) -> Dict[str, List[str]]:
        """å¹¶è¡Œçˆ¬å–äº§å“é“¾æ¥ (ä¼˜åŒ–ç‰ˆ)"""
        all_results = {}
        
        # å¦‚æœç¼“å­˜ä¸­å·²åŒ…å«äº§å“é“¾æ¥ï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(self, '_has_cached_products') and self._has_cached_products:
            self.logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„äº§å“é“¾æ¥æ•°æ®")
            
            # ä»å¶èŠ‚ç‚¹æå–äº§å“é“¾æ¥
            for leaf in leaves:
                products = leaf.get('products', [])
                all_results[leaf['code']] = products
                
                if products:
                    self.stats['success_leaves'] += 1
                    self.stats['total_products'] += len(products)
                else:
                    self.stats['failed_leaves'].append(leaf['code'])
            
            # æ˜¾ç¤ºç»Ÿè®¡
            self.logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å®Œæˆ:")
            self.logger.info(f"   â€¢ æˆåŠŸå¶èŠ‚ç‚¹: {self.stats['success_leaves']}/{len(leaves)}")
            self.logger.info(f"   â€¢ ç©ºå¶èŠ‚ç‚¹: {len(self.stats['failed_leaves'])}")
            self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {self.stats['total_products']} ä¸ª")
            self.logger.info(f"   ğŸ’¡ æç¤º: ä½¿ç”¨ --no-cache å‚æ•°å¼ºåˆ¶é‡æ–°çˆ¬å–")
            
            return all_results
        
        # æ³¨å†Œä»»åŠ¡
        self.progress_tracker.register_task("äº§å“é“¾æ¥æå–", len(leaves))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        self.logger.info(f"å¯åŠ¨ {self.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹...")
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            self.logger.info("æ­£åœ¨æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± ...")
            future_to_leaf = {
                executor.submit(
                    self.products_crawler.extract_product_links, 
                    leaf['url']
                ): leaf 
                for leaf in leaves
            }
            self.logger.info(f"å·²æäº¤ {len(future_to_leaf)} ä¸ªä»»åŠ¡ï¼Œå¼€å§‹å¹¶è¡Œå¤„ç†...")
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_leaf):
                leaf = future_to_leaf[future]
                completed += 1
                
                try:
                    links = future.result()
                    all_results[leaf['code']] = links
                    
                    if links:
                        self.stats['success_leaves'] += 1
                        self.stats['total_products'] += len(links)
                        self.progress_tracker.update_task("äº§å“é“¾æ¥æå–", success=True)
                        # æ˜¾ç¤ºå…·ä½“çš„å¶èŠ‚ç‚¹ä¿¡æ¯å’Œäº§å“æ•°
                        self.logger.info(f"âœ… å¶èŠ‚ç‚¹ {leaf['code']} äº§å“æ•°: {len(links)}")
                        self.logger.info(f"   åœ°å€: {leaf['url']}")
                    else:
                        self.stats['failed_leaves'].append(leaf['code'])
                        self.progress_tracker.update_task("äº§å“é“¾æ¥æå–", success=False)
                        self.logger.warning(f"âš ï¸  å¶èŠ‚ç‚¹ {leaf['code']} æ— äº§å“")
                        self.logger.warning(f"   åœ°å€: {leaf['url']}")
                        
                except Exception as e:
                    self.logger.error(f"âŒ å¶èŠ‚ç‚¹ {leaf['code']} å¤±è´¥: {e}")
                    self.logger.error(f"   åœ°å€: {leaf['url']}")
                    all_results[leaf['code']] = []
                    self.stats['failed_leaves'].append(leaf['code'])
                    self.progress_tracker.update_task("äº§å“é“¾æ¥æå–", success=False)
                
                # å®šæœŸæ±‡æ€»å½“å‰è¿›åº¦
                if completed % 50 == 0 or completed == len(leaves):
                    total_products = sum(len(links) for links in all_results.values())
                    self.logger.info(f"\nå½“å‰ç´¯è®¡äº§å“é“¾æ¥æ•°: {total_products}")
        

        
        # äº§å“é“¾æ¥æå–æ±‡æ€»
        self.logger.info(f"\nâœ… äº§å“é“¾æ¥æå–å®Œæˆ:")
        self.logger.info(f"   â€¢ æˆåŠŸå¶èŠ‚ç‚¹: {self.stats['success_leaves']}/{len(leaves)}")
        self.logger.info(f"   â€¢ å¤±è´¥å¶èŠ‚ç‚¹: {len(self.stats['failed_leaves'])}")
        self.logger.info(f"   â€¢ æ€»äº§å“æ•°: {self.stats['total_products']} ä¸ª")
        
        return all_results
    
    def _crawl_specifications_parallel(self, 
                                     leaves: List[Dict], 
                                     leaf_products: Dict[str, List[str]]) -> List[Dict]:
        """å¹¶è¡Œçˆ¬å–äº§å“è§„æ ¼ (ä¼˜åŒ–ç‰ˆ)"""
        final_leaves = []
        
        # å‡†å¤‡æ‰€æœ‰äº§å“URLåŠå…¶æ‰€å±å¶èŠ‚ç‚¹
        all_product_tasks = []
        for leaf in leaves:
            code = leaf['code']
            product_urls = leaf_products.get(code, [])
            
            leaf_info = {
                'name': leaf['name'],
                'code': code,
                'url': leaf['url'],
                'level': leaf['level'],
                'products': []
            }
            
            if product_urls:
                # å¤„ç†æœ‰äº§å“çš„å¶èŠ‚ç‚¹
                self.logger.info(f"å¤„ç†å¶èŠ‚ç‚¹ {code} çš„ {len(product_urls)} ä¸ªäº§å“")
                for url in product_urls:
                    all_product_tasks.append((url, leaf_info))
            
            final_leaves.append(leaf_info)
        
        # æ³¨å†Œä»»åŠ¡
        self.progress_tracker.register_task("äº§å“è§„æ ¼æå–", len(all_product_tasks))
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰äº§å“
        if len(all_product_tasks) == 0:
            self.logger.warning("æ²¡æœ‰äº§å“éœ€è¦çˆ¬å–è§„æ ¼")
            return final_leaves
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(
                    self.specifications_crawler.extract_specifications,
                    task[0]
                ): task
                for task in all_product_tasks
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_task):
                url, leaf_info = future_to_task[future]
                completed += 1
                
                try:
                    spec_result = future.result()
                    
                    product_info = {
                        'product_url': spec_result['product_url'],
                        'specifications': spec_result['specifications'],
                        'success': spec_result['success']
                    }
                    
                    if not spec_result['success']:
                        product_info['error'] = spec_result.get('error', 'unknown')
                        self.stats['failed_products'].append(spec_result['product_url'])
                        self.progress_tracker.update_task("äº§å“è§„æ ¼æå–", success=False)
                    else:
                        self.stats['success_products'] += 1
                        self.stats['total_specifications'] += len(spec_result['specifications'])
                        self.progress_tracker.update_task("äº§å“è§„æ ¼æå–", success=True)
                        
                        # æ˜¾ç¤ºå…·ä½“çš„äº§å“è§„æ ¼æå–æˆåŠŸä¿¡æ¯
                        if self.stats['success_products'] <= 5 or self.stats['success_products'] % 100 == 0:
                            self.logger.info(f"âœ… [{self.stats['success_products']}/{len(all_product_tasks)}] "
                                           f"æå–æˆåŠŸ: {len(spec_result['specifications'])} ä¸ªè§„æ ¼ - {url}")
                    
                    leaf_info['products'].append(product_info)
                    
                    # å®šæœŸæ˜¾ç¤ºè¿›åº¦æ±‡æ€»
                    if completed % 500 == 0 or completed == len(all_product_tasks):
                        self.logger.info(f"\nè¿›åº¦æ±‡æ€»: {completed}/{len(all_product_tasks)} äº§å“å·²å¤„ç†")
                        self.logger.info(f"æˆåŠŸ: {self.stats['success_products']}, å¤±è´¥: {len(self.stats['failed_products'])}")
                        
                except Exception as e:
                    self.logger.error(f"äº§å“ {url} è§„æ ¼æå–å¤±è´¥: {e}")
                    leaf_info['products'].append({
                        'product_url': url,
                        'specifications': [],
                        'success': False,
                        'error': str(e)
                    })
                    self.stats['failed_products'].append(url)
                    self.progress_tracker.update_task("äº§å“è§„æ ¼æå–", success=False)
        

        
        # äº§å“è§„æ ¼çˆ¬å–æ±‡æ€»
        self.logger.info(f"\nâœ… äº§å“è§„æ ¼çˆ¬å–å®Œæˆ:")
        self.logger.info(f"   â€¢ æˆåŠŸäº§å“: {self.stats['success_products']}/{len(all_product_tasks)}")
        self.logger.info(f"   â€¢ å¤±è´¥äº§å“: {len(self.stats['failed_products'])}")
        self.logger.info(f"   â€¢ æ€»è§„æ ¼æ•°: {self.stats['total_specifications']} ä¸ª")
        
        return final_leaves
    
    def _save_results(self, root: Dict, leaves: List[Dict], output_file: str = None) -> Dict:
        """ä¿å­˜ç»“æœ (ä¼˜åŒ–ç‰ˆ)"""
        if not output_file:
            timestamp = int(time.time())
            output_file = f'results/products/optimized_pipeline_{timestamp}.json'
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        result = {
            'metadata': {
                'generated': datetime.now().isoformat(),
                'version': '3.0-optimized',
                'stats': self.stats
            },
            'root': root,
            'leaves': leaves
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path.resolve()}")
        self.logger.info(f"   æ–‡ä»¶å: {output_path.name}")
        self.logger.info(f"   å¤§å°: {output_path.stat().st_size / 1024 / 1024:.1f} MB")
        return result
    
    def _calculate_stats(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['end_time'] = datetime.now()
        self.stats['duration'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
    
    def _print_summary(self):
        """æ‰“å°æœ€ç»ˆæ±‡æ€»"""
        duration_min = self.stats['duration'] / 60
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š çˆ¬å–å®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        self.logger.info("="*60)
        
        # æ—¶é—´ç»Ÿè®¡
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration_min:.1f} åˆ†é’Ÿ")
        
        # æ•°æ®ç»Ÿè®¡
        self.logger.info(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        self.logger.info(f"   â€¢ å¶èŠ‚ç‚¹: {self.stats['total_leaves']} ä¸ª")
        self.logger.info(f"   â€¢ äº§å“æ•°: {self.stats['total_products']} ä¸ª")
        self.logger.info(f"   â€¢ è§„æ ¼æ•°: {self.stats['total_specifications']} ä¸ª")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        leaf_success_rate = (self.stats['success_leaves'] / self.stats['total_leaves'] * 100) if self.stats['total_leaves'] > 0 else 0
        self.logger.info(f"\nğŸ“Š æˆåŠŸç‡:")
        self.logger.info(f"   â€¢ å¶èŠ‚ç‚¹æˆåŠŸç‡: {leaf_success_rate:.1f}% ({self.stats['success_leaves']}/{self.stats['total_leaves']})")
        self.logger.info(f"   â€¢ äº§å“æˆåŠŸ: {self.stats['success_products']} ä¸ª")
        self.logger.info(f"   â€¢ äº§å“å¤±è´¥: {len(self.stats['failed_products'])} ä¸ª")
        
        self.logger.info("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TraceParts ä¼˜åŒ–ç‰ˆå®Œæ•´çˆ¬å–æµæ°´çº¿')
    parser.add_argument('--workers', type=int, default=32, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 32)')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    
    args = parser.parse_args()
    
    # ä¸å†è®¾ç½®basicConfigï¼Œè®©ThreadSafeLoggerå®Œå…¨æ§åˆ¶æ—¥å¿—è¾“å‡º
    # è¿è¡Œä¼˜åŒ–ç‰ˆæµæ°´çº¿
    pipeline = OptimizedFullPipeline(max_workers=args.workers)
    
    pipeline.run(
        output_file=args.output,
        cache_enabled=not args.no_cache
    )


if __name__ == '__main__':
    main() 