#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆå®Œæ•´æµæ°´çº¿ V2 - åŸºäºæ¸è¿›å¼ç¼“å­˜ç®¡ç†å™¨
=======================================
ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨ï¼Œæ”¯æŒä¸‰é˜¶æ®µç¼“å­˜
"""

import json
import time
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

from src.pipelines.cache_manager import CacheManager, CacheLevel
from src.utils.thread_safe_logger import ThreadSafeLogger


class OptimizedFullPipelineV2:
    """åŸºäºç¼“å­˜ç®¡ç†å™¨çš„ä¼˜åŒ–æµæ°´çº¿"""
    
    def __init__(self, max_workers: int = 32, cache_dir: str = 'results/cache'):
        self.max_workers = max_workers
        self.cache_dir = cache_dir
        self.logger = ThreadSafeLogger("pipeline-v2", logging.INFO)
        
        # ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(cache_dir=cache_dir, max_workers=max_workers)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'duration': None,
            'cache_level_start': None,
            'cache_level_end': None,
            'total_leaves': 0,
            'total_products': 0,
            'total_specifications': 0
        }
    
    def run(self, output_file: str = None, cache_enabled: bool = True, target_level: CacheLevel = CacheLevel.SPECIFICATIONS):
        """
        è¿è¡Œä¼˜åŒ–ç‰ˆæµæ°´çº¿V2
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            target_level: ç›®æ ‡ç¼“å­˜çº§åˆ«
        """
        self.stats['start_time'] = datetime.now()
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸš€ TraceParts äº§å“æ•°æ®çˆ¬å–ç³»ç»Ÿ v4.0")
        self.logger.info("   åŸºäºæ¸è¿›å¼ç¼“å­˜ç®¡ç†å™¨")
        self.logger.info("="*60)
        
        # æ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        current_level, _ = self.cache_manager.get_cache_level()
        self.stats['cache_level_start'] = current_level.name
        
        self.logger.info(f"ğŸ“Š ç¼“å­˜çŠ¶æ€:")
        self.logger.info(f"   â€¢ å½“å‰çº§åˆ«: {current_level.name}")
        self.logger.info(f"   â€¢ ç›®æ ‡çº§åˆ«: {target_level.name}")
        self.logger.info(f"   â€¢ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        self.logger.info(f"   â€¢ å¹¶å‘çº¿ç¨‹: {self.max_workers}")
        
        if not cache_enabled:
            self.logger.info("   â€¢ âš ï¸  ç¼“å­˜å·²ç¦ç”¨ï¼Œå°†å¼ºåˆ¶åˆ·æ–°")
        
        self.logger.info("="*60)
        
        try:
            # ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨è¿è¡Œ
            data = self.cache_manager.run_progressive_cache(
                target_level=target_level,
                force_refresh=not cache_enabled
            )
            
            if not data:
                self.logger.error("âŒ æ•°æ®è·å–å¤±è´¥")
                return None
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(data)
            
            # ä¿å­˜ç»“æœï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼‰
            if output_file:
                self._save_results(data, output_file)
            
            # æ‰“å°æ±‡æ€»
            self._print_summary()
            
            return data
            
        except Exception as e:
            self.logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
    
    def _update_stats(self, data: Dict):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        metadata = data.get('metadata', {})
        self.stats['cache_level_end'] = metadata.get('cache_level_name', 'UNKNOWN')
        self.stats['total_leaves'] = metadata.get('total_leaves', 0)
        self.stats['total_products'] = metadata.get('total_products', 0)
        self.stats['total_specifications'] = metadata.get('total_specifications', 0)
    
    def _save_results(self, data: Dict, output_file: str):
        """ä¿å­˜ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ·»åŠ å¯¼å‡ºå…ƒæ•°æ®
        export_data = data.copy()
        export_data['export_metadata'] = {
            'exported_at': datetime.now().isoformat(),
            'export_file': str(output_path),
            'pipeline_version': '4.0-cache-manager'
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        file_size_mb = output_path.stat().st_size / 1024 / 1024
        self.logger.info(f"\nğŸ“„ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
        self.logger.info(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
    
    def _print_summary(self):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
        self.stats['end_time'] = datetime.now()
        self.stats['duration'] = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        duration_min = self.stats['duration'] / 60
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š çˆ¬å–å®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        self.logger.info("="*60)
        
        # æ—¶é—´ç»Ÿè®¡
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration_min:.1f} åˆ†é’Ÿ")
        
        # ç¼“å­˜çº§åˆ«å˜åŒ–
        self.logger.info(f"\nğŸ“ˆ ç¼“å­˜è¿›åº¦:")
        self.logger.info(f"   â€¢ èµ·å§‹çº§åˆ«: {self.stats['cache_level_start']}")
        self.logger.info(f"   â€¢ æœ€ç»ˆçº§åˆ«: {self.stats['cache_level_end']}")
        
        # æ•°æ®ç»Ÿè®¡
        self.logger.info(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        self.logger.info(f"   â€¢ å¶èŠ‚ç‚¹æ•°: {self.stats['total_leaves']:,}")
        self.logger.info(f"   â€¢ äº§å“æ€»æ•°: {self.stats['total_products']:,}")
        self.logger.info(f"   â€¢ è§„æ ¼æ€»æ•°: {self.stats['total_specifications']:,}")
        
        # å¹³å‡ç»Ÿè®¡
        if self.stats['total_leaves'] > 0:
            avg_products = self.stats['total_products'] / self.stats['total_leaves']
            self.logger.info(f"\nğŸ“ˆ å¹³å‡ç»Ÿè®¡:")
            self.logger.info(f"   â€¢ æ¯ä¸ªå¶èŠ‚ç‚¹å¹³å‡äº§å“æ•°: {avg_products:.1f}")
            
        if self.stats['total_products'] > 0:
            avg_specs = self.stats['total_specifications'] / self.stats['total_products']
            self.logger.info(f"   â€¢ æ¯ä¸ªäº§å“å¹³å‡è§„æ ¼æ•°: {avg_specs:.1f}")
        
        self.logger.info("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TraceParts ä¼˜åŒ–ç‰ˆæµæ°´çº¿ V2 (åŸºäºç¼“å­˜ç®¡ç†å™¨)')
    
    # ç¼“å­˜çº§åˆ«
    parser.add_argument(
        '--level',
        type=str,
        choices=['classification', 'products', 'specifications'],
        default='specifications',
        help='ç›®æ ‡ç¼“å­˜çº§åˆ« (é»˜è®¤: specifications)'
    )
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--workers', type=int, default=32, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 32)')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°çˆ¬å–')
    parser.add_argument('--cache-dir', type=str, default='results/cache', help='ç¼“å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # æ˜ å°„ç¼“å­˜çº§åˆ«
    level_map = {
        'classification': CacheLevel.CLASSIFICATION,
        'products': CacheLevel.PRODUCTS,
        'specifications': CacheLevel.SPECIFICATIONS
    }
    target_level = level_map[args.level]
    
    # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
    pipeline = OptimizedFullPipelineV2(
        max_workers=args.workers,
        cache_dir=args.cache_dir
    )
    
    pipeline.run(
        output_file=args.output,
        cache_enabled=not args.no_cache,
        target_level=target_level
    )


if __name__ == '__main__':
    main() 