#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangChain ç½‘é¡µçˆ¬å–æµ‹è¯• - TraceParts æ¨¡å‹ç±»ç›®ä¿¡æ¯æŠ“å–
ä½¿ç”¨Seleniumå¤„ç†JavaScriptåŠ¨æ€å†…å®¹
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import re
import time
from datetime import datetime
from typing import List, Dict, Any

from langchain_openai import ChatOpenAI
from config import get_openai_config, get_masked_api_key, validate_config
from langchain.schema import HumanMessage, SystemMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate

# å¯¼å…¥seleniumç›¸å…³æ¨¡å—
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# å¤‡ç”¨ï¼šä½¿ç”¨requests+beautifulsoup
import requests
from bs4 import BeautifulSoup

def setup_llm():
    """é…ç½®LLM"""
    try:
        config = get_openai_config()
        if not validate_config(config):
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")
        
        print(f"âœ… ä½¿ç”¨API Key: {get_masked_api_key(config['api_key'])}")
        print(f"âœ… ä½¿ç”¨Base URL: {config['base_url']}")
        
        llm = ChatOpenAI(
            model=config['model'],
            openai_api_key=config['api_key'],
            openai_api_base=config['base_url'],
            temperature=0.3,
            max_tokens=2000
        )
        return llm
    except Exception as e:
        print(f"âŒ LLMé…ç½®å¤±è´¥: {e}")
        raise

def load_web_content_selenium(url: str) -> str:
    """ä½¿ç”¨SeleniumåŠ è½½åŠ¨æ€ç½‘é¡µå†…å®¹"""
    print(f"ğŸŒ ä½¿ç”¨SeleniumåŠ è½½ç½‘é¡µ: {url}")
    
    if not SELENIUM_AVAILABLE:
        raise Exception("Seleniumæœªå®‰è£…ï¼Œè¯·è¿è¡Œï¼špip install selenium")
    
    try:
        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # åˆå§‹åŒ–WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        
        # è®¿é—®ç½‘é¡µ
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(5)
        
        # å°è¯•ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print("âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å°è¯•è·å–å†…å®¹...")
        
        # è·å–é¡µé¢å†…å®¹
        content = driver.page_source
        
        # æŸ¥æ‰¾ç±»ç›®ç›¸å…³çš„å†…å®¹
        try:
            # å°è¯•æ‰¾åˆ°ç±»ç›®å®¹å™¨
            category_elements = driver.find_elements(By.CSS_SELECTOR, 
                "[class*='category'], [class*='component'], [class*='product'], div[role='button'], a[href*='category']")
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(category_elements)} ä¸ªå¯èƒ½çš„ç±»ç›®å…ƒç´ ")
            
            # æå–ç±»ç›®æ–‡æœ¬
            category_texts = []
            for element in category_elements[:20]:  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
                try:
                    text = element.text.strip()
                    if text and len(text) > 2 and len(text) < 100:
                        category_texts.append(text)
                except:
                    continue
            
            if category_texts:
                content += "\n\n=== æå–çš„ç±»ç›®ä¿¡æ¯ ===\n" + "\n".join(category_texts)
                print(f"âœ… é¢å¤–æå–äº† {len(category_texts)} ä¸ªç±»ç›®æ–‡æœ¬")
        
        except Exception as e:
            print(f"âš ï¸  ç±»ç›®å…ƒç´ æå–å¤±è´¥: {str(e)}")
        
        driver.quit()
        
        print(f"âœ… Seleniumç½‘é¡µåŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        return content
        
    except Exception as e:
        print(f"âŒ SeleniumåŠ è½½å¤±è´¥: {str(e)}")
        raise

def load_web_content_requests(url: str) -> str:
    """ä½¿ç”¨requests+BeautifulSoupä½œä¸ºå¤‡ç”¨æ–¹æ³•"""
    print(f"ğŸŒ ä½¿ç”¨requests+BeautifulSoupåŠ è½½ç½‘é¡µ: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ–‡æœ¬å†…å®¹
        content = soup.get_text()
        
        # å°è¯•æŸ¥æ‰¾ç±»ç›®ç›¸å…³çš„å…ƒç´ 
        category_elements = soup.find_all(['div', 'a', 'span'], 
            string=re.compile(r'(MECHANICAL|ELECTRICAL|MATERIAL|HYDRAULICS|PNEUMATICS|OPTICS)', re.I))
        
        if category_elements:
            category_texts = [elem.get_text().strip() for elem in category_elements[:10]]
            content += "\n\n=== æ‰¾åˆ°çš„ç±»ç›®å…³é”®è¯ ===\n" + "\n".join(category_texts)
            print(f"âœ… æ‰¾åˆ° {len(category_texts)} ä¸ªç±»ç›®å…³é”®è¯")
        
        print(f"âœ… requestsåŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        return content
        
    except Exception as e:
        print(f"âŒ requestsåŠ è½½å¤±è´¥: {str(e)}")
        raise

def create_mock_data() -> str:
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•AIæå–åŠŸèƒ½"""
    print("ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    
    mock_content = """
    TraceParts - CAD Parts Library
    
    Explore by category:
    
    MECHANICAL COMPONENTS
    - Bearings and seals
    - Gears and transmission
    - Fasteners and hardware
    - Springs and dampers
    
    MANUFACTURING ENGINEERING
    - Machine tools
    - Cutting tools
    - Measurement tools
    - Assembly equipment
    
    MATERIALS (BARS, BEAMS, TUBES, ETC.)
    - Steel profiles
    - Aluminum profiles
    - Plastic materials
    - Composite materials
    
    MATERIAL HANDLING AND LIFTING EQUIPMENT
    - Conveyors
    - Hoists and cranes
    - Forklifts
    - Storage systems
    
    ELECTRICAL
    - Connectors
    - Switches
    - Circuit breakers
    - Cable management
    
    SENSORS AND MEASUREMENT SYSTEMS
    - Temperature sensors
    - Pressure sensors
    - Flow sensors
    - Level sensors
    
    ELECTRONICS
    - Circuit boards
    - Processors
    - Memory devices
    - Power supplies
    
    OPTICS
    - Lenses
    - Mirrors
    - Prisms
    - Optical fibers
    
    PNEUMATICS
    - Air cylinders
    - Valves
    - Air filters
    - Pressure regulators
    
    VACUUM EQUIPMENT
    - Vacuum pumps
    - Vacuum chambers
    - Vacuum valves
    - Vacuum gauges
    
    HYDRAULICS
    - Hydraulic pumps
    - Hydraulic cylinders
    - Hydraulic valves
    - Hydraulic filters
    
    HEAT TRANSMISSION
    - Heat exchangers
    - Radiators
    - Cooling fans
    - Thermal insulation
    
    BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)
    - Construction materials
    - Building tools
    - Safety equipment
    - Access systems
    
    CIVIL ENGINEERING
    - Infrastructure components
    - Road construction
    - Bridge components
    - Drainage systems
    """
    
    return mock_content

def load_web_content(url: str, use_mock: bool = False) -> str:
    """åŠ è½½ç½‘é¡µå†…å®¹çš„ä¸»å‡½æ•°ï¼Œå°è¯•å¤šç§æ–¹æ³•"""
    
    if use_mock:
        return create_mock_data()
    
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨Selenium
    if SELENIUM_AVAILABLE:
        try:
            return load_web_content_selenium(url)
        except Exception as e:
            print(f"âš ï¸  Seleniumæ–¹æ³•å¤±è´¥: {str(e)}")
    
    # æ–¹æ³•2: å°è¯•ä½¿ç”¨requests
    try:
        return load_web_content_requests(url)
    except Exception as e:
        print(f"âš ï¸  requestsæ–¹æ³•å¤±è´¥: {str(e)}")
    
    # æ–¹æ³•3: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    print("ğŸ­ æ‰€æœ‰ç½‘ç»œæ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    return create_mock_data()

def split_content(content: str) -> List[str]:
    """åˆ†å‰²ç½‘é¡µå†…å®¹ä¸ºåˆé€‚çš„å—"""
    print("ğŸ“„ æ­£åœ¨åˆ†å‰²ç½‘é¡µå†…å®¹...")
    
    # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        length_function=len,
    )
    
    # åˆ†å‰²æ–‡æœ¬
    chunks = text_splitter.split_text(content)
    print(f"âœ… å†…å®¹åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªå—çš„ç‰‡æ®µç”¨äºè°ƒè¯•
    for i, chunk in enumerate(chunks[:3]):
        preview = chunk[:200].replace('\n', ' ')
        print(f"  ğŸ“„ å— {i+1} é¢„è§ˆ: {preview}...")
    
    return chunks

def extract_model_categories(llm, chunks: List[str]) -> Dict[str, Any]:
    """ä½¿ç”¨LLMæå–æ¨¡å‹ç±»ç›®ä¿¡æ¯"""
    print("ğŸ¤– æ­£åœ¨ä½¿ç”¨AIåˆ†ææ¨¡å‹ç±»ç›®ä¿¡æ¯...")
    
    # åˆ›å»ºæç¤ºæ¨¡æ¿
    extraction_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯TracePartsç½‘ç«™çš„ä¸“ä¸šåˆ†æå¸ˆã€‚è¯·ä»ç½‘é¡µå†…å®¹ä¸­æå–äº§å“ç±»ç›®ä¿¡æ¯ã€‚

TracePartsæ˜¯ä¸€ä¸ªCADé›¶ä»¶åº“å¹³å°ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹ç±»å‹çš„äº§å“ç±»ç›®ï¼š
- MECHANICAL COMPONENTS (æœºæ¢°ç»„ä»¶)
- ELECTRICAL (ç”µæ°”)
- MATERIALS (ææ–™)
- HYDRAULICS (æ¶²å‹)
- PNEUMATICS (æ°”åŠ¨)
- OPTICS (å…‰å­¦)
- SENSORS (ä¼ æ„Ÿå™¨)
- ELECTRONICS (ç”µå­)

è¯·ä»”ç»†åˆ†æå†…å®¹ï¼Œæå–æ‰€æœ‰èƒ½æ‰¾åˆ°çš„äº§å“ç±»ç›®å’Œå­ç±»ç›®ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "main_categories": [
        {{
            "name": "ä¸»ç±»ç›®åç§°",
            "description": "æè¿°",
            "subcategories": [
                {{
                    "name": "å­ç±»ç›®åç§°",
                    "description": "æè¿°"
                }}
            ]
        }}
    ]
}}"""),
        ("human", "è¯·åˆ†æä»¥ä¸‹TracePartsç½‘ç«™å†…å®¹ï¼Œæå–æ‰€æœ‰äº§å“ç±»ç›®ï¼š\n\n{content}")
    ])
    
    all_categories = []
    
    for i, chunk in enumerate(chunks[:3]):  # å¤„ç†å‰3ä¸ªå—
        print(f"  ğŸ“ åˆ†æç¬¬ {i+1}/{min(len(chunks), 3)} ä¸ªå†…å®¹å—...")
        
        try:
            # ä½¿ç”¨Chainå¤„ç†
            chain = extraction_prompt | llm
            response = chain.invoke({
                "content": chunk
            })
            
            print(f"    ğŸ¤– AIå“åº”é¢„è§ˆ: {response.content[:100]}...")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                chunk_result = json.loads(response.content)
                if "main_categories" in chunk_result:
                    all_categories.extend(chunk_result["main_categories"])
                    print(f"    âœ… æå–åˆ° {len(chunk_result['main_categories'])} ä¸ªç±»ç›®")
            except json.JSONDecodeError:
                print(f"    âš ï¸  éJSONæ ¼å¼å“åº”ï¼Œå°è¯•æ–‡æœ¬è§£æ...")
                # å°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–ç±»ç›®åç§°
                lines = response.content.split('\n')
                for line in lines:
                    if any(keyword in line.upper() for keyword in ['MECHANICAL', 'ELECTRICAL', 'MATERIAL', 'HYDRAULIC', 'PNEUMATIC']):
                        all_categories.append({
                            "name": line.strip(),
                            "description": "ä»æ–‡æœ¬æå–",
                            "subcategories": []
                        })
                
        except Exception as e:
            print(f"    âŒ ç¬¬ {i+1} å—åˆ†æå¤±è´¥: {str(e)}")
            continue
    
    # åˆå¹¶å’Œå»é‡ç±»ç›®
    unique_categories = []
    seen_names = set()
    
    for category in all_categories:
        name = category.get("name", "").strip()
        if name and name not in seen_names:
            seen_names.add(name)
            unique_categories.append(category)
    
    # æ„å»ºæœ€ç»ˆç»“æœ
    result = {
        "main_categories": unique_categories,
        "extracted_info": {
            "total_categories": len(unique_categories),
            "extraction_time": datetime.now().isoformat(),
            "source_url": "https://www.traceparts.cn/en",
            "chunks_processed": min(len(chunks), 3)
        }
    }
    
    print(f"âœ… æ¨¡å‹ç±»ç›®æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_categories)} ä¸ªä¸»è¦ç±»ç›®")
    
    return result

def save_results(data: Dict[str, Any], filename: str = "traceparts_categories.json"):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ° {filename}...")
    
    try:
        # åˆ›å»ºresultsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs("results", exist_ok=True)
        
        filepath = os.path.join("results", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
        # æ˜¾ç¤ºä¿å­˜çš„å†…å®¹æ‘˜è¦
        method1_count = len(data.get("extraction_method_1", {}).get("main_categories", []))
        method2_count = len(data.get("extraction_method_2", {}).get("product_categories", []))
        total_count = max(method1_count, method2_count)
        
        print(f"ğŸ“Š ä¿å­˜äº† {total_count} ä¸ªç±»ç›®ä¿¡æ¯")
        
        return filepath
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        raise

def test_web_scraping():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 80)
    print("ğŸ•·ï¸  LangChain ç½‘é¡µçˆ¬å–æµ‹è¯• - TraceParts æ¨¡å‹ç±»ç›®æå– (æ”¹è¿›ç‰ˆ)")
    print("=" * 80)
    
    target_url = "https://www.traceparts.cn/en"
    
    try:
        # 1. é…ç½®LLM
        print("ğŸ”§ é…ç½®LangChain LLM...")
        llm = setup_llm()
        print("âœ… LLMé…ç½®å®Œæˆ")
        
        # 2. åŠ è½½ç½‘é¡µå†…å®¹ï¼ˆå°è¯•å¤šç§æ–¹æ³•ï¼‰
        print("\nğŸŒ å¼€å§‹åŠ è½½ç½‘é¡µå†…å®¹...")
        content = load_web_content(target_url, use_mock=False)
        
        print(f"ğŸ“Š å†…å®¹ç»Ÿè®¡:")
        print(f"  - æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"  - è¡Œæ•°: {len(content.split('\\n'))} è¡Œ")
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if len(content) < 1000:
            print("âš ï¸  å†…å®¹è¿‡å°‘ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            content = create_mock_data()
            print("ğŸ­ å·²åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
        
        # 3. åˆ†å‰²å†…å®¹
        chunks = split_content(content)
        
        # 4. æå–æ¨¡å‹ç±»ç›®ä¿¡æ¯
        print("\nğŸ” å¼€å§‹AIåˆ†æ...")
        categories_data = extract_model_categories(llm, chunks)
        
        # 5. åˆå¹¶ç»“æœ
        final_result = {
            "extraction_method_1": categories_data,
            "extraction_method_2": {"product_categories": []},  # ç®€åŒ–ç‰ˆæœ¬
            "metadata": {
                "url": target_url,
                "extraction_date": datetime.now().isoformat(),
                "content_length": len(content),
                "chunks_count": len(chunks),
                "extraction_methods": ["improved_ai_extraction"],
                "selenium_available": SELENIUM_AVAILABLE,
                "content_preview": content[:300].replace('\n', ' ')
            }
        }
        
        # 6. ä¿å­˜ç»“æœ
        saved_file = save_results(final_result)
        
        print("\n" + "=" * 80)
        print("ğŸ“‹ æå–ç»“æœæ‘˜è¦:")
        print("=" * 80)
        
        method1_count = len(categories_data.get("main_categories", []))
        
        print(f"ğŸ¯ æå–åˆ°çš„ç±»ç›®æ•°é‡: {method1_count} ä¸ª")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {saved_file}")
        
        # æ˜¾ç¤ºæå–ç»“æœ
        if method1_count > 0:
            print("\nğŸ”– æå–åˆ°çš„ç±»ç›®:")
            for i, cat in enumerate(categories_data["main_categories"]):
                name = cat.get('name', 'N/A')
                sub_count = len(cat.get('subcategories', []))
                print(f"  {i+1}. {name} ({sub_count} ä¸ªå­ç±»ç›®)")
        else:
            print("\nâš ï¸  æœªæå–åˆ°ç±»ç›®ï¼Œå¯èƒ½éœ€è¦:")
            print("  1. å®‰è£…selenium: pip install selenium")
            print("  2. å®‰è£…ChromeDriver")
            print("  3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        print("=" * 80)
        print("ğŸ‰ ç½‘é¡µçˆ¬å–æµ‹è¯•å®Œæˆï¼")
        
        return method1_count > 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("=" * 80)
        return False

if __name__ == "__main__":
    success = test_web_scraping()
    
    if success:
        print("âœ… TraceParts ç½‘é¡µçˆ¬å–æµ‹è¯•æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ TraceParts ç½‘é¡µçˆ¬å–æµ‹è¯•å¤±è´¥ï¼")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•å®‰è£…seleniumæˆ–ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
        
    exit(0 if success else 1) 