#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚æ­¥å¹¶å‘ç½‘é¡µçˆ¬å–æµ‹è¯• - TraceParts æ¨¡å‹ç±»ç›®ä¿¡æ¯æŠ“å–
å‚è€ƒ cot_extractor_safe.py çš„å¼‚æ­¥å¹¶å‘è®¾è®¡
"""

import os
import json
import re
import time
import asyncio
import aiofiles
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import random

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate

# å¯¼å…¥seleniumç›¸å…³æ¨¡å—
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
LOG = logging.getLogger("async-web-scraper")

# å¹¶å‘æ§åˆ¶å¸¸é‡ï¼ˆå‚è€ƒcot_extractor_safe.pyï¼‰
MAX_CHUNK_CONCURRENCY = 3  # åŒæ—¶å¤„ç†çš„å†…å®¹å—æ•°é‡
MAX_RETRIES = 3  # æ¯ä¸ªå—çš„æœ€å¤§é‡è¯•æ¬¡æ•°
CHUNK_TIMEOUT = 60  # æ¯ä¸ªå—çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
BACKOFF_BASE = 2.0  # é‡è¯•å»¶è¿ŸåŸºæ•°
BACKOFF_JITTER = 0.25  # Â±25% éšæœºæ€§

@dataclass
class ScrapingConfig:
    """çˆ¬å–é…ç½®"""
    api_key: str = "sk-YgL2cnnuifh9AloZFa6d63111aC64e4898Ba0769077521Ac"
    base_url: str = "https://ai.pumpkinai.online"
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    max_tokens: int = 2000
    chunk_concurrency: int = MAX_CHUNK_CONCURRENCY
    max_retries: int = MAX_RETRIES
    chunk_timeout: int = CHUNK_TIMEOUT

@dataclass
class ExtractionStats:
    """æå–ç»Ÿè®¡"""
    total_chunks: int = 0
    successful_chunks: int = 0
    failed_chunks: int = 0
    categories_found: int = 0
    retries_used: int = 0

def backoff_delay(attempt: int) -> float:
    """è®¡ç®—é€€é¿å»¶è¿Ÿï¼ˆå‚è€ƒcot_extractor_safe.pyï¼‰"""
    base = BACKOFF_BASE * (2 ** (attempt - 1))
    return base * (1 + BACKOFF_JITTER * (2 * random.random() - 1))

class AsyncJsonlWriter:
    """å¼‚æ­¥JSONLå†™å…¥å™¨ï¼ˆå‚è€ƒcot_extractor_safe.pyï¼‰"""
    def __init__(self, path: str):
        self.path = path
        self.lock = asyncio.Lock()
        os.makedirs(os.path.dirname(path), exist_ok=True)

    async def write(self, obj: Dict):
        async with self.lock:
            async with aiofiles.open(self.path, "a", encoding="utf-8") as f:
                await f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def create_mock_data() -> str:
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•ï¼ˆä¸åŒ…å«å‡é“¾æ¥ï¼‰"""
    return """
    TraceParts - CAD Parts Library
    
    Explore by category:
    
    MECHANICAL COMPONENTS
    - Bearings and seals
    - Gears and transmission
    - Fasteners and hardware
    - Springs and dampers
    
    MANUFACTURING ENGINEERING
    - Machine tools
    - Cutting tools
    - Measurement tools
    - Assembly equipment
    
    MATERIALS (BARS, BEAMS, TUBES, ETC.)
    - Steel profiles
    - Aluminum profiles
    - Plastic materials
    - Composite materials
    
    ELECTRICAL
    - Connectors
    - Switches
    - Circuit breakers
    - Cable management
    
    SENSORS AND MEASUREMENT SYSTEMS
    - Temperature sensors
    - Pressure sensors
    - Flow sensors
    - Level sensors
    
    ELECTRONICS
    - Circuit boards
    - Processors
    - Memory devices
    - Power supplies
    
    OPTICS
    - Lenses
    - Mirrors
    - Prisms
    - Optical fibers
    
    PNEUMATICS
    - Air cylinders
    - Valves
    - Air filters
    - Pressure regulators
    
    HYDRAULICS
    - Hydraulic pumps
    - Hydraulic cylinders
    - Hydraulic valves
    - Hydraulic filters
    
    æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸åŒ…å«çœŸå®é“¾æ¥
    """

def load_web_content_selenium(url: str) -> str:
    """ä½¿ç”¨SeleniumåŠ è½½åŠ¨æ€ç½‘é¡µå†…å®¹å¹¶æå–ç±»ç›®é“¾æ¥"""
    LOG.info(f"ğŸŒ ä½¿ç”¨SeleniumåŠ è½½ç½‘é¡µ: {url}")
    
    if not SELENIUM_AVAILABLE:
        raise Exception("Seleniumæœªå®‰è£…ï¼Œè¯·è¿è¡Œï¼špip install selenium")
    
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.get(url)
        
        LOG.info("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(8)  # å¢åŠ ç­‰å¾…æ—¶é—´
        
        try:
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            LOG.warning("âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å°è¯•è·å–å†…å®¹...")
        
        content = driver.page_source
        
        # æ”¹è¿›çš„ç±»ç›®é“¾æ¥æå–ç­–ç•¥
        category_data = []
        try:
            LOG.info("ğŸ” å¼€å§‹æå–ç±»ç›®é“¾æ¥...")
            
            # é’ˆå¯¹TracePartsç½‘ç«™çš„ç‰¹å®šé€‰æ‹©å™¨
            selectors = [
                # ä¸»è¦ç±»ç›®é€‰æ‹©å™¨
                "a[href*='/category/']",           # åŒ…å«categoryè·¯å¾„çš„é“¾æ¥
                "a[href*='/products/']",           # åŒ…å«productsè·¯å¾„çš„é“¾æ¥
                "a[href*='/components/']",         # åŒ…å«componentsè·¯å¾„çš„é“¾æ¥
                ".category-link",                  # ç±»ç›®é“¾æ¥class
                ".product-category",               # äº§å“ç±»ç›®class
                ".nav-category",                   # å¯¼èˆªç±»ç›®class
                "[data-category]",                 # åŒ…å«categoryå±æ€§çš„å…ƒç´ 
                # é€šç”¨é€‰æ‹©å™¨
                "nav a",                          # å¯¼èˆªæ é“¾æ¥
                ".menu a",                        # èœå•é“¾æ¥
                ".navigation a",                  # å¯¼èˆªé“¾æ¥
            ]
            
            all_elements = []
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        LOG.info(f"  é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        all_elements.extend(elements)
                except Exception as e:
                    LOG.debug(f"  é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {str(e)}")
            
            LOG.info(f"ğŸ“‹ æ€»å…±æ‰¾åˆ° {len(all_elements)} ä¸ªå¯èƒ½çš„ç±»ç›®å…ƒç´ ")
            
            # æå–å¹¶è¿‡æ»¤æœ‰æ•ˆçš„ç±»ç›®ä¿¡æ¯
            seen_urls = set()
            seen_texts = set()
            
            for i, element in enumerate(all_elements[:100]):  # å¤„ç†å‰100ä¸ªå…ƒç´ 
                try:
                    text = element.text.strip()
                    href = element.get_attribute('href') or ""
                    
                    # è¿‡æ»¤æ¡ä»¶
                    if not text or len(text) < 2 or len(text) > 100:
                        continue
                    
                    # è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å†…å®¹
                    skip_patterns = [
                        'cookie', 'privacy', 'terms', 'contact', 'about', 'login', 'register',
                        'help', 'support', 'news', 'blog', 'search', 'download', 'upload',
                        'home', 'javascript:', 'mailto:', '#'
                    ]
                    
                    if any(pattern in text.lower() or pattern in href.lower() for pattern in skip_patterns):
                        continue
                    
                    # åªä¿ç•™åŒ…å«äº§å“ç±»ç›®å…³é”®è¯çš„å†…å®¹
                    category_keywords = [
                        'mechanical', 'electrical', 'materials', 'hydraulics', 'pneumatics',
                        'sensors', 'electronics', 'optics', 'components', 'engineering',
                        'bearings', 'gears', 'fasteners', 'connectors', 'switches'
                    ]
                    
                    text_lower = text.lower()
                    href_lower = href.lower()
                    
                    # å¦‚æœæ–‡æœ¬æˆ–é“¾æ¥åŒ…å«ç±»ç›®å…³é”®è¯ï¼Œæˆ–è€…çœ‹èµ·æ¥åƒç±»ç›®åç§°
                    is_category = (
                        any(keyword in text_lower for keyword in category_keywords) or
                        any(keyword in href_lower for keyword in category_keywords) or
                        (len(text.split()) <= 5 and text.isupper()) or  # å…¨å¤§å†™çš„çŸ­æ–‡æœ¬
                        'category' in href_lower or
                        'component' in href_lower
                    )
                    
                    if not is_category:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if href and not href.startswith('http'):
                        if href.startswith('/'):
                            href = f"https://www.traceparts.cn{href}"
                        else:
                            href = f"https://www.traceparts.cn/{href}"
                    
                    # å»é‡
                    if href in seen_urls or text in seen_texts:
                        continue
                    
                    if href:
                        seen_urls.add(href)
                    seen_texts.add(text)
                    
                    category_data.append({
                        "text": text,
                        "url": href,
                        "element_tag": element.tag_name,
                        "element_index": i
                    })
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    LOG.debug(f"  æå–ç±»ç›®: '{text}' -> '{href}'")
                    
                except Exception as e:
                    continue
            
            if category_data:
                # å°†ç±»ç›®æ•°æ®æ·»åŠ åˆ°å†…å®¹ä¸­ï¼Œä¾›AIåˆ†æ
                category_info = "\n\n=== æå–çš„ç±»ç›®ä¿¡æ¯ï¼ˆçœŸå®æ•°æ®ï¼‰ ===\n"
                for item in category_data:
                    category_info += f"ç±»ç›®: {item['text']}\n"
                    if item['url']:
                        category_info += f"é“¾æ¥: {item['url']}\n"
                    category_info += f"å…ƒç´ : {item['element_tag']}\n"
                    category_info += "---\n"
                
                content += category_info
                LOG.info(f"âœ… æˆåŠŸæå–äº† {len(category_data)} ä¸ªçœŸå®ç±»ç›®å’Œé“¾æ¥")
                
                # è¾“å‡ºæå–ç»“æœçš„æ‘˜è¦
                LOG.info("ğŸ“‹ æå–çš„ç±»ç›®æ‘˜è¦:")
                for i, item in enumerate(category_data[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                    LOG.info(f"  {i+1}. {item['text'][:50]} -> {item['url'][:80] if item['url'] else 'æ— é“¾æ¥'}")
                if len(category_data) > 10:
                    LOG.info(f"  ... è¿˜æœ‰ {len(category_data) - 10} ä¸ªç±»ç›®")
                
            else:
                LOG.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç±»ç›®é“¾æ¥")
                LOG.info("ğŸ” å°è¯•ä½¿ç”¨æ›´å®½æ³›çš„é€‰æ‹©å™¨...")
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæ›´å®½æ³›çš„æœç´¢
                backup_elements = driver.find_elements(By.TAG_NAME, "a")
                LOG.info(f"ğŸ” å¤‡ç”¨æœç´¢æ‰¾åˆ° {len(backup_elements)} ä¸ªé“¾æ¥")
                
                # æ‰©å±•ç±»ç›®å…³é”®è¯åˆ—è¡¨
                extended_keywords = [
                    'mechanical', 'electrical', 'materials', 'hydraulics', 'pneumatics',
                    'sensors', 'electronics', 'optics', 'components', 'engineering',
                    'bearings', 'gears', 'fasteners', 'connectors', 'switches',
                    'manufacturing', 'measurement', 'lifting', 'equipment', 'vacuum',
                    'heat', 'transmission', 'building', 'constructions', 'civil',
                    'bars', 'beams', 'tubes', 'handling'
                ]
                
                # äº§å“ç±»ç›®URLçš„ç‰¹å¾æ¨¡å¼
                product_category_patterns = [
                    'traceparts-classification-',  # TracePartsçš„ç±»ç›®URLç‰¹å¾
                    'catalogpath=traceparts%3atp'   # CatalogPathå‚æ•°ç‰¹å¾
                ]
                
                # éœ€è¦æ’é™¤çš„éäº§å“ç±»ç›®é“¾æ¥æ¨¡å¼
                exclude_patterns = [
                    'sign-in', 'sign-up', 'login', 'register', 'catalogs', 'news', 
                    'info.traceparts.com', 'discover', 'advertising', 'read'
                ]
                
                for element in backup_elements[:100]:  # å¢åŠ æœç´¢èŒƒå›´
                    try:
                        text = element.text.strip()
                        href = element.get_attribute('href') or ""
                        
                        # æ”¾å®½æ¡ä»¶ï¼šåªè¦åŒ…å«ç±»ç›®å…³é”®è¯æˆ–ç‰¹å®šURLæ¨¡å¼
                        if (text and len(text) > 2 and len(text) < 80 and href):
                            text_lower = text.lower()
                            href_lower = href.lower()
                            
                            # é¦–å…ˆæ’é™¤ä¸ç›¸å…³çš„é“¾æ¥
                            if any(pattern in href_lower for pattern in exclude_patterns):
                                continue
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç±»ç›®å…³é”®è¯
                            has_keyword = any(keyword in text_lower for keyword in extended_keywords)
                            
                            # æ£€æŸ¥URLæ¨¡å¼ - ä¼˜å…ˆåŒ¹é…çœŸæ­£çš„äº§å“ç±»ç›®é“¾æ¥
                            is_product_category_url = any(pattern in href_lower for pattern in product_category_patterns)
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å…¨å¤§å†™çš„äº§å“ç±»ç›®åç§°ï¼ˆè¿›ä¸€æ­¥è¿‡æ»¤ï¼‰
                            is_valid_category_name = (
                                text.isupper() and 
                                len(text.split()) <= 6 and 
                                has_keyword and  # å¿…é¡»åŒ…å«äº§å“å…³é”®è¯
                                not any(skip in text_lower for skip in ['sign', 'register', 'see', 'start', 'read', 'discover'])
                            )
                            
                            # åªä¿ç•™çœŸæ­£çš„äº§å“ç±»ç›®é“¾æ¥
                            if is_product_category_url or (has_keyword and is_valid_category_name):
                                if href and not href.startswith('http'):
                                    if href.startswith('/'):
                                        href = f"https://www.traceparts.cn{href}"
                                
                                # å»é‡æ£€æŸ¥
                                duplicate = any(
                                    item['text'].upper() == text.upper() or item['url'] == href 
                                    for item in category_data
                                )
                                
                                if not duplicate:
                                    match_reason = "product_url" if is_product_category_url else "keyword"
                                    
                                    category_data.append({
                                        "text": text,
                                        "url": href,
                                        "element_tag": "a",
                                        "element_index": len(category_data),
                                        "match_reason": match_reason
                                    })
                                    
                                    LOG.info(f"  âœ… äº§å“ç±»ç›®: '{text}' -> '{href}'")
                                    
                                    # å¢åŠ æå–æ•°é‡é™åˆ¶
                                    if len(category_data) >= 30:
                                        break
                    except:
                        continue
                
                if category_data:
                    category_info = "\n\n=== TracePartsäº§å“ç±»ç›®ä¿¡æ¯ï¼ˆçœŸå®é“¾æ¥ï¼‰ ===\n"
                    category_info += "æ³¨æ„ï¼šä»¥ä¸‹æ˜¯ä»ç½‘é¡µæå–çš„çœŸå®äº§å“ç±»ç›®é“¾æ¥ï¼Œè¯·åŠ¡å¿…åœ¨JSONä¸­ä¿ç•™è¿™äº›URL\n\n"
                    
                    for item in category_data:
                        if item.get('match_reason') == 'product_url':  # ä¼˜å…ˆæ˜¾ç¤ºäº§å“URL
                            category_info += f"ğŸ”— äº§å“ç±»ç›®: {item['text']}\n"
                            category_info += f"ğŸŒ çœŸå®é“¾æ¥: {item['url']}\n"
                            category_info += f"ğŸ“ ç±»å‹: TracePartsäº§å“åˆ†ç±»é“¾æ¥\n"
                            category_info += "=" * 50 + "\n"
                        else:
                            category_info += f"ç±»ç›®: {item['text']}\n"
                            if item['url']:
                                category_info += f"é“¾æ¥: {item['url']}\n"
                            category_info += f"ç±»å‹: {item.get('match_reason', 'unknown')}\n"
                            category_info += "---\n"
                    
                    content += category_info
                    
                    # ç»Ÿè®¡çœŸå®äº§å“é“¾æ¥æ•°é‡
                    product_links = [item for item in category_data if item.get('match_reason') == 'product_url']
                    LOG.info(f"âœ… å¤‡ç”¨æ–¹æ¡ˆæå–äº† {len(category_data)} ä¸ªç±»ç›®ï¼Œå…¶ä¸­ {len(product_links)} ä¸ªæ˜¯çœŸå®äº§å“é“¾æ¥")
                    
                    # è¾“å‡ºè¯¦ç»†æå–ç»“æœ
                    LOG.info("ğŸ“‹ çœŸå®äº§å“ç±»ç›®é“¾æ¥:")
                    for i, item in enumerate(product_links[:10]):  # æ˜¾ç¤ºå‰10ä¸ªäº§å“é“¾æ¥
                        LOG.info(f"  {i+1}. {item['text']}")
                        LOG.info(f"      -> {item['url']}")
                    if len(product_links) > 10:
                        LOG.info(f"  ... è¿˜æœ‰ {len(product_links) - 10} ä¸ªäº§å“ç±»ç›®")
                    
                    if len(category_data) > len(product_links):
                        other_links = len(category_data) - len(product_links)
                        LOG.info(f"ğŸ“„ å…¶ä»–é“¾æ¥: {other_links} ä¸ª")
                else:
                    LOG.warning("âš ï¸  å¤‡ç”¨æ–¹æ¡ˆæœªæ‰¾åˆ°ä»»ä½•ç±»ç›®é“¾æ¥")
        
        except Exception as e:
            LOG.warning(f"âš ï¸  ç±»ç›®é“¾æ¥æå–å¤±è´¥: {str(e)}")
        
        driver.quit()
        
        LOG.info(f"âœ… Seleniumç½‘é¡µåŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        return content
        
    except Exception as e:
        LOG.error(f"âŒ SeleniumåŠ è½½å¤±è´¥: {str(e)}")
        raise

def load_web_content(url: str, use_mock: bool = False) -> str:
    """åŠ è½½ç½‘é¡µå†…å®¹çš„ä¸»å‡½æ•°"""
    
    if use_mock:
        return create_mock_data()
    
    if SELENIUM_AVAILABLE:
        try:
            return load_web_content_selenium(url)
        except Exception as e:
            LOG.warning(f"âš ï¸  Seleniumæ–¹æ³•å¤±è´¥: {str(e)}")
    
    # å¤‡ç”¨ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    LOG.info("ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    return create_mock_data()

class AsyncCategoryExtractor:
    """å¼‚æ­¥ç±»ç›®æå–å™¨ï¼ˆå‚è€ƒcot_extractor_safe.pyçš„è®¾è®¡ï¼‰"""
    
    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.chunk_sem = asyncio.Semaphore(config.chunk_concurrency)
        self.stats = ExtractionStats()
        self.error_writer = None
        
        # é…ç½®LLM
        self.llm = ChatOpenAI(
            model=config.model,
            openai_api_key=config.api_key,
            openai_api_base=config.base_url + "/v1",
            temperature=config.temperature,
            max_tokens=config.max_tokens
        )
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿ - æ”¹è¿›ç‰ˆæœ¬ï¼ˆåŒ…å«é“¾æ¥æå–ï¼‰
        self.extraction_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯TracePartsç½‘ç«™çš„ä¸“ä¸šåˆ†æå¸ˆã€‚è¯·ä»ç½‘é¡µå†…å®¹ä¸­æå–äº§å“ç±»ç›®ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç±»ç›®åç§°ã€æè¿°å’Œé“¾æ¥ã€‚

TracePartsæ˜¯ä¸€ä¸ªCADé›¶ä»¶åº“å¹³å°ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹ç±»å‹çš„äº§å“ç±»ç›®ï¼š
- MECHANICAL COMPONENTS (æœºæ¢°ç»„ä»¶)
- ELECTRICAL (ç”µæ°”) 
- MATERIALS (ææ–™)
- HYDRAULICS (æ¶²å‹)
- PNEUMATICS (æ°”åŠ¨)
- OPTICS (å…‰å­¦)
- SENSORS (ä¼ æ„Ÿå™¨)
- ELECTRONICS (ç”µå­)
- MANUFACTURING ENGINEERING (åˆ¶é€ å·¥ç¨‹)
- MATERIAL HANDLING AND LIFTING EQUIPMENT (ç‰©æ–™æ¬è¿å’Œèµ·é‡è®¾å¤‡)
- VACUUM EQUIPMENT (çœŸç©ºè®¾å¤‡)
- HEAT TRANSMISSION (çƒ­ä¼ å¯¼)
- BUILDING & CONSTRUCTIONS (å»ºç­‘å’Œæ–½å·¥)
- CIVIL ENGINEERING (åœŸæœ¨å·¥ç¨‹)

**å…³é”®è¦æ±‚ï¼š**
1. **ä¸¥ç¦è‡ªå·±ç¼–é€ URL**ï¼šåªèƒ½ä½¿ç”¨å†…å®¹ä¸­æ˜ç¡®æä¾›çš„çœŸå®é“¾æ¥
2. **ä¸¥æ ¼åŒ¹é…**ï¼šå½“çœ‹åˆ°"ğŸ”— äº§å“ç±»ç›®: XXX"å’Œ"ğŸŒ çœŸå®é“¾æ¥: XXX"æ—¶ï¼Œå¿…é¡»åŸæ ·ä½¿ç”¨è¿™ä¸ªé“¾æ¥
3. **å®Œå…¨å¤åˆ¶**ï¼šURLå¿…é¡»ä¸å†…å®¹ä¸­æä¾›çš„å®Œå…¨ä¸€è‡´ï¼ŒåŒ…æ‹¬æ‰€æœ‰å‚æ•°å’Œç¼–ç 

**è¿”å›æ ¼å¼è¦æ±‚ï¼š**
è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒURLå¿…é¡»æ˜¯å†…å®¹ä¸­æä¾›çš„çœŸå®é“¾æ¥ï¼š

{{
    "main_categories": [
        {{
            "name": "MECHANICAL COMPONENTS",
            "description": "æœºæ¢°ç»„ä»¶ç›¸å…³äº§å“",
            "url": "å®Œå…¨å¤åˆ¶å†…å®¹ä¸­çš„çœŸå®é“¾æ¥ï¼Œä¸è¦ä¿®æ”¹ä»»ä½•å­—ç¬¦",
            "subcategories": []
        }}
    ]
}}

**ä¸¥é‡è­¦å‘Šï¼š**
- ç»å¯¹ä¸è¦çŒœæµ‹æˆ–ç¼–é€ CatalogPathå‚æ•°ï¼ˆå¦‚TP01ã€TP02ç­‰ï¼‰
- ç»å¯¹ä¸è¦ä¿®æ”¹URLä¸­çš„ä»»ä½•å­—ç¬¦ï¼ŒåŒ…æ‹¬%3Aã€%3ATPç­‰ç¼–ç 
- å¦‚æœå†…å®¹ä¸­æ²¡æœ‰æä¾›æŸä¸ªç±»ç›®çš„é“¾æ¥ï¼Œurlå­—æ®µè®¾ä¸ºç©ºå­—ç¬¦ä¸²""
- åªæœ‰å†…å®¹ä¸­æ˜ç¡®æ ‡è®°ä¸º"ğŸŒ çœŸå®é“¾æ¥"çš„URLæ‰èƒ½ä½¿ç”¨"""),
            ("human", "è¯·åˆ†æä»¥ä¸‹TracePartsç½‘ç«™å†…å®¹ï¼Œæå–æ‰€æœ‰äº§å“ç±»ç›®ã€‚**é‡è¦ï¼šä¸¥æ ¼ä½¿ç”¨å†…å®¹ä¸­çš„çœŸå®é“¾æ¥ï¼Œä¸è¦è‡ªå·±ç¼–é€ ä»»ä½•URL**ï¼š\n\n{content}")
        ])

    def clean_category_name(self, name: str) -> str:
        """æ¸…ç†ç±»ç›®åç§°ï¼Œç§»é™¤å¤šä½™å­—ç¬¦å’Œæ ¼å¼é—®é¢˜"""
        if not name:
            return ""
        
        # ç§»é™¤å„ç§JSONæ ¼å¼æ®‹ç•™
        name = re.sub(r'^["\'\s]*', '', name)  # å¼€å¤´çš„å¼•å·å’Œç©ºæ ¼
        name = re.sub(r'["\'\s,]*$', '', name)  # ç»“å°¾çš„å¼•å·ã€ç©ºæ ¼ã€é€—å·
        name = re.sub(r'^name["\'\s]*:["\'\s]*', '', name, flags=re.IGNORECASE)  # "name": å‰ç¼€
        name = re.sub(r'["\'\s]*,["\'\s]*$', '', name)  # ç»“å°¾çš„é€—å·
        name = re.sub(r'\\["\']', '', name)  # è½¬ä¹‰å­—ç¬¦
        name = re.sub(r'^\s*[\[{]\s*', '', name)  # å¼€å¤´çš„æ‹¬å·
        name = re.sub(r'\s*[}\]]\s*$', '', name)  # ç»“å°¾çš„æ‹¬å·
        
        # ç»Ÿä¸€å¤§å°å†™å’Œç©ºæ ¼
        name = name.strip()
        name = re.sub(r'\s+', ' ', name)  # å¤šä¸ªç©ºæ ¼å˜æˆä¸€ä¸ª
        
        return name

    async def initialize(self, output_dir: str):
        """åˆå§‹åŒ–è¾“å‡ºç›®å½•å’Œé”™è¯¯è®°å½•å™¨"""
        os.makedirs(output_dir, exist_ok=True)
        self.error_writer = AsyncJsonlWriter(os.path.join(output_dir, "_errors.jsonl"))

    async def extract_chunk_categories(self, chunk_id: int, chunk_content: str) -> Optional[List[Dict[str, Any]]]:
        """å¼‚æ­¥æå–å•ä¸ªå†…å®¹å—çš„ç±»ç›®ä¿¡æ¯ï¼ˆæ”¹è¿›çš„è§£æé€»è¾‘ï¼‰"""
        
        async with self.chunk_sem:  # æ§åˆ¶å¹¶å‘æ•°é‡
            LOG.info(f"ğŸ“ å¼€å§‹åˆ†æç¬¬ {chunk_id} ä¸ªå†…å®¹å—...")
            
            for attempt in range(1, self.config.max_retries + 1):
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„LangChainè°ƒç”¨
                    loop = asyncio.get_event_loop()
                    with ThreadPoolExecutor() as executor:
                        chain = self.extraction_prompt | self.llm
                        
                        future = executor.submit(
                            chain.invoke, 
                            {"content": chunk_content}
                        )
                        
                        response = await asyncio.wait_for(
                            asyncio.wrap_future(future),
                            timeout=self.config.chunk_timeout
                        )
                    
                    LOG.debug(f"ğŸ¤– å— {chunk_id} AIå“åº”é¢„è§ˆ: {response.content[:200]}...")
                    
                    # å°è¯•è§£æJSONå“åº” - æ”¹è¿›ç‰ˆæœ¬
                    categories = self.parse_ai_response(response.content, chunk_id)
                    
                    if categories:
                        LOG.info(f"âœ… å— {chunk_id} æå–åˆ° {len(categories)} ä¸ªç±»ç›®")
                        return categories
                    else:
                        LOG.warning(f"âš ï¸  å— {chunk_id} æœªèƒ½æå–åˆ°æœ‰æ•ˆç±»ç›®")
                        return []
                    
                except asyncio.TimeoutError:
                    error_type = "TIMEOUT"
                    error_msg = f"ç¬¬ {attempt} æ¬¡å°è¯•è¶…æ—¶"
                    LOG.warning(f"â° å— {chunk_id} {error_msg}")
                    
                except Exception as e:
                    error_type = "API_ERROR"
                    error_msg = f"ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {str(e)}"
                    LOG.warning(f"âš ï¸  å— {chunk_id} {error_msg}")
                
                # è®°å½•é”™è¯¯å¹¶é‡è¯•
                if attempt < self.config.max_retries:
                    delay = backoff_delay(attempt)
                    LOG.info(f"ğŸ”„ å— {chunk_id} å°†åœ¨ {delay:.2f}s åé‡è¯•...")
                    await asyncio.sleep(delay)
                    self.stats.retries_used += 1
                else:
                    # è®°å½•æœ€ç»ˆå¤±è´¥
                    await self.error_writer.write({
                        "chunk_id": chunk_id,
                        "error_type": error_type,
                        "error_msg": error_msg,
                        "attempts": attempt
                    })
                    LOG.error(f"âŒ å— {chunk_id} æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ”¾å¼ƒ")
                    return None
            
            return None

    def parse_ai_response(self, response_content: str, chunk_id: int) -> List[Dict[str, Any]]:
        """è§£æAIå“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œé”™è¯¯æ¢å¤"""
        categories = []
        
        # æ–¹æ³•1: å°è¯•ç›´æ¥JSONè§£æ
        try:
            # æ¸…ç†å“åº”å†…å®¹
            cleaned_content = response_content.strip()
            if cleaned_content.startswith('```json'):
                cleaned_content = re.sub(r'^```json\s*', '', cleaned_content)
            if cleaned_content.endswith('```'):
                cleaned_content = re.sub(r'\s*```$', '', cleaned_content)
            
            result = json.loads(cleaned_content)
            if "main_categories" in result and isinstance(result["main_categories"], list):
                for cat in result["main_categories"]:
                    if isinstance(cat, dict) and "name" in cat:
                        clean_name = self.clean_category_name(cat["name"])
                        if clean_name:
                            clean_cat = {
                                "name": clean_name,
                                "description": cat.get("description", "").strip(),
                                "url": cat.get("url", ""),
                                "subcategories": []
                            }
                            # å¤„ç†å­ç±»ç›®
                            if "subcategories" in cat and isinstance(cat["subcategories"], list):
                                for sub in cat["subcategories"]:
                                    if isinstance(sub, dict) and "name" in sub:
                                        clean_sub_name = self.clean_category_name(sub["name"])
                                        if clean_sub_name:
                                            clean_cat["subcategories"].append({
                                                "name": clean_sub_name,
                                                "description": sub.get("description", "").strip(),
                                                "url": sub.get("url", "")
                                            })
                            categories.append(clean_cat)
                
                LOG.debug(f"å— {chunk_id} JSONè§£ææˆåŠŸï¼Œæå–åˆ° {len(categories)} ä¸ªç±»ç›®")
                return categories
                
        except json.JSONDecodeError as e:
            LOG.debug(f"å— {chunk_id} JSONè§£æå¤±è´¥: {str(e)}, å°è¯•æ–‡æœ¬è§£æ")
        
        # æ–¹æ³•2: æ–‡æœ¬æ¨¡å¼è§£æ
        categories = self.extract_categories_from_text(response_content, chunk_id)
        
        return categories

    def extract_categories_from_text(self, text: str, chunk_id: int) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–ç±»ç›®ä¿¡æ¯ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        categories = []
        
        # é¢„å®šä¹‰çš„ç±»ç›®å…³é”®è¯
        known_categories = [
            "MECHANICAL COMPONENTS", "ELECTRICAL", "MATERIALS", "HYDRAULICS", 
            "PNEUMATICS", "OPTICS", "SENSORS", "ELECTRONICS", 
            "MANUFACTURING ENGINEERING", "MATERIAL HANDLING AND LIFTING EQUIPMENT",
            "VACUUM EQUIPMENT", "HEAT TRANSMISSION", "BUILDING & CONSTRUCTIONS",
            "CIVIL ENGINEERING"
        ]
        
        # åœ¨æ–‡æœ¬ä¸­æœç´¢å·²çŸ¥ç±»ç›®
        for category in known_categories:
            # ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…
            patterns = [
                rf'\b{re.escape(category)}\b',
                rf'["\']?\s*{re.escape(category)}\s*["\']?',
                rf'name["\']?\s*:\s*["\']?\s*{re.escape(category)}\s*["\']?'
            ]
            
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    categories.append({
                        "name": category,
                        "description": f"{category}ç›¸å…³äº§å“",
                        "url": "",
                        "subcategories": []
                    })
                    break  # æ‰¾åˆ°å°±è·³å‡ºå†…å±‚å¾ªç¯
        
        # ç§»é™¤é‡å¤
        unique_categories = []
        seen_names = set()
        for cat in categories:
            name = cat["name"]
            if name not in seen_names:
                seen_names.add(name)
                unique_categories.append(cat)
        
        LOG.debug(f"å— {chunk_id} æ–‡æœ¬è§£ææå–åˆ° {len(unique_categories)} ä¸ªç±»ç›®")
        return unique_categories

    def merge_and_deduplicate_categories(self, all_categories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡ç±»ç›®ï¼Œæ™ºèƒ½å¤„ç†é‡å¤å’Œç›¸ä¼¼é¡¹ï¼Œå¹¶ç¡®ä¿æŒ‰é¢„å®šä¹‰é¡ºåºæ’åˆ—"""
        
        # é¢„å®šä¹‰çš„ç±»ç›®é¡ºåºï¼ˆæŒ‰é‡è¦æ€§å’Œé€»è¾‘é¡ºåºï¼‰
        predefined_order = [
            "MECHANICAL COMPONENTS",
            "MANUFACTURING ENGINEERING", 
            "MATERIALS (BARS, BEAMS, TUBES, ETC.)",
            "MATERIAL HANDLING AND LIFTING EQUIPMENT",
            "ELECTRICAL",
            "SENSORS AND MEASUREMENT SYSTEMS",
            "ELECTRONICS",
            "OPTICS",
            "PNEUMATICS",
            "VACUUM EQUIPMENT",
            "HYDRAULICS",
            "HEAT TRANSMISSION",
            "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)",
            "CIVIL ENGINEERING"
        ]
        
        # åç§°æ˜ å°„å­—å…¸ï¼šAIå¯èƒ½è¿”å›çš„ç®€åŒ–åç§° -> æ ‡å‡†åç§°
        name_mapping = {
            "MECHANICAL COMPONENTS": "MECHANICAL COMPONENTS",
            "MECHANICAL": "MECHANICAL COMPONENTS",
            "MANUFACTURING ENGINEERING": "MANUFACTURING ENGINEERING", 
            "MANUFACTURING": "MANUFACTURING ENGINEERING",
            "MATERIALS": "MATERIALS (BARS, BEAMS, TUBES, ETC.)",
            "MATERIALS (BARS, BEAMS, TUBES, ETC.)": "MATERIALS (BARS, BEAMS, TUBES, ETC.)",
            "MATERIAL HANDLING AND LIFTING EQUIPMENT": "MATERIAL HANDLING AND LIFTING EQUIPMENT",
            "MATERIAL HANDLING": "MATERIAL HANDLING AND LIFTING EQUIPMENT",
            "ELECTRICAL": "ELECTRICAL",
            "SENSORS AND MEASUREMENT SYSTEMS": "SENSORS AND MEASUREMENT SYSTEMS",
            "SENSORS": "SENSORS AND MEASUREMENT SYSTEMS",  # é‡è¦ï¼šç®€åŒ–åç§°æ˜ å°„
            "MEASUREMENT SYSTEMS": "SENSORS AND MEASUREMENT SYSTEMS",
            "ELECTRONICS": "ELECTRONICS",
            "OPTICS": "OPTICS",
            "PNEUMATICS": "PNEUMATICS",
            "VACUUM EQUIPMENT": "VACUUM EQUIPMENT",
            "VACUUM": "VACUUM EQUIPMENT",
            "HYDRAULICS": "HYDRAULICS",
            "HEAT TRANSMISSION": "HEAT TRANSMISSION",
            "BUILDING & CONSTRUCTIONS": "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)",
            "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)": "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)",
            "BUILDING": "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)",
            "CONSTRUCTIONS": "BUILDING & CONSTRUCTIONS (MATERIALS AND EQUIPMENTS)",
            "CIVIL ENGINEERING": "CIVIL ENGINEERING",
            "CIVIL": "CIVIL ENGINEERING"
        }
        
        category_map = {}
        
        for category in all_categories:
            if not isinstance(category, dict) or "name" not in category:
                continue
                
            # æ¸…ç†åç§°
            clean_name = self.clean_category_name(category["name"])
            if not clean_name:
                continue
            
            # æ ‡å‡†åŒ–åç§°æ˜ å°„
            normalized_input = clean_name.upper().strip()
            
            # æŸ¥æ‰¾æ˜ å°„çš„æ ‡å‡†åç§°
            standard_name = None
            
            # 1. ç›´æ¥åŒ¹é…
            if normalized_input in name_mapping:
                standard_name = name_mapping[normalized_input]
            else:
                # 2. éƒ¨åˆ†åŒ¹é…ï¼ˆé’ˆå¯¹åŒ…å«å…³ç³»ï¼‰
                for short_name, full_name in name_mapping.items():
                    if short_name in normalized_input or normalized_input in short_name:
                        standard_name = full_name
                        break
                
                # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨åŸåç§°
                if standard_name is None:
                    standard_name = clean_name
            
            # ä½¿ç”¨æ ‡å‡†åç§°ä½œä¸ºkey
            normalized_standard = standard_name.upper()
            
            if normalized_standard not in category_map:
                # æ–°ç±»ç›®
                category_map[normalized_standard] = {
                    "name": standard_name,  # ä½¿ç”¨æ ‡å‡†åç§°
                    "description": category.get("description", f"{standard_name}ç›¸å…³äº§å“").strip(),
                    "url": category.get("url", ""),
                    "subcategories": []
                }
            else:
                # åˆå¹¶æè¿°ï¼ˆé€‰æ‹©æ›´è¯¦ç»†çš„ï¼‰
                existing_desc = category_map[normalized_standard]["description"]
                new_desc = category.get("description", "").strip()
                
                if new_desc and len(new_desc) > len(existing_desc) and "ä»æ–‡æœ¬æå–" not in new_desc:
                    category_map[normalized_standard]["description"] = new_desc
            
            # åˆå¹¶å­ç±»ç›®
            if "subcategories" in category and isinstance(category["subcategories"], list):
                existing_subs = {sub["name"].upper(): sub for sub in category_map[normalized_standard]["subcategories"]}
                
                for sub in category["subcategories"]:
                    if isinstance(sub, dict) and "name" in sub:
                        clean_sub_name = self.clean_category_name(sub["name"])
                        if clean_sub_name:
                            normalized_sub = clean_sub_name.upper()
                            if normalized_sub not in existing_subs:
                                category_map[normalized_standard]["subcategories"].append({
                                    "name": clean_sub_name,
                                    "description": sub.get("description", "").strip(),
                                    "url": sub.get("url", "")
                                })
        
        # æŒ‰é¢„å®šä¹‰é¡ºåºæ’åˆ—ç±»ç›®
        ordered_categories = []
        
        # é¦–å…ˆæ·»åŠ é¢„å®šä¹‰é¡ºåºä¸­çš„ç±»ç›®
        for predefined_name in predefined_order:
            normalized_predefined = predefined_name.upper()
            if normalized_predefined in category_map:
                category = category_map[normalized_predefined]
                # å¯¹å­ç±»ç›®ä¹Ÿè¿›è¡Œæ’åº
                category["subcategories"].sort(key=lambda x: x["name"])
                ordered_categories.append(category)
                # ä»mapä¸­ç§»é™¤å·²å¤„ç†çš„ç±»ç›®
                del category_map[normalized_predefined]
        
        # ç„¶åæ·»åŠ å…¶ä»–æœªé¢„å®šä¹‰çš„ç±»ç›®ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
        remaining_categories = list(category_map.values())
        remaining_categories.sort(key=lambda x: x["name"])
        
        for category in remaining_categories:
            # å¯¹å­ç±»ç›®è¿›è¡Œæ’åº
            category["subcategories"].sort(key=lambda x: x["name"])
            ordered_categories.append(category)
        
        LOG.info(f"ğŸ“‹ ç±»ç›®æ ‡å‡†åŒ–æ˜ å°„å®Œæˆï¼Œé¢„å®šä¹‰ç±»ç›®: {len(ordered_categories) - len(remaining_categories)} ä¸ªï¼Œå…¶ä»–ç±»ç›®: {len(remaining_categories)} ä¸ª")
        
        return ordered_categories

    async def extract_all_categories(self, chunks: List[str]) -> Dict[str, Any]:
        """å¼‚æ­¥å¹¶å‘æå–æ‰€æœ‰å—çš„ç±»ç›®ä¿¡æ¯ï¼ˆæ”¹è¿›çš„å»é‡é€»è¾‘å’Œé¡ºåºä¿è¯ï¼‰"""
        LOG.info(f"ğŸ¤– å¼€å§‹å¼‚æ­¥AIåˆ†æï¼Œå¹¶å‘æ•°: {self.config.chunk_concurrency}")
        
        self.stats.total_chunks = len(chunks)
        
        # åˆ›å»ºå¸¦ç´¢å¼•çš„ä»»åŠ¡ï¼Œä¿æŒchunké¡ºåºä¿¡æ¯
        indexed_tasks = []
        for i, chunk in enumerate(chunks[:5]):  # é™åˆ¶å¤„ç†å‰5ä¸ªå—
            task = self.extract_chunk_categories_with_index(i+1, chunk, i)
            indexed_tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        LOG.info(f"ğŸš€ å¯åŠ¨ {len(indexed_tasks)} ä¸ªå¹¶å‘ä»»åŠ¡...")
        indexed_results = await asyncio.gather(*indexed_tasks, return_exceptions=True)
        
        # æŒ‰åŸå§‹chunké¡ºåºå¤„ç†ç»“æœ
        all_categories = []
        for i, result in enumerate(indexed_results):
            if isinstance(result, Exception):
                LOG.error(f"âŒ å— {i+1} ä»»åŠ¡å¼‚å¸¸: {str(result)}")
                self.stats.failed_chunks += 1
            elif result is None:
                LOG.warning(f"âš ï¸  å— {i+1} è¿”å›ç©ºç»“æœ")
                self.stats.failed_chunks += 1
            else:
                chunk_idx, categories = result
                if categories:
                    self.stats.successful_chunks += 1
                    # ä¸ºæ¯ä¸ªç±»ç›®æ·»åŠ æ¥æºä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    for cat in categories:
                        cat["_source_chunk"] = chunk_idx
                    all_categories.extend(categories)
                else:
                    self.stats.failed_chunks += 1
        
        # æ”¹è¿›çš„å»é‡å’Œåˆå¹¶é€»è¾‘ï¼ˆç°åœ¨åŒ…å«æ’åºï¼‰
        unique_categories = self.merge_and_deduplicate_categories(all_categories)
        
        # ç§»é™¤è°ƒè¯•ä¿¡æ¯
        for cat in unique_categories:
            cat.pop("_source_chunk", None)
        
        self.stats.categories_found = len(unique_categories)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            "main_categories": unique_categories,
            "extracted_info": {
                "total_categories": self.stats.categories_found,
                "extraction_time": datetime.now().isoformat(),
                "source_url": "https://www.traceparts.cn/en",
                "chunks_processed": self.stats.successful_chunks,
                "chunks_failed": self.stats.failed_chunks,
                "retries_used": self.stats.retries_used,
                "concurrency_used": self.config.chunk_concurrency,
                "ordering_applied": "é¢„å®šä¹‰ç±»ç›®é¡ºåº + å­—æ¯é¡ºåº",
                "processing_stats": {
                    "total_chunks": self.stats.total_chunks,
                    "successful_chunks": self.stats.successful_chunks,
                    "failed_chunks": self.stats.failed_chunks,
                    "success_rate": f"{self.stats.successful_chunks/self.stats.total_chunks*100:.1f}%" if self.stats.total_chunks > 0 else "0%"
                }
            }
        }
        
        LOG.info(f"âœ… å¼‚æ­¥åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {self.stats.categories_found} ä¸ªå”¯ä¸€ç±»ç›®")
        LOG.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {self.stats.successful_chunks}/{self.stats.total_chunks} æˆåŠŸ ({self.stats.successful_chunks/self.stats.total_chunks*100:.1f}%)" if self.stats.total_chunks > 0 else "ğŸ“Š å¤„ç†ç»Ÿè®¡: 0/0")
        LOG.info(f"ğŸ”„ æ€»é‡è¯•æ¬¡æ•°: {self.stats.retries_used}")
        LOG.info(f"ğŸ“‹ ç±»ç›®å·²æŒ‰é¢„å®šä¹‰é¡ºåºæ’åˆ—")
        
        return result

    async def extract_chunk_categories_with_index(self, chunk_id: int, chunk_content: str, original_index: int) -> Optional[tuple]:
        """å¼‚æ­¥æå–å•ä¸ªå†…å®¹å—çš„ç±»ç›®ä¿¡æ¯ï¼Œè¿”å›å¸¦ç´¢å¼•çš„ç»“æœ"""
        categories = await self.extract_chunk_categories(chunk_id, chunk_content)
        if categories is not None:
            return (original_index, categories)
        return None

async def test_async_web_scraping():
    """å¼‚æ­¥ä¸»æµ‹è¯•å‡½æ•°"""
    LOG.info("=" * 80)
    LOG.info("ğŸ•·ï¸  å¼‚æ­¥å¹¶å‘ç½‘é¡µçˆ¬å–æµ‹è¯• - TraceParts æ¨¡å‹ç±»ç›®æå–")
    LOG.info("=" * 80)
    
    target_url = "https://www.traceparts.cn/en"
    config = ScrapingConfig()
    
    try:
        # 1. åˆå§‹åŒ–æå–å™¨
        LOG.info("ğŸ”§ åˆå§‹åŒ–å¼‚æ­¥ç±»ç›®æå–å™¨...")
        extractor = AsyncCategoryExtractor(config)
        await extractor.initialize("results")
        LOG.info("âœ… æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 2. åŠ è½½ç½‘é¡µå†…å®¹ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
        LOG.info("\nğŸŒ å¼€å§‹åŠ è½½ç½‘é¡µå†…å®¹...")
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            content = await loop.run_in_executor(
                executor, 
                load_web_content, 
                target_url, 
                False  # use_mock
            )
        
        LOG.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡:")
        LOG.info(f"  - æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        LOG.info(f"  - è¡Œæ•°: {len(content.split('\\n'))} è¡Œ")
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if len(content) < 1000:
            LOG.warning("âš ï¸  å†…å®¹è¿‡å°‘ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®")
            content = create_mock_data()
            LOG.info("ğŸ­ å·²åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
        
        # 3. åˆ†å‰²å†…å®¹
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=3000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=len,
        )
        chunks = text_splitter.split_text(content)
        LOG.info(f"âœ… å†…å®¹åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
        
        # 4. å¼‚æ­¥æå–æ¨¡å‹ç±»ç›®ä¿¡æ¯
        LOG.info(f"\nğŸ” å¼€å§‹å¼‚æ­¥AIåˆ†æ (å¹¶å‘æ•°: {config.chunk_concurrency})...")
        start_time = time.time()
        
        categories_data = await extractor.extract_all_categories(chunks)
        
        analysis_time = time.time() - start_time
        LOG.info(f"â±ï¸  AIåˆ†ææ€»è€—æ—¶: {analysis_time:.2f} ç§’")
        
        # 5. åˆå¹¶ç»“æœ
        final_result = {
            "extraction_method_1": categories_data,
            "metadata": {
                "url": target_url,
                "extraction_date": datetime.now().isoformat(),
                "content_length": len(content),
                "chunks_count": len(chunks),
                "analysis_time_seconds": round(analysis_time, 2),
                "extraction_methods": ["async_concurrent_ai_extraction"],
                "selenium_available": SELENIUM_AVAILABLE,
                "config": {
                    "chunk_concurrency": config.chunk_concurrency,
                    "max_retries": config.max_retries,
                    "chunk_timeout": config.chunk_timeout
                },
                "content_preview": content[:300].replace('\n', ' ')
            }
        }
        
        # 6. å¼‚æ­¥ä¿å­˜ç»“æœ
        LOG.info("ğŸ’¾ æ­£åœ¨å¼‚æ­¥ä¿å­˜ç»“æœ...")
        filepath = "results/traceparts_categories_async.json"
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(final_result, ensure_ascii=False, indent=2))
        
        LOG.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
        LOG.info("\n" + "=" * 80)
        LOG.info("ğŸ“‹ æå–ç»“æœæ‘˜è¦:")
        LOG.info("=" * 80)
        
        method1_count = len(categories_data.get("main_categories", []))
        
        LOG.info(f"ğŸ¯ æå–åˆ°çš„ç±»ç›®æ•°é‡: {method1_count} ä¸ª")
        LOG.info(f"ğŸ“ ç»“æœæ–‡ä»¶: {filepath}")
        LOG.info(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {analysis_time:.2f} ç§’")
        LOG.info(f"ğŸ”„ é‡è¯•ç»Ÿè®¡: {extractor.stats.retries_used} æ¬¡é‡è¯•")
        
        # æ˜¾ç¤ºæå–ç»“æœ
        if method1_count > 0:
            LOG.info("\nğŸ”– æå–åˆ°çš„ç±»ç›®:")
            for i, cat in enumerate(categories_data["main_categories"]):
                name = cat.get('name', 'N/A')
                sub_count = len(cat.get('subcategories', []))
                LOG.info(f"  {i+1}. {name} ({sub_count} ä¸ªå­ç±»ç›®)")
        else:
            LOG.warning("\nâš ï¸  æœªæå–åˆ°ç±»ç›®ï¼Œå¯èƒ½éœ€è¦:")
            LOG.warning("  1. å®‰è£…selenium: pip install selenium")
            LOG.warning("  2. å®‰è£…ChromeDriver")
            LOG.warning("  3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        LOG.info("=" * 80)
        LOG.info("ğŸ‰ å¼‚æ­¥ç½‘é¡µçˆ¬å–æµ‹è¯•å®Œæˆï¼")
        
        return method1_count > 0
        
    except Exception as e:
        LOG.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        LOG.info("=" * 80)
        return False

def test_web_scraping():
    """åŒæ­¥åŒ…è£…å‡½æ•°ï¼Œè¿è¡Œå¼‚æ­¥æµ‹è¯•"""
    return asyncio.run(test_async_web_scraping())

if __name__ == "__main__":
    success = test_web_scraping()
    
    if success:
        print("âœ… TraceParts å¼‚æ­¥ç½‘é¡µçˆ¬å–æµ‹è¯•æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ TraceParts å¼‚æ­¥ç½‘é¡µçˆ¬å–æµ‹è¯•å¤±è´¥ï¼")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•å®‰è£…seleniumæˆ–ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
        
    exit(0 if success else 1) 