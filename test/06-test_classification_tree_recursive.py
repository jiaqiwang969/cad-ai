#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• 06  â€”â€” Selenium é€’å½’/éå†æ–¹å¼æŠ“å– TraceParts Classification å®Œæ•´æ ‘çŠ¶ç»“æ„ï¼Œ
ä¸ä¾èµ– AIï¼Œç›´æ¥è§£æ DOMï¼Œè¾“å‡ºå…¨é‡ JSONã€‚
"""

import os
import json
import time
import logging
from datetime import datetime
from collections import defaultdict
from typing import Dict, List
from bs4 import BeautifulSoup

# Selenium
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
LOG = logging.getLogger("test-06")

ROOT_URL = "https://www.traceparts.cn/en/search/traceparts-classification?CatalogPath=TRACEPARTS%3ATRACEPARTS"

EXCLUDE_PATTERNS = [
    "sign-in", "sign-up", "login", "register", "javascript:", "mailto:", "#"
]


def prepare_driver() -> webdriver.Chrome:
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    driver = webdriver.Chrome(options=chrome_options)
    driver.set_page_load_timeout(40)
    return driver


def scroll_full(driver: webdriver.Chrome):
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
    # å›åˆ°é¡¶éƒ¨
    driver.execute_script("window.scrollTo(0,0);")


def extract_links(driver: webdriver.Chrome) -> List[Dict]:
    elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='traceparts-classification-']")
    LOG.info(f"ğŸ”— å…±æ•è· {len(elements)} ä¸ªåŒ…å« classification çš„é“¾æ¥èŠ‚ç‚¹")
    records = []
    seen = set()

    def guess_name_from_href(href: str) -> str:
        try:
            if 'traceparts-classification-' in href:
                tail = href.split('traceparts-classification-')[1]
                path_part = tail.split('?')[0].strip('-')
                if path_part:
                    # æ‹¿æœ€åä¸€ä¸ªæ®µè½ä½œä¸ºåç§°
                    last_seg = path_part.split('-')[-1]
                    return last_seg.replace('-', ' ').replace('_', ' ').title()
        except Exception:
            pass
        return "Unnamed"

    for el in elements:
        href = el.get_attribute('href') or ""
        # å¯è§æ–‡æœ¬
        raw_text = el.text.strip()
        if not href or any(pat in href.lower() for pat in EXCLUDE_PATTERNS):
            continue
        # å»é‡
        if href in seen:
            continue
        seen.add(href)

        name = raw_text
        # å¦‚æœå¯è§æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•å…¶ä»–å±æ€§
        if not name:
            alt_sources = [
                el.get_attribute('title'),
                el.get_attribute('aria-label'),
                el.get_attribute('data-original-title'),
                el.get_attribute('innerText'),
                el.get_attribute('textContent')
            ]
            for src in alt_sources:
                if src and src.strip():
                    name = src.strip()
                    break
        # ä»ä¸ºç©ºï¼Œè§£æ innerHTML æ‹¿å­å…ƒç´ æ–‡æœ¬
        if not name:
            inner_html = el.get_attribute('innerHTML') or ""
            soup = BeautifulSoup(inner_html, 'html.parser')
            txt = soup.get_text(" ", strip=True)
            if txt:
                name = txt
        # ä»ä¸ºç©ºï¼Œå°è¯•ä» href æ¨æ–­
        if not name:
            name = guess_name_from_href(href)

        records.append({"name": name, "url": href})
    LOG.info(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(records)} æ¡å”¯ä¸€åˆ†ç±»é“¾æ¥ï¼Œå…¶ä¸­å·²å¡«å……åç§°")
    return records


def analyse_level(item_url: str) -> int:
    """æ ¹æ® CatalogPath çš„ TP ç¼–ç æ¨æ–­å±‚çº§ï¼Œé«˜å¯é ï¼š
    TP##                           -> L2 (ä¸»ç±»ç›®)
    TP##XXX                        -> L3 (1 ç»„ 3 ä½)
    TP##XXXYYY                     -> L4 (2 ç»„)
    ä¾æ­¤ç±»æ¨ï¼›è‹¥æ— ç¬¦åˆè§„åˆ™ï¼Œé€€å›åˆ°åŸºäº '-' è®¡æ•°æ³•ã€‚"""

    if "%3ATRACEPARTS" in item_url:
        return 1  # æ ¹åˆ†ç±»é¡µé¢

    level_by_dash = None
    # å¤‡ç”¨ï¼š'-' è®¡æ•°
    try:
        tail = item_url.split('traceparts-classification-')[1]
        path_part = tail.split('?')[0].strip('-')
        if path_part:
            level_by_dash = len(path_part.split('-')) + 1  # L2 èµ·
    except Exception:
        pass

    # CatalogPath æ¨æ–­
    cat_path_part = None
    if "CatalogPath=TRACEPARTS%3A" in item_url:
        cat_path_part = item_url.split("CatalogPath=TRACEPARTS%3A")[1].split('&')[0]
    if cat_path_part and cat_path_part.startswith("TP"):
        code = cat_path_part[2:]
        if len(code) <= 2:  # TP01..TP14 ç­‰
            return 2
        # å‰©ä½™æ¯ 3 ä½ä¸€ä¸ªæ·±åº¦
        depth_groups = len(code) // 3
        return 2 + depth_groups

    return level_by_dash if level_by_dash else 2


def build_tree(records: List[Dict]) -> List[Dict]:
    enriched = []
    for rec in records:
        level = analyse_level(rec['url'])
        rec['level'] = level
        enriched.append(rec)
    # sort by level then name
    enriched.sort(key=lambda x: (x['level'], x['name']))
    return enriched


def main():
    if not SELENIUM_AVAILABLE:
        LOG.error("Selenium æœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œ test-06ï¼")
        return False
    driver = prepare_driver()
    try:
        LOG.info(f"ğŸŒ æ‰“å¼€æ ¹åˆ†ç±»é¡µé¢: {ROOT_URL}")
        driver.get(ROOT_URL)
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        time.sleep(4)
        scroll_full(driver)
        records = extract_links(driver)
        tree = build_tree(records)
        # ç»Ÿè®¡
        stats = defaultdict(int)
        for item in tree:
            stats[item['level']] += 1
        LOG.info("ğŸ“Š å±‚çº§ç»Ÿè®¡:" + ", ".join([f"L{lv}:{cnt}" for lv,cnt in sorted(stats.items())]))
        # ä¿å­˜
        result_file = "results/traceparts_classification_tree_full.json"
        os.makedirs("results", exist_ok=True)
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({"total": len(tree), "records": tree, "stats": stats, "crawl_time": datetime.now().isoformat()}, f, ensure_ascii=False, indent=2)
        LOG.info(f"âœ… å·²ä¿å­˜å®Œæ•´åˆ†ç±»æ ‘åˆ° {result_file}")
        return True
    except Exception as e:
        LOG.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return False
    finally:
        driver.quit()

if __name__ == "__main__":
    ok = main()
    print("âœ… test-06 æˆåŠŸ" if ok else "âŒ test-06 å¤±è´¥") 